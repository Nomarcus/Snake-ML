
<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
<title>Snake — RL studio with smooth rendering</title>
<script defer src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.20.0/dist/tf.min.js"></script>
<style>
:root {
  --bg:#090d1f;
  --panel:#141936;
  --ink:#eef2ff;
  --muted:#9ba8d6;
  --accent-a:#8b5cf6;
  --accent-b:#ec4899;
  --danger:#f43f5e;
  --stroke:rgba(134,144,214,0.18);
}
*{box-sizing:border-box}
body{
  margin:0;
  min-height:100vh;
  background:radial-gradient(160% 140% at 20% -20%,#202866 0%,#131834 45%,#090d1f 100%);
  color:var(--ink);
  font:14px/1.5 "Inter","Segoe UI",Roboto,sans-serif;
}
header{
  padding:20px 0;
  position:sticky;
  top:0;
  z-index:10;
  background:linear-gradient(135deg,rgba(28,33,72,0.85),rgba(12,16,36,0.88));
  backdrop-filter:blur(18px);
  border-bottom:1px solid var(--stroke);
  box-shadow:0 20px 40px rgba(6,8,20,0.6);
}
header .header-inner{
  margin:0 auto;
  padding:0 32px;
  display:flex;
  align-items:center;
  gap:18px;
}
.logo{
  font-weight:800;
  font-size:20px;
  display:flex;
  align-items:center;
  gap:12px;
  color:var(--ink);
  letter-spacing:0.01em;
}
.logo::before{
  content:"";
  width:24px;
  height:24px;
  border-radius:8px;
  background:linear-gradient(140deg,var(--accent-a),var(--accent-b));
  box-shadow:0 0 24px rgba(236,72,153,0.4);
}
.logo-text{
  background:linear-gradient(140deg,var(--accent-a),var(--accent-b));
  -webkit-background-clip:text;
  -webkit-text-fill-color:transparent;
}
.status-group{
  margin-left:auto;
  display:flex;
  gap:8px;
  align-items:center;
  flex-wrap:wrap;
}
.badge{
  padding:6px 12px;
  border-radius:999px;
  border:1px solid rgba(134,144,214,0.35);
  background:rgba(26,32,74,0.75);
  color:#cfd5ff;
  font-size:12px;
  font-weight:600;
  box-shadow:0 8px 18px rgba(8,12,32,0.35);
}
.badge.soft{
  background:rgba(139,92,246,0.18);
  color:#e9ecff;
  border-color:rgba(139,92,246,0.45);
}
main.layout{
  margin:32px auto 48px;
  padding:0 24px 40px;
  display:grid;
  gap:32px;
}
.card{
  background:linear-gradient(155deg,rgba(34,41,82,0.88) 0%,rgba(18,21,46,0.92) 100%);
  border:1px solid var(--stroke);
  border-radius:24px;
  padding:24px;
  box-shadow:0 24px 50px rgba(6,8,24,0.55);
  display:flex;
  flex-direction:column;
  gap:18px;
  backdrop-filter:blur(18px);
}
.game-card{grid-column:1;}
.control-card{
  grid-column:2;
  align-self:start;
}
.card-head{
  display:flex;
  justify-content:space-between;
  align-items:center;
  gap:12px;
}
.game-card .card-head{
  align-items:flex-start;
  gap:18px;
}
.game-card .card-head .subtitle{
  margin:4px 0 0;
  font-size:13px;
  color:var(--muted);
}
.card-actions{
  display:flex;
  gap:8px;
  flex-wrap:wrap;
}
.card-actions button.secondary.active{
  background:rgba(139,92,246,0.22);
  border-color:rgba(139,92,246,0.55);
  color:#f5f4ff;
}
h2{
  margin:0;
  font-size:20px;
  color:#f0f2ff;
  letter-spacing:0.01em;
}
canvas#board{
  width:100%;
  max-width:560px;
  aspect-ratio:1/1;
  border-radius:26px;
  background:radial-gradient(140% 140% at 30% 20%,#273067 0%,#151a3a 50%,#0d1127 100%);
  border:1px solid rgba(128,140,210,0.25);
  box-shadow:0 40px 80px rgba(8,12,42,0.55);
  align-self:center;
}
.controls{
  display:flex;
  flex-wrap:wrap;
  gap:12px;
  align-items:center;
}
.controls.primary{
  width:100%;
  justify-content:center;
  background:rgba(21,26,60,0.62);
  border-radius:18px;
  padding:14px;
  border:1px solid rgba(128,138,206,0.25);
  box-shadow:0 18px 36px rgba(8,12,34,0.45);
}
.controls.primary button{
  flex:1 1 140px;
  min-width:140px;
}
.controls.secondary{
  width:100%;
  justify-content:space-between;
  background:rgba(19,24,54,0.6);
  border-radius:16px;
  padding:14px 16px;
  border:1px solid rgba(128,138,206,0.2);
  box-shadow:0 16px 34px rgba(6,10,30,0.38);
}
.controls.tertiary{
  width:100%;
  justify-content:space-between;
  align-items:center;
  gap:14px;
  flex-wrap:wrap;
  background:rgba(19,24,54,0.6);
  border-radius:16px;
  padding:14px 16px;
  border:1px solid rgba(128,138,206,0.2);
  box-shadow:0 16px 34px rgba(6,10,30,0.38);
}
.ai-auto-tune{
  display:flex;
  flex-direction:column;
  gap:14px;
  background:rgba(19,24,54,0.6);
  border-radius:16px;
  padding:14px 16px;
  border:1px solid rgba(128,138,206,0.2);
  box-shadow:0 16px 34px rgba(6,10,30,0.38);
}
.ai-auto-tune__header{
  display:flex;
  align-items:center;
  justify-content:space-between;
  gap:16px;
}
.ai-auto-tune__header h3{
  margin:0;
  font-size:14px;
  letter-spacing:0.06em;
  text-transform:uppercase;
  color:#e7ebff;
}
.ai-auto-tune__header .hint{
  margin:4px 0 0;
}
.ai-auto-tune__controls{
  display:flex;
  align-items:stretch;
  justify-content:space-between;
  gap:18px;
  flex-wrap:wrap;
}
.ai-auto-tune__column{
  flex:1 1 240px;
  display:flex;
  flex-direction:column;
  gap:12px;
  padding:14px 16px;
  border-radius:14px;
  background:rgba(17,22,48,0.55);
  border:1px solid rgba(128,138,206,0.2);
  box-shadow:inset 0 0 0 1px rgba(84,98,176,0.08);
}
.ai-auto-tune__column h4{
  margin:0;
  font-size:12px;
  font-weight:700;
  letter-spacing:0.08em;
  text-transform:uppercase;
  color:#9fa9ff;
}
.ai-auto-tune__column .field{
  margin:0;
}
.toggle{
  display:flex;
  align-items:center;
  gap:10px;
  color:#e7ebff;
  font-weight:600;
  cursor:pointer;
}
.toggle input[type="checkbox"]{
  width:18px;
  height:18px;
  accent-color:var(--accent-a);
  cursor:pointer;
}
button{
  appearance:none;
  border:none;
  padding:11px 16px;
  border-radius:12px;
  color:#fff;
  background:linear-gradient(135deg,var(--accent-a),var(--accent-b));
  font-weight:700;
  cursor:pointer;
  transition:.25s transform,.25s box-shadow;
  box-shadow:0 18px 36px rgba(236,72,153,0.35);
  letter-spacing:0.01em;
}
button:hover{transform:translateY(-2px)}
button.secondary{
  background:rgba(32,38,82,0.85);
  box-shadow:none;
  color:#d4dcff;
  border:1px solid rgba(128,138,206,0.35);
}
button.secondary:hover{
  background:rgba(40,48,102,0.9);
  border-color:rgba(155,168,235,0.45);
}
button.danger{
  background:linear-gradient(135deg,#fb7185,#ef4444);
  box-shadow:none;
}
button.danger:hover{
  background:linear-gradient(135deg,#f43f5e,#dc2626);
}
button:disabled{
  opacity:0.6;
  cursor:not-allowed;
  transform:none;
  box-shadow:none;
}
button:focus-visible{
  outline:2px solid rgba(139,92,246,0.6);
  outline-offset:3px;
}
.pill-group{
  display:inline-flex;
  gap:6px;
  background:rgba(26,32,74,0.8);
  padding:6px;
  border-radius:999px;
  border:1px solid rgba(128,138,206,0.35);
  box-shadow:0 14px 30px rgba(8,12,34,0.4);
}
.pill-group .pill{
  appearance:none;
  border:none;
  padding:8px 16px;
  border-radius:999px;
  background:transparent;
  color:#d4dcff;
  font-weight:600;
  cursor:pointer;
  transition:.2s background,.2s color,.2s box-shadow;
}
.pill-group .pill.active{
  background:linear-gradient(135deg,var(--accent-a),var(--accent-b));
  color:#fff;
  box-shadow:0 12px 26px rgba(236,72,153,0.35);
}
.field{
  display:flex;
  flex-direction:column;
  gap:4px;
  font-size:12px;
  color:var(--muted);
}
.field.compact{
  min-width:160px;
}
.field.block select{
  width:100%;
}
.field label{
  color:var(--muted);
  font-size:12px;
  font-weight:600;
}
.ai-interval-field .field-header{
  display:flex;
  align-items:center;
  justify-content:space-between;
  gap:12px;
}
.ai-interval-field .toggle span{
  font-size:12px;
}
input[type="range"]{
  width:200px;
  -webkit-appearance:none;
  background:rgba(38,45,92,0.85);
  height:6px;
  border-radius:999px;
  outline:none;
}
input[type="range"]::-webkit-slider-runnable-track{
  height:6px;
  border-radius:999px;
  background:rgba(38,45,92,0.85);
}
input[type="range"]::-webkit-slider-thumb{
  -webkit-appearance:none;
  width:16px;
  height:16px;
  border-radius:50%;
  background:linear-gradient(135deg,var(--accent-a),var(--accent-b));
  box-shadow:0 6px 16px rgba(236,72,153,0.35);
  margin-top:-5px;
  border:none;
}
input[type="range"]::-moz-range-track{
  height:6px;
  border-radius:999px;
  background:rgba(38,45,92,0.85);
}
input[type="range"]::-moz-range-thumb{
  width:16px;
  height:16px;
  border-radius:50%;
  background:linear-gradient(135deg,var(--accent-a),var(--accent-b));
  box-shadow:0 6px 16px rgba(236,72,153,0.35);
  border:none;
}
select{
  background:rgba(20,24,56,0.85);
  border:1px solid rgba(128,138,206,0.35);
  border-radius:12px;
  color:#d4dcff;
  padding:11px 12px;
  font-size:14px;
}
.kpi{
  display:grid;
  grid-template-columns:repeat(4,1fr);
  gap:12px;
}
.kpi .item{
  background:linear-gradient(160deg,rgba(30,36,78,0.9) 0%,rgba(17,20,44,0.92) 100%);
  padding:14px;
  border-radius:16px;
  border:1px solid var(--stroke);
  box-shadow:0 20px 36px rgba(8,12,32,0.45);
}
.kpi .item b{
  display:block;
  font-size:12px;
  color:#9aa3c7;
  font-weight:600;
}
.kpi .item span{
  font-weight:900;
  font-size:20px;
  color:#f6f7ff;
}
.progress-chart{
  background:linear-gradient(160deg,rgba(30,36,78,0.9) 0%,rgba(17,20,44,0.92) 100%);
  border:1px solid var(--stroke);
  border-radius:16px;
  padding:16px 18px;
  display:flex;
  flex-direction:column;
  gap:12px;
  box-shadow:0 20px 38px rgba(6,8,26,0.5);
}
.progress-chart__header{
  display:flex;
  align-items:center;
  justify-content:space-between;
  gap:12px;
}
.progress-chart__header h3{
  margin:0;
  font-size:14px;
  letter-spacing:0.06em;
  text-transform:uppercase;
  color:#e7ebff;
}
.progress-chart__header .hint{
  display:block;
  margin-top:4px;
}
.progress-chart__body{
  display:flex;
  flex-direction:column;
  gap:12px;
}
.progress-chart__body[hidden]{
  display:none;
}
.progress-chart__canvas{
  width:100%;
  max-width:100%;
  height:160px;
}
.progress-chart svg{
  width:100%;
  height:100%;
}
.progress-chart path.line{
  fill:none;
  stroke-width:2.5;
  stroke-linecap:round;
  stroke-linejoin:round;
}
.progress-chart path.line.reward{
  stroke:var(--accent-a);
}
.progress-chart path.line.fruit{
  stroke:#5ad1a7;
}
.progress-chart__grid line{
  stroke:rgba(139,92,246,0.18);
  stroke-width:1;
}
.progress-chart__grid text{
  font-size:10px;
  fill:var(--muted);
}
.progress-chart__legend{
  display:flex;
  flex-wrap:wrap;
  gap:14px;
  font-size:12px;
  color:#c7cdef;
}
.progress-chart__legend .legend-item{
  display:inline-flex;
  align-items:center;
  gap:6px;
}
.progress-chart__legend .legend-swatch{
  width:10px;
  height:10px;
  border-radius:999px;
  box-shadow:0 0 10px rgba(139,92,246,0.35);
}
.progress-chart__legend .legend-swatch.reward{
  background:var(--accent-a);
}
.progress-chart__legend .legend-swatch.fruit{
  background:#5ad1a7;
}
.progress-chart__meta{
  display:flex;
  justify-content:space-between;
  align-items:center;
  font-size:11px;
  color:var(--muted);
}
.progress-chart__meta .mono{
  color:#c7d2fe;
}
.progress-chart__empty{
  font-size:12px;
  color:var(--muted);
  text-align:center;
  padding:20px 0;
  border:1px dashed rgba(134,144,214,0.35);
  border-radius:12px;
  background:rgba(19,24,54,0.45);
}
.progress-chart.collapsed .progress-chart__header button{
  opacity:0.9;
}
.split{
  display:grid;
  gap:16px;
  grid-template-columns:repeat(auto-fit,minmax(240px,1fr));
}
.telemetry-panel{
  background:linear-gradient(160deg,rgba(24,29,66,0.9) 0%,rgba(14,17,40,0.92) 100%);
  border:1px solid var(--stroke);
  border-radius:16px;
  padding:16px 18px;
  display:flex;
  flex-direction:column;
  gap:12px;
  box-shadow:0 22px 40px rgba(6,8,26,0.5);
}
.telemetry-panel__header{
  display:flex;
  align-items:flex-end;
  justify-content:space-between;
  gap:8px;
}
.telemetry-panel__header h2{
  margin:0;
  font-size:14px;
  color:#dfe3ff;
}
.telemetry-panel__header .hint{
  font-size:11px;
  color:#9aa3c7;
}
.telemetry-summary{
  display:flex;
  justify-content:space-between;
  align-items:center;
  font-size:12px;
  color:#9aa3c7;
}
.telemetry-summary .mono{
  color:#c7d2fe;
}
.telemetry-summary .positive{
  color:#5ad1a7;
}
.telemetry-summary .negative{
  color:#ff8da4;
}
.telemetry-table{
  width:100%;
  border-collapse:collapse;
  font-size:12px;
  color:#c3caef;
}
.telemetry-table thead th{
  text-align:left;
  padding-bottom:6px;
  font-size:10px;
  letter-spacing:.08em;
  text-transform:uppercase;
  color:#7d86c6;
}
.telemetry-table tbody td{
  padding:8px 4px;
  border-top:1px solid rgba(45,52,96,0.75);
}
.telemetry-table tbody tr:first-child td{
  border-top:none;
}
.telemetry-table td.mono{
  font-family:ui-monospace,SFMono-Regular,Menlo,Consolas,monospace;
}
.telemetry-table td.positive{
  color:#5ad1a7;
}
.telemetry-table td.negative{
  color:#ff8da4;
}
.telemetry-table td.neutral{
  color:#c7cdef;
}
.telemetry-table td.share{
  color:#9aa3c7;
}
.telemetry-table td.trend{
  color:#9aa3c7;
}
.telemetry-table td.trend.positive{
  color:#5ad1a7;
}
.telemetry-table td.trend.negative{
  color:#ff8da4;
}
.mono{
  font-family:ui-monospace,SFMono-Regular,Menlo,Consolas,monospace;
  color:#c7d2fe;
  font-size:12px;
}
.hint{
  color:#a7b2e8;
  font-size:12px;
}
.game-card .hint{
  font-size:13px;
  line-height:1.6;
  max-width:520px;
}
.tabs{
  margin-left:32px;
  display:flex;
  gap:24px;
  align-items:center;
}
.tabs button{
  appearance:none;
  border:none;
  padding:8px 0;
  background:transparent;
  color:#9fa8db;
  font-weight:600;
  letter-spacing:0.02em;
  cursor:pointer;
  position:relative;
  transition:.2s color ease;
}
.tabs button::after{
  content:"";
  position:absolute;
  left:0;
  bottom:0;
  width:100%;
  height:3px;
  border-radius:999px;
  background:linear-gradient(135deg,var(--accent-a),var(--accent-b));
  opacity:0;
  transform:scaleX(0.3);
  transform-origin:center;
  transition:.25s opacity ease,.25s transform ease;
}
.tabs button.active{
  color:#f0f2ff;
}
.tabs button.active::after{
  opacity:1;
  transform:scaleX(1);
}
.tabs button:hover{
  color:#fff;
}
.tabs button:focus-visible{
  outline:none;
  color:#fff;
  box-shadow:0 8px 20px rgba(139,92,246,0.35);
}
.tabs button:focus-visible::after{
  opacity:1;
  transform:scaleX(1);
}
.auto-log{
  background:linear-gradient(165deg,rgba(24,29,66,0.9) 0%,rgba(14,17,40,0.92) 100%);
  border:1px solid var(--stroke);
  border-radius:16px;
  padding:14px 18px;
  display:flex;
  flex-direction:column;
  gap:12px;
  font-size:12px;
  color:#c7cdef;
  box-shadow:0 20px 38px rgba(8,10,30,0.45);
}
.auto-log__header{
  display:flex;
  align-items:center;
  gap:10px;
}
.auto-log__header h3{
  margin:0;
  font-size:12px;
  color:#dfe3ff;
  letter-spacing:.08em;
  text-transform:uppercase;
}
.auto-log__stream{
  display:flex;
  flex-direction:column;
  gap:8px;
  max-height:160px;
  overflow-y:auto;
  padding-right:4px;
}
.auto-log__entry{
  background:rgba(23,28,68,0.95);
  border:1px solid rgba(58,68,132,0.6);
  border-left:3px solid var(--accent-a);
  border-radius:12px;
  padding:10px 12px;
  display:flex;
  flex-direction:column;
  gap:3px;
  line-height:1.4;
}
.auto-log__entry--board{border-left-color:var(--accent-b);}
.auto-log__entry--epsilon{border-left-color:#5ad1a7;}
.auto-log__entry--lr{border-left-color:#f9c74f;}
.auto-log__entry--reward{border-left-color:#ff8da4;}
.auto-log__entry--summary{border-left-color:#8d99ff;}
.auto-log__entry--info{border-left-color:var(--accent-a);}
.auto-log__entry--ai{border-left-color:#22d3ee;}
.auto-log__entry--error{border-left-color:var(--danger);}
.auto-log__title{
  font-weight:600;
  color:#e5e9ff;
}
.auto-log__detail{color:#c3caef;}
.auto-log__metrics{
  font-family:ui-monospace,SFMono-Regular,Menlo,Consolas,monospace;
  font-size:11px;
  color:#9aa3c7;
}
.auto-log__tag{
  align-self:flex-end;
  font-size:10px;
  color:#a0a8dc;
  background:#1e2450;
  border-radius:999px;
  padding:2px 8px;
  margin-top:4px;
}
button.micro{
  padding:4px 8px;
  font-size:11px;
  border-radius:6px;
  box-shadow:none;
}
details{
  background:linear-gradient(170deg,rgba(24,29,66,0.92) 0%,rgba(14,17,40,0.94) 100%);
  border-radius:18px;
  border:1px solid var(--stroke);
  padding:16px 18px;
  box-shadow:0 20px 38px rgba(6,8,24,0.45);
}
details[open]{
  padding-bottom:16px;
}
details summary{
  cursor:pointer;
  font-weight:700;
  color:#f0f2ff;
  margin:-16px -18px 18px;
  padding:16px 18px;
  border-radius:18px;
  background:rgba(26,32,74,0.35);
}
details summary::-webkit-details-marker{display:none}
.stack{
  display:flex;
  flex-direction:column;
  gap:10px;
  margin-bottom:14px;
}
.stack h3{
  margin:0;
  font-size:13px;
  color:#dfe3ff;
  letter-spacing:.01em;
  text-transform:uppercase;
}
.row{
  display:flex;
  flex-wrap:wrap;
  gap:16px;
  align-items:flex-end;
}
.row label{
  display:flex;
  flex-direction:column;
  gap:4px;
}
.hidden{display:none!important}
#guideView{
  max-width:900px;
  margin:24px auto 60px;
  padding:0 16px;
  display:flex;
  flex-direction:column;
  gap:16px;
}
#guideView .card{
  padding:24px;
}
#guideView h2{
  font-size:20px;
  margin-bottom:12px;
}
#guideView h3{
  margin-top:20px;
  margin-bottom:8px;
  color:#dfe3ff;
}
#guideView p,#guideView li{
  color:#c7cdef;
}
#guideView ul{
  padding-left:20px;
  margin:8px 0;
}
footer{
  opacity:.7;
  text-align:center;
  padding:18px;
  font-size:12px;
}
  main.layout{
    grid-template-columns:minmax(0,560px) minmax(0,1fr);
  }
  .control-card{
    grid-column:2;
  }
  .tuning-card{
    grid-column:2;
    grid-row:2;
  }
}
@media(max-width:1050px){
  main.layout{
    grid-template-columns:1fr;
  }
  .game-card,
  .control-card,
  .tuning-card{
    grid-column:1;
  }
  canvas#board{
    width:100%;
    height:auto;
  }
  .controls.secondary{
    flex-direction:column;
    align-items:flex-start;
    gap:12px;
  }
  .status-group{
    justify-content:flex-end;
  }
}
@media(max-width:640px){
  header .header-inner{
    padding:0 16px;
    flex-wrap:wrap;
    gap:12px;
  }
  .status-group{
    width:100%;
    justify-content:flex-start;
  }
  .tabs{
    margin-left:0;
    width:100%;
    justify-content:flex-start;
  }
  .controls.primary{
    flex-direction:column;
    align-items:stretch;
  }
  .controls.primary button{
    min-width:0;
    width:100%;
  }
  input[type="range"]{
    width:160px;
  }
}
</style>
</head>
<body>
<header>
  <div class="header-inner">
    <div class="logo"><span class="logo-text">Snake Simulation</span></div>
    <div class="status-group">
      <span class="badge" id="trainState">idle</span>
      <span class="badge" id="algoBadge">Dueling DQN</span>
      <span class="badge">ε <span id="epsReadout">1.00</span></span>
      <span class="badge">γ <span id="gammaBadge">0.98</span></span>
      <span class="badge">LR <span id="lrBadge">0.0005</span></span>
    </div>
    <nav class="tabs">
      <button type="button" id="tabTraining" class="active">Training</button>
      <button type="button" id="tabGuide">Guide</button>
    </nav>
  </div>
</header>

<main id="trainingView" class="layout">
  <section class="card game-card" id="simulationCard">
    <div class="card-head">
      <div>
        <h2>Snake Simulation</h2>
        <p class="subtitle">Smooth rendering for reinforcement learning experiments.</p>
      </div>
      <div class="card-actions">
        <span class="badge soft" id="playbackLabel">Smooth realtime</span>
        <button id="btnToggleLiveView" class="secondary" type="button">Hide live view</button>
      </div>
    </div>
    <canvas id="board" width="500" height="500"></canvas>
    <div class="controls primary">
      <button id="btnTrain">▶ Start training</button>
      <button id="btnPause" class="secondary">⏸ Pause</button>
      <button id="btnStep" class="secondary">Step one episode</button>
      <button id="btnWatch" class="secondary">Watch</button>
      <button id="btnReset" class="secondary">Reset</button>
    </div>
    <div class="controls secondary">
      <div class="pill-group" id="playbackGroup" role="group" aria-label="Playback mode">
        <button type="button" class="pill active" data-speed="cinematic">Smooth</button>
        <button type="button" class="pill" data-speed="fast">Fast</button>
        <button type="button" class="pill" data-speed="turbo">Turbo</button>
      </div>
      <div class="field compact">
        <label for="gridSize">Board size</label>
        <input type="range" id="gridSize" min="10" max="30" step="2" value="20">
        <span class="mono" id="gridLabel">20×20</span>
      </div>
    </div>
  </section>

  <section class="card control-card">
    <div class="card-head">
      <h2>Learning</h2>
      <div class="card-actions">
        <button id="btnCheckpointToggle" class="secondary" type="button">Enable Checkpoint Save</button>
        <button id="btnSave" class="secondary">Save</button>
        <button id="btnLoad" class="secondary">Load</button>
        <button id="btnClear" class="danger">Clear cache</button>
      </div>
    </div>
    <div class="controls tertiary">
      <div class="pill-group" id="modeGroup" role="group" aria-label="Training mode">
        <button type="button" class="pill active" data-mode="manual">Manual</button>
        <button type="button" class="pill" data-mode="auto">Auto</button>
      </div>
      <span class="hint">Auto handles the curriculum, save logic, and hyperparameters for you.</span>
    </div>
    <div class="field block">
      <label for="algoSelect">Algorithm</label>
      <select id="algoSelect">
        <option value="dueling">Dueling Double DQN</option>
        <option value="vanilla">Classic DQN</option>
        <option value="policy">Policy Gradient (REINFORCE)</option>
        <option value="a2c">Advantage Actor-Critic</option>
        <option value="ppo">Proximal Policy Optimization</option>
      </select>
    </div>
    <div class="field block hidden" id="agentPresetField">
      <label for="presetSelect">Agent preset</label>
      <select id="presetSelect"></select>
    </div>
    <div class="field block">
      <label for="stagePresetSelect">Stage preset</label>
      <select id="stagePresetSelect"></select>
    </div>
    <p class="hint" id="algoDescription">Prioritized replay, n-step returns, and dueling heads make DQN stable and sample efficient.</p>

    <div class="ai-auto-tune" id="aiAutoTunePanel">
      <div class="ai-auto-tune__header">
        <div>
          <h3>AI Auto-Tune</h3>
          <p class="hint">LLM-analys justerar belöningar och hyperparametrar för att maximera överlevnad och frukt.</p>
        </div>
      </div>

      <div class="ai-auto-tune__controls" role="group" aria-label="AI Auto-Tune inställningar">
        <section class="ai-auto-tune__column" aria-labelledby="aiAutoTuneAiHeading">
          <h4 id="aiAutoTuneAiHeading">AI-analys</h4>
          <div class="field compact ai-interval-field">
            <div class="field-header">
              <label for="aiIntervalSlider">Analysintervall</label>
              <label class="toggle" for="aiAutoTuneToggle">
                <input type="checkbox" id="aiAutoTuneToggle" aria-label="Aktivera AI Auto-Tune">
                <span>Aktivera</span>
              </label>
            </div>
            <input type="range" id="aiIntervalSlider" min="100" max="5000" step="100" value="500">
            <span class="mono" id="aiIntervalReadout">500 ep</span>
          </div>
        </section>
        <section class="ai-auto-tune__column" aria-labelledby="aiAutoTuneAutoHeading">
         
          <h4 id="aiAutoTuneAutoHeading">Auto-körning</h4>
          <div class="field compact">
            <label for="aiRunLimit">Rundor att köra</label>
            <input type="number" id="aiRunLimit" min="0" step="100" inputmode="numeric" placeholder="0">
            <span class="hint" id="aiRunLimitHint">0 = obegränsat</span>
          </div>
         <div class="field">
            <span class="hint">Justeringstempo</span>
            <div class="pill-group" id="aiStyleGroup" role="group" aria-label="Justeringstempo">
              <button type="button" class="pill" data-style="calm">Lugn</button>
              <button type="button" class="pill active" data-style="balanced">Medel</button>
              <button type="button" class="pill" data-style="aggressive">Aggressiv</button>
            </div>
          </div>
        </section>
      </div>
    </div>

    <div id="autoLogPanel" class="auto-log hidden">
      <div class="auto-log__header">
        <h3>Auto adjustments</h3>
        <button type="button" id="autoLogClear" class="secondary micro">Clear</button>
      </div>
      <div id="autoLogStream" class="auto-log__stream" role="log" aria-live="polite"></div>
    </div>

    <div class="kpi">
      <div class="item"><b>Episodes</b><span id="kEpisodes">0</span></div>
      <div class="item"><b>Avg reward (100)</b><span id="kAvgRw">0.0</span></div>
      <div class="item"><b>Best length</b><span id="kBest">0</span></div>
      <div class="item"><b>Fruit / ep</b><span id="kFruitRate">0.0</span></div>
    </div>

    <div class="progress-chart" id="progressChartPanel">
      <div class="progress-chart__header">
        <div>
          <h3>Träningsprogress</h3>
          <span class="hint">Medel per 100 episoder</span>
        </div>
        <button type="button" class="secondary micro" id="progressChartToggle" aria-expanded="true" aria-controls="progressChartBody">Dölj diagram</button>
      </div>
      <div class="progress-chart__body" id="progressChartBody">
        <div class="progress-chart__canvas" id="progressChartCanvas">
          <svg id="progressChartSvg" viewBox="0 0 360 160" role="img" aria-label="Linje-diagram över medelbelöning och frukt per 100 episoder">
            <g id="progressChartGrid" class="progress-chart__grid"></g>
            <path id="progressRewardPath" class="line reward" d=""></path>
            <path id="progressFruitPath" class="line fruit" d=""></path>
          </svg>
        </div>
        <div class="progress-chart__empty" id="progressChartEmpty">Kör minst 100 episoder för att se trenderna.</div>
        <div class="progress-chart__legend" id="progressChartLegend">
          <span class="legend-item"><span class="legend-swatch reward"></span>Medelbelöning</span>
          <span class="legend-item"><span class="legend-swatch fruit"></span>Medel frukt</span>
        </div>
        <div class="progress-chart__meta" id="progressChartMeta">
          <span class="hint">Episoder</span>
          <span class="mono" id="progressChartRange">—</span>
        </div>
      </div>
    </div>

    <div class="split charts">
      <div class="telemetry-panel" id="rewardTelemetryPanel">
        <div class="telemetry-panel__header">
          <h2>Reward telemetry</h2>
          <span class="hint">Rolling averages per component</span>
        </div>
        <div class="telemetry-summary">
          <span class="hint">Net reward (avg 100)</span>
          <span id="rewardTelemetrySummary" class="mono neutral">0.00 (trend +0.00)</span>
        </div>
        <table class="telemetry-table">
          <thead>
            <tr>
              <th>Component</th>
              <th>Last</th>
              <th>Avg 100</th>
              <th>Avg 500</th>
              <th>Share</th>
              <th>Trend</th>
            </tr>
          </thead>
          <tbody id="rewardTelemetryBody"></tbody>
        </table>
      </div>
    </div>
  </section>

  <section class="card tuning-card">
    <div class="card-head">
      <h2>Reward &amp; tuning</h2>
      <span class="hint">Adjust incentives and hyperparameters.</span>
    </div>

    <details id="rewardPanel" open>
      <summary>Reward model</summary>
      <div class="stack">
        <h3>Tempo &amp; direction</h3>
        <div class="row">
          <label>Step penalty
            <input type="range" id="rewardStep" min="0" max="0.05" step="0.001" value="0.010">
            <span class="mono" id="rewardStepReadout">0.010</span>
          </label>
          <label>Turn penalty
            <input type="range" id="rewardTurn" min="0" max="0.02" step="0.001" value="0.001">
            <span class="mono" id="rewardTurnReadout">0.001</span>
          </label>
          <label>Toward fruit bonus
            <input type="range" id="rewardApproach" min="0" max="0.1" step="0.005" value="0.030">
            <span class="mono" id="rewardApproachReadout">0.030</span>
          </label>
        </div>
        <div class="row">
          <label>Away from fruit penalty
            <input type="range" id="rewardRetreat" min="0" max="0.1" step="0.005" value="0.030">
            <span class="mono" id="rewardRetreatReadout">0.030</span>
          </label>
        </div>
      </div>
      <div class="stack">
        <h3>Loops &amp; revisits</h3>
        <div class="row">
          <label>Loop penalty
            <input type="range" id="rewardLoop" min="0" max="1" step="0.01" value="0.50">
            <span class="mono" id="rewardLoopReadout">0.50</span>
          </label>
          <label>Revisit penalty
            <input type="range" id="rewardRevisit" min="0" max="0.1" step="0.001" value="0.050">
            <span class="mono" id="rewardRevisitReadout">0.050</span>
          </label>
        </div>
      </div>
      <div class="stack">
        <h3>Crashes &amp; stalling</h3>
        <div class="row">
          <label>Wall crash
            <input type="range" id="rewardWall" min="0" max="30" step="0.5" value="10">
            <span class="mono" id="rewardWallReadout">10.0</span>
          </label>
          <label>Self crash
            <input type="range" id="rewardSelf" min="0" max="30" step="0.5" value="25.5">
            <span class="mono" id="rewardSelfReadout">25.5</span>
          </label>
          <label>Timeout penalty
            <input type="range" id="rewardTimeout" min="0" max="20" step="0.5" value="5">
            <span class="mono" id="rewardTimeoutReadout">5.0</span>
          </label>
        </div>
        <div class="row">
          <label>Trap penalty
            <input type="range" id="rewardTrap" min="0" max="2" step="0.05" value="0.50">
            <span class="mono" id="rewardTrapReadout">0.50</span>
          </label>
          <label>Open space bonus
            <input type="range" id="rewardSpace" min="0" max="0.2" step="0.01" value="0.05">
            <span class="mono" id="rewardSpaceReadout">0.05</span>
          </label>
        </div>
      </div>
      <div class="stack">
        <h3>High-impact rewards</h3>
        <div class="row">
          <label>Fruit reward
            <input type="range" id="rewardFruit" min="0" max="30" step="0.5" value="10">
            <span class="mono" id="rewardFruitReadout">10.0</span>
          </label>
          <label>Tillväxtbonus
            <input type="range" id="rewardGrowth" min="0" max="5" step="0.1" value="1.0">
            <span class="mono" id="rewardGrowthReadout">1.0</span>
          </label>
          <label>Compactness bonus
            <input type="range" id="rewardCompact" min="0" max="0.1" step="0.001" value="0.000">
            <span class="mono" id="rewardCompactReadout">0.000</span>
          </label>
        </div>
      </div>
    </details>

    <details id="advancedPanel" open>
      <summary>Advanced settings</summary>
      <div class="stack" data-config="shared">
        <h3>Shared</h3>
        <div class="row">
          <label>Parallel environments
            <input type="range" id="envCount" min="1" max="24" step="1" value="1">
            <span class="mono" id="envCountReadout">1</span>
          </label>
          <label>γ (discount)
            <input type="range" id="gamma" min="0.90" max="0.999" step="0.001" value="0.98">
            <span class="mono" id="gammaReadout">0.98</span>
          </label>
          <label>LR
            <input type="range" id="lr" min="0.0001" max="0.005" step="0.0001" value="0.0005">
            <span class="mono" id="lrReadout">0.0005</span>
          </label>
        </div>
      </div>
      <div class="stack" data-config="dqn">
        <h3>DQN family</h3>
        <div class="row">
          <label>ε start
            <input type="range" id="epsStart" min="0.2" max="1.0" step="0.05" value="1.0">
            <span class="mono" id="epsStartReadout">1.00</span>
          </label>
          <label>ε end
            <input type="range" id="epsEnd" min="0.01" max="0.3" step="0.01" value="0.12">
            <span class="mono" id="epsEndReadout">0.12</span>
          </label>
          <label>ε decay (steps)
            <input type="range" id="epsDecay" min="5000" max="200000" step="5000" value="80000">
            <span class="mono" id="epsDecayReadout">80000</span>
          </label>
        </div>
        <div class="row">
          <label>Batch
            <input type="range" id="batchSize" min="32" max="512" step="32" value="128">
            <span class="mono" id="batchReadout">128</span>
          </label>
          <label>Replay size
            <input type="range" id="bufferSize" min="5000" max="200000" step="5000" value="50000">
            <span class="mono" id="bufferReadout">50000</span>
          </label>
          <label>Target sync (steps)
            <input type="range" id="targetSync" min="500" max="10000" step="500" value="2000">
            <span class="mono" id="targetSyncReadout">2000</span>
          </label>
        </div>
        <div class="row">
          <label>n-step
            <input type="range" id="nStep" min="1" max="5" step="1" value="3">
            <span class="mono" id="nStepReadout">3</span>
          </label>
          <label>PER α
            <input type="range" id="priorityAlpha" min="0.1" max="1" step="0.05" value="0.6">
            <span class="mono" id="alphaReadout">0.60</span>
          </label>
          <label>PER β
            <input type="range" id="priorityBeta" min="0.1" max="1" step="0.05" value="0.4">
            <span class="mono" id="betaReadout">0.40</span>
          </label>
        </div>
      </div>
      <div class="stack hidden" data-config="policy">
        <h3>Policy Gradient</h3>
        <div class="row">
          <label>Entropy weight
            <input type="range" id="pgEntropy" min="0" max="0.05" step="0.001" value="0.01">
            <span class="mono" id="pgEntropyReadout">0.010</span>
          </label>
        </div>
      </div>
      <div class="stack hidden" data-config="a2c">
        <h3>Actor-Critic</h3>
        <div class="row">
          <label>Entropy weight
            <input type="range" id="acEntropy" min="0" max="0.05" step="0.001" value="0.005">
            <span class="mono" id="acEntropyReadout">0.005</span>
          </label>
          <label>Value weight
            <input type="range" id="acValueCoef" min="0.1" max="1.0" step="0.05" value="0.5">
            <span class="mono" id="acValueCoefReadout">0.50</span>
          </label>
        </div>
      </div>
      <div class="stack hidden" data-config="ppo">
        <h3>PPO</h3>
        <div class="row">
          <label>Entropy weight
            <input type="range" id="ppoEntropy" min="0" max="0.05" step="0.001" value="0.003">
            <span class="mono" id="ppoEntropyReadout">0.003</span>
          </label>
          <label>Clip factor
            <input type="range" id="ppoClip" min="0.05" max="0.4" step="0.01" value="0.2">
            <span class="mono" id="ppoClipReadout">0.20</span>
          </label>
          <label>GAE λ
            <input type="range" id="ppoLambda" min="0.5" max="1.0" step="0.01" value="0.95">
            <span class="mono" id="ppoLambdaReadout">0.95</span>
          </label>
        </div>
        <div class="row">
          <label>Batch
            <input type="range" id="ppoBatch" min="32" max="512" step="32" value="256">
            <span class="mono" id="ppoBatchReadout">256</span>
          </label>
          <label>Epochs
            <input type="range" id="ppoEpochs" min="1" max="10" step="1" value="4">
            <span class="mono" id="ppoEpochsReadout">4</span>
          </label>
          <label>Value weight
            <input type="range" id="ppoValueCoef" min="0.1" max="1.0" step="0.05" value="0.5">
            <span class="mono" id="ppoValueCoefReadout">0.50</span>
          </label>
        </div>
      </div>
    </details>
  </section>
</main>

<section id="guideView" class="hidden">
  <div class="card">
    <h2>Training primer – overview</h2>
    <p>This primer explains how the training environment works and why the different reinforcement learning strategies in Snake-ML behave the way they do. Keep it alongside the controls so you can make confident tweaks while you experiment.</p>
    <ul>
      <li><strong>Background:</strong> what happens on the board and how rewards are generated.</li>
      <li><strong>Algorithms:</strong> theoretical explanations of DQN, policy gradient, A2C, and PPO.</li>
      <li><strong>Sliders:</strong> practical guidance for every control and how to adjust it.</li>
    </ul>
  </div>

  <div class="card">
    <h2>AI Auto-Tune data &amp; loggning</h2>
    <p>Auto-Tune skickar inte bara senaste episoden till LLM: varje avslutad episod lagras i ett lokalt minne (<code>aiEpisodeHistory</code>) med plats för 6&nbsp;000 poster innan de äldsta kastas. Därifrån byggs flera vyer upp <em>innan</em> telemetrin serialiseras:</p>
    <ul>
      <li><strong>Intervallfönster:</strong> ett glidande fönster på <span class="mono">interval</span> episoder (standard 500) sammanfattar medelvärden, spridning, trender och kraschningsfördelning.</li>
      <li><strong>Rollup:</strong> en separat summering över upp till 1&nbsp;000 episoder används som långtidsreferens.</li>
      <li><strong>Senaste körningar:</strong> de 32 färskaste episoderna samt den allra senaste episodens detaljer tas med för att visa kortsiktiga svängningar.</li>
    </ul>
    <p>Resultatet packas i samma <code>messagePayload</code> tillsammans med aktuell belöningskonfiguration och hyperparametrar, så modellen kan väga både långsiktiga trender och senaste händelserna.</p>
    <p>Varje prompt/svar-par och checkpoint markerade episoder loggas dessutom till <code>api/logs/snake-history.jsonl</code> via proxyn. Filen roteras automatiskt när den närmar sig 8&nbsp;MB, så du får en komplett historik utan risk att den checkas in i Git (filtypen ignoreras i <code>.gitignore</code>).</p>
    <h3>Så tolkas modellens svar</h3>
    <ul>
      <li>LLM:en instrueras att svara med JSON som innehåller <code>rewardConfig</code>, <code>hyper</code> och <code>analysisText</code>.</li>
      <li>Klienten plockar ut första JSON-objektet i svaret och försöker reparera vanliga formateringsfel (t.ex. felciterade nycklar, snygga citattecken eller släpande kommatecken) innan det parsas.</li>
      <li>Endast giltiga numeriska förändringar appliceras. Allt annat loggas i Auto adjustments-flödet, så du ser exakt vilka justeringar som användes eller avvisades.</li>
    </ul>
    <p>Vill du låta modellen analysera längre spann? Dra reglaget <strong>Avslutade episoder</strong> i AI Auto-Tune-panelen. Reglaget stegar i hundratal och går mellan 100 och 5&nbsp;000 episoder.</p>
  </div>

  <div class="card">
    <h2>Quickstart</h2>
    <ol>
      <li>Select a playback mode in the <em>Smooth/Fast/Turbo</em> pill group. Smooth shows every move and is perfect for studying behaviours, while Turbo skips rendering for maximum training speed.</li>
      <li>Press <strong>Start training</strong>. The KPI tiles show how reward and length evolve.</li>
      <li>Adjust one slider at a time. Switch to <strong>Step one episode</strong> to see the effect of a choice without letting the model keep learning.</li>
      <li>Save your parameters with <strong>Save</strong>. The downloaded file can be imported again with <strong>Load</strong>.</li>
    </ol>
    <p>Board size controls how many tiles the snake can work with. Larger boards mean longer distances between fruit and more complex strategies, but they also require more training steps.</p>
  </div>

  <div class="card">
    <h2>Algorithms and why they work</h2>
    <details open>
      <summary>Dueling Double DQN</summary>
      <p>Dueling Double DQN is the default. The network splits the state value V(s) and the advantage A(s,a) and combines them into <code>Q(s,a) = V(s) + A(s,a) - ar{A}(s)</code>. Double updates use the online network to pick the action but the target network to evaluate it, reducing overestimation.</p>
      <ul>
        <li><strong>Dueling architecture:</strong> helpful when some actions barely change the outcome, such as continuing straight in empty corridors.</li>
        <li><strong>Double networks:</strong> the <code>targetSync</code> slider controls how often the target network is copied, keeping learning stable.</li>
        <li><strong>Prioritized replay:</strong> the buffer samples experiences with high TD error more often. The <strong>PER α</strong> and <strong>PER β</strong> sliders tune how strong that priority is.</li>
        <li><strong>n-step returns:</strong> summarise rewards across several moves to deliver faster feedback when the snake is heading toward fruit.</li>
      </ul>
      <p>Recommended starting point: LR 5e-4, γ = 0.98, batch 128, and replay 50&nbsp;000. Increase <strong>ε decay</strong> if you need more exploration in larger worlds.</p>
    </details>
    <details>
      <summary>Classic DQN</summary>
      <p>The classic DQN uses a simpler Q-network without the dueling head. Its update follows Bellman's equation <code>Q(s,a) \leftarrow r + γ · \max_{a'} Q_{target}(s',a')</code>. It is easy to reason about and works well on smaller boards.</p>
      <ul>
        <li><strong>Epsilon-greedy:</strong> uses the same ε start/end/decay sliders. A high start produces more random moves in the beginning.</li>
        <li><strong>Replay buffer:</strong> keep the buffer large enough (≥ 20&nbsp;000) to avoid correlated experiences.</li>
        <li><strong>n-step:</strong> can be reduced to 1 if you want to compare with the original DQN paper.</li>
      </ul>
      <p>If the loss curve oscillates heavily, lower the learning rate or increase the target sync interval.</p>
    </details>
    <details>
      <summary>Policy Gradient (REINFORCE)</summary>
      <p>Policy gradient learns a direct policy π(a|s) and maximises the expected return's log probability: <code>∇θ J(θ) = E[ G_t ∇θ log π_θ(a_t|s_t) ]</code>. The variance is high, so a baseline equal to the average return is subtracted.</p>
      <ul>
        <li><strong>Entropy weight:</strong> higher values encourage more random policies and prevent the model from locking in too early.</li>
        <li>No replay buffer is used; each episode updates the weights directly. Run Turbo mode to gather more episodes.</li>
        <li>Keep the learning rate around 1e-3. If reward swings, lower LR or increase the entropy weight slightly.</li>
      </ul>
    </details>
    <details>
      <summary>Advantage Actor-Critic</summary>
      <p>A2C trains two networks simultaneously: the actor learns π(a|s) while the critic estimates V(s). Updates use the advantage A(s,a) = Q(s,a) − V(s) to reduce variance.</p>
      <ul>
        <li><strong>Entropy weight:</strong> similar effect as in policy gradient but typically lower (0.003–0.01).</li>
        <li><strong>Value weight:</strong> controls how strongly the critic's MSE loss influences the total loss. High weight stabilises learning but can slow the policy.</li>
        <li>The shared γ and LR sliders apply here as well. γ around 0.99 helps the model plan several steps ahead.</li>
      </ul>
      <p>A2C is quick when episodes are short. If the critic loss diverges, reduce the value weight or learning rate.</p>
    </details>
    <details>
      <summary>Proximal Policy Optimization</summary>
      <p>PPO uses a clipped objective: <code>L_{clip}(θ) = E[\min(r_t(θ)A_t, \operatorname{clip}(r_t(θ), 1-ε, 1+ε) A_t)]</code>. This prevents updates from pushing the policy too far in a single step.</p>
      <ul>
        <li><strong>Clip factor:</strong> corresponds to ε in the formula. Lower values (~0.1) yield cautious updates; higher values (~0.3) allow more change.</li>
        <li><strong>GAE λ:</strong> balances bias and variance in generalised advantage estimation. 0.95 is a solid default; lower it for more responsiveness.</li>
        <li><strong>Batch</strong> and <strong>Epochs:</strong> PPO performs several gradient steps per collected batch. Smaller batches with more epochs adapt quickly but risk overfitting the data.</li>
        <li><strong>Value weight</strong> and <strong>Entropy weight:</strong> influence the same components as in A2C but inside PPO's combined loss.</li>
      </ul>
      <p>If the policy becomes unstable, reduce the clip factor or the number of epochs per batch.</p>
    </details>
  </div>

  <div class="card">
    <h2>Reward guide</h2>
    <p>The reward model determines how the snake values every step. Adjust the sliders in the <em>Reward model</em> panel to fine-tune the strategy without retraining from scratch.</p>
    <h3>Tempo &amp; direction</h3>
    <ul>
      <li><strong>Step penalty:</strong> base cost per move (default 0.01). Raise it for shorter, more purposeful routes.</li>
      <li><strong>Turn penalty:</strong> extra cost when the snake turns. Lower it if you want the model to try more small corrections.</li>
      <li><strong>Toward/Away from fruit:</strong> the bonus for closing the distance to the fruit and the penalty for increasing it. Keep the values similar for symmetric feedback.</li>
    </ul>
    <h3>Loops &amp; revisits</h3>
    <ul>
      <li><strong>Loop penalty:</strong> triggers when the history shows left/right loops (pattern 1,2,1,2). Increase it if the snake often gets stuck in figure-eights.</li>
      <li><strong>Revisit penalty:</strong> multiplied by how recently a tile was visited. Higher values push the snake to explore fresh space.</li>
    </ul>
    <h3>Crashes &amp; stalling</h3>
    <ul>
      <li><strong>Wall crash:</strong> penalty when the head leaves the board.</li>
      <li><strong>Self crash:</strong> penalty when the snake bites itself.</li>
      <li><strong>Timeout penalty:</strong> applied if no fruit is collected for two full board areas worth of moves. Prevents endless loops.</li>
    </ul>
    <h3>High-impact rewards</h3>
    <ul>
      <li><strong>Fruit reward:</strong> the main reward when a fruit is eaten. Raise it for more aggressive fruit chasing.</li>
      <li><strong>Compactness bonus:</strong> gives a small bonus when the occupied area becomes denser (lower difference between the bounding box and snake length). Increase it late in training if you want the body packed tightly.</li>
    </ul>
    <h3>Reading the reward telemetry</h3>
    <ul>
      <li><strong>Columns:</strong> <em>Last</em> shows the latest episode, <em>Avg 100</em>/<em>Avg 500</em> are rolling means, <em>Trend</em> compares the most recent 100 episodes with the previous 100, and <em>Share</em> normalises each component against the absolute total so you can spot dominant contributors.</li>
      <li><strong>Net reward row:</strong> sums every component; a positive trend confirms that training is heading in the right direction.</li>
      <li><strong>Component labels:</strong> bonuses tagged as positive (fruit, approach, open space) should stay ≥ 0, whereas penalties (step, loop, revisit, crashes) normally sit ≤ 0. Large negative shares mean the snake spends much of its time incurring that cost.</li>
      <li><strong>Healthy patterns:</strong> rising fruit/approach bonuses combined with shrinking loop/revisit penalties signal better navigation. Persistent step penalties dominating the share column indicate wandering without progress.</li>
      <li><strong>Warning signs:</strong> a falling fruit trend or growing crash penalties hint at stagnation. Track the 100-episode averages to confirm whether the issue is a blip or a sustained regression.</li>
    </ul>
    <h3>Automatic reward tuning thresholds</h3>
    <p>The auto-scheduler tweaks sliders when telemetry crosses strict limits. Manual overrides are still available, but these rules explain why certain values change on their own:</p>
    <ul>
      <li><strong>Loop penalty:</strong> increased when <em>LoopHitRate</em> (loop detections ÷ total steps over the last 500 episodes) exceeds 1 % <em>and</em> the fruit trend stalls or turns negative. At the same time the compactness bonus is disabled to avoid encouraging tight spirals.</li>
      <li><strong>Revisit penalty:</strong> raised once <em>RevisitRate</em> (recent-tile penalties ÷ total steps across 500 episodes) climbs beyond 1 %, pushing the policy toward fresh territory.</li>
      <li><strong>Self crash penalty:</strong> boosted only if self-collisions account for more than 40 % of episode endings, signalling that survivability has become the main issue.</li>
      <li><strong>Approach/retreat weights:</strong> adjusted together when the average time-to-fruit stays high while both loop and revisit rates are low, nudging the snake to pursue fruit more assertively.</li>
      <li><strong>Metric windows:</strong> all trigger checks use the latest 500-episode aggregates, so brief spikes rarely trip them. If a slider never moves automatically, its condition likely has not been met.</li>
    </ul>
  </div>

  <div class="card">
    <h2>Sliders &amp; how they affect training</h2>
    <h3>Environment &amp; playback</h3>
    <ul>
      <li><strong>Playback mode:</strong> <em>Smooth</em> shows every move, <em>Fast</em> skips every other frame, and <em>Turbo</em> disables rendering. All modes continue updating the network.</li>
      <li><strong>Board size:</strong> 10×10 yields short episodes and quick feedback. 30×30 requires longer episodes but rewards planning.</li>
    </ul>
    <h3>Shared hyperparameters</h3>
    <ul>
      <li><strong>γ (discount):</strong> high values (0.97–0.995) prioritise long-term fruit. Lower to 0.94–0.96 if the snake often crashes before reaching reward.</li>
      <li><strong>LR:</strong> sets the gradient step size. 0.0005 works for DQN; drop toward 0.0003 for PPO/A2C if the loss oscillates.</li>
    </ul>
    <h3>DQN family</h3>
    <ul>
      <li><strong>ε start/end/decay:</strong> control exploration. Longer decay (≥ 80&nbsp;000) yields a slow transition to exploitation. Raise the end value (e.g. 0.10) if the snake loops in repetitive patterns.</li>
      <li><strong>Batch:</strong> larger batches reduce variance but need bigger buffers. 256 requires at least 100&nbsp;000 replay entries.</li>
      <li><strong>Replay size:</strong> 50&nbsp;000 by default. Increase it for large boards, but remember old experiences may become irrelevant.</li>
      <li><strong>Target sync:</strong> how often the policy network is copied to the target network. Lower values (1000) adapt quickly but can become unstable.</li>
      <li><strong>n-step:</strong> more steps give stronger signals but mixed rewards can add noise. Try 2–3 for small boards and 4–5 for large ones.</li>
      <li><strong>PER α:</strong> how strongly TD error affects prioritisation. Higher (0.8–1.0) focuses on hard experiences; drop to 0.5 for more variety.</li>
      <li><strong>PER β:</strong> corrects the bias introduced by prioritisation. Increase it gradually toward 1.0 during long training runs.</li>
    </ul>
    <h3>Policy-based methods</h3>
    <ul>
      <li><strong>Entropy weight (Policy/A2C/PPO):</strong> lower values give decisive policies, higher values prevent premature convergence. Reduce it once the model finds a stable strategy.</li>
      <li><strong>Value weight (A2C/PPO):</strong> controls how much the value loss matters. High weight helps the critic track long-term rewards.</li>
      <li><strong>Clip factor (PPO):</strong> keep between 0.1–0.25 for stability. Pair it with a lower LR if you increase it.</li>
      <li><strong>GAE λ (PPO):</strong> lower (0.90) reacts faster to new signals; higher (0.97) yields smoother estimates.</li>
      <li><strong>PPO Batch/Epochs:</strong> more epochs on the same data risk overfitting. If the policy swings, lower the epochs or increase the batch size.</li>
    </ul>
    <p>Always change one slider at a time and monitor the reward and loss charts. When the 100-episode reward average levels out, try an adjustment, run a few hundred episodes, and compare.</p>
  </div>

  <div class="card">
    <h2>Diagnostics and common patterns</h2>
    <ul>
      <li><strong>Reward drops after rising:</strong> raise ε end or the entropy weight to reintroduce exploration.</li>
      <li><strong>Loss explodes:</strong> lower the learning rate, increase the target sync interval, or reduce the batch size.</li>
      <li><strong>The snake loops in circles:</strong> try higher entropy (policy/A2C/PPO) or a longer ε decay in DQN so the model dares to break the pattern.</li>
      <li><strong>No improvement on large boards:</strong> increase n-step and replay size, and run Turbo mode to gather more experience.</li>
      <li><strong>Big differences between episodes:</strong> inspect the KPI tiles. If average reward is low but best length high it signals an unstable policy — adjust exploration or lower LR.</li>
    </ul>
    <p>Use <strong>Pause</strong> and <strong>Step one episode</strong> to analyse individual sequences. <strong>Reset</strong> restarts the episode, while <strong>Clear cache</strong> removes stored weights from the browser if you want a fresh start.</p>
  </div>
</section>

<input type="file" id="fileLoader" accept="application/json" hidden>

<footer class="hint">© Marcus — Snake learns with multiple RL strategies and cinematic movement.</footer>

<script>
  // Viktigt: ange Render-basen här
  window.API_BASE_URL = "https://snake-ml.onrender.com";
</script>

<script type="module" src="hf-tuner.js"></script>
<script type="module">
import {createAITuner} from './hf-tuner.js';

const REWARD_DEFAULTS={
  stepPenalty:0.01,
  turnPenalty:0.001,
  approachBonus:0.03,
  retreatPenalty:0.03,
  loopPenalty:0.50,
  revisitPenalty:0.05,
  wallPenalty:10,
  selfPenalty:25.5,
  timeoutPenalty:5,
  fruitReward:10,
  growthBonus:1,
  compactWeight:0,
  trapPenalty:0.5,
  spaceGainBonus:0.05,
};
const REWARD_COMPONENTS=[
  {key:'fruitReward',label:'Fruit bonus',sign:'positive'},
  {key:'growthBonus',label:'Growth bonus',sign:'positive'},
  {key:'approachBonus',label:'Toward fruit bonus',sign:'positive'},
  {key:'spaceGainBonus',label:'Open space bonus',sign:'positive'},
  {key:'compactness',label:'Compactness bonus',sign:'neutral'},
  {key:'stepPenalty',label:'Step penalty',sign:'negative'},
  {key:'turnPenalty',label:'Turn penalty',sign:'negative'},
  {key:'retreatPenalty',label:'Retreat penalty',sign:'negative'},
  {key:'loopPenalty',label:'Loop penalty',sign:'negative'},
  {key:'revisitPenalty',label:'Revisit penalty',sign:'negative'},
  {key:'trapPenalty',label:'Trap penalty',sign:'negative'},
  {key:'selfPenalty',label:'Self crash penalty',sign:'negative'},
  {key:'wallPenalty',label:'Wall crash penalty',sign:'negative'},
  {key:'timeoutPenalty',label:'Timeout penalty',sign:'negative'},
];
const REWARD_COMPONENT_KEYS=REWARD_COMPONENTS.map(item=>item.key);
const REWARD_LABELS={
  stepPenalty:'Step penalty',
  turnPenalty:'Turn penalty',
  approachBonus:'Toward fruit bonus',
  retreatPenalty:'Retreat penalty',
  loopPenalty:'Loop penalty',
  revisitPenalty:'Revisit penalty',
  trapPenalty:'Trap penalty',
  spaceGainBonus:'Space bonus',
  wallPenalty:'Wall crash penalty',
  selfPenalty:'Self crash penalty',
  timeoutPenalty:'Timeout penalty',
  fruitReward:'Fruit reward',
  growthBonus:'Growth bonus',
  compactWeight:'Compactness weight',
  compactness:'Compactness bonus',
};
const REWARD_INPUT_IDS={
  stepPenalty:'rewardStep',
  turnPenalty:'rewardTurn',
  approachBonus:'rewardApproach',
  retreatPenalty:'rewardRetreat',
  loopPenalty:'rewardLoop',
  revisitPenalty:'rewardRevisit',
  trapPenalty:'rewardTrap',
  spaceGainBonus:'rewardSpace',
  wallPenalty:'rewardWall',
  selfPenalty:'rewardSelf',
  timeoutPenalty:'rewardTimeout',
  fruitReward:'rewardFruit',
  growthBonus:'rewardGrowth',
  compactWeight:'rewardCompact',
};
const RECENT_EPISODES_MAX=32;
const ROLLUP_WINDOW=1000;
let rewardConfig={...REWARD_DEFAULTS};
const LOOP_PATTERNS=new Set(['1,2,1,2','2,1,2,1']);

/* ---------------- Serialization helpers ---------------- */
const DTYPE_ARRAYS={float32:Float32Array,int32:Int32Array,bool:Uint8Array};
function typedArrayToBase64(arr){
  if(!(arr instanceof Float32Array||arr instanceof Int32Array||arr instanceof Uint8Array)){
    arr=Float32Array.from(arr);
  }
  const view=new Uint8Array(arr.buffer,arr.byteOffset||0,arr.byteLength);
  let binary='';
  const chunk=0x8000;
  for(let i=0;i<view.length;i+=chunk){
    binary+=String.fromCharCode.apply(null,view.subarray(i,i+chunk));
  }
  return btoa(binary);
}
function base64ToTypedArray(str,dtype='float32'){
  const binary=atob(str);
  const len=binary.length;
  const bytes=new Uint8Array(len);
  for(let i=0;i<len;i++) bytes[i]=binary.charCodeAt(i);
  const C=DTYPE_ARRAYS[dtype]||Float32Array;
  return new C(bytes.buffer);
}
function assignArray(target,source,mapper=v=>v){
  target.length=0;
  if(!Array.isArray(source)) return;
  source.forEach(v=>target.push(mapper(v)));
}

/* ---------------- Snake environment ---------------- */
class SnakeEnv{
  constructor(cols=20,rows=20,rewardOverrides={}){
    this.cols=cols;
    this.rows=rows;
    this.setRewardConfig(rewardOverrides);
    this.reset();
  }
  _makeRewardBreakdown(){
    const base={total:0};
    REWARD_COMPONENT_KEYS.forEach(key=>{ base[key]=0; });
    return base;
  }
  setRewardConfig(cfg={}){
    this.reward={...REWARD_DEFAULTS,...cfg};
  }
  neighbors(x,y){
    return [
      {x:x+1,y},
      {x:x-1,y},
      {x,y:y+1},
      {x,y:y-1},
    ].filter(p=>p.x>=0&&p.y>=0&&p.x<this.cols&&p.y<this.rows);
  }
  freeSpaceFrom(sx,sy,tailWillMove){
    const seen=new Set();
    const q=[{x:sx,y:sy}];
    const blocked=new Set(this.snakeSet);
    blocked.delete(`${sx},${sy}`);
    if(tailWillMove&&this.snake.length){
      const t=this.snake[this.snake.length-1];
      blocked.delete(`${t.x},${t.y}`);
    }
    while(q.length){
      const p=q.pop();
      const key=`${p.x},${p.y}`;
      if(seen.has(key)) continue;
      if(blocked.has(key)) continue;
      seen.add(key);
      for(const n of this.neighbors(p.x,p.y)) q.push(n);
      if(seen.size>this.cols*this.rows) break;
    }
    return seen.size;
  }
  computeSlack(){
    if(!this.snake?.length) return 0;
    let minX=this.snake[0].x, maxX=this.snake[0].x;
    let minY=this.snake[0].y, maxY=this.snake[0].y;
    for(const seg of this.snake){
      if(seg.x<minX) minX=seg.x;
      if(seg.x>maxX) maxX=seg.x;
      if(seg.y<minY) minY=seg.y;
      if(seg.y>maxY) maxY=seg.y;
    }
    const width=(maxX-minX+1);
    const height=(maxY-minY+1);
    const area=width*height;
    return Math.max(0,area-this.snake.length);
  }
  reset(){
    this.dir={x:1,y:0};
    const cx=(this.cols/2|0), cy=(this.rows/2|0);
    this.snake=[{x:cx-1,y:cy},{x:cx,y:cy}];
    this.snakeSet=new Set(this.snake.map(p=>`${p.x},${p.y}`));
    this.visit=new Float32Array(this.cols*this.rows).fill(0);
    this.actionHist=[];
    this.spawnFruit();
    this.rewardBreakdown=this._makeRewardBreakdown();
    this.steps=0;
    this.stepsSinceFruit=0;
    this.alive=true;
    this.prevSlack=this.computeSlack();
    this.maxLength=this.snake.length;
    this.loopHits=0;
    this.revisitAccum=0;
    this.timeToFruitAccum=0;
    this.timeToFruitCount=0;
    this.episodeFruit=0;
    this.lastCrash=null;
    return this.getState();
  }
  idx(x,y){return y*this.cols+x;}
  spawnFruit(){
    const free=[];
    for(let y=0;y<this.rows;y++){
      for(let x=0;x<this.cols;x++){
        if(!this.snakeSet.has(`${x},${y}`)) free.push({x,y});
      }
    }
    this.fruit=free.length?free[(Math.random()*free.length)|0]:{x:-1,y:-1};
  }
  turn(a){
    const d=this.dir;
    if(a===1)this.dir={x:-d.y,y:d.x};
    else if(a===2)this.dir={x:d.y,y:-d.x};
  }
  step(a){
    if(!this.alive) return {state:this.getState(),reward:0,done:true,ateFruit:false};
    const R=this.reward;
    const breakdown=this.rewardBreakdown||(this.rewardBreakdown=this._makeRewardBreakdown());
    this.lastCrash=null;
    this.turn(a);
    const h=this.snake[0];
    const nx=h.x+this.dir.x;
    const ny=h.y+this.dir.y;
    this.steps++;
    this.stepsSinceFruit++;
    const key=`${nx},${ny}`;
    const tail=this.snake[this.snake.length-1];
    const willGrow=(nx===this.fruit.x && ny===this.fruit.y);
    const hitsWall=nx<0||ny<0||nx>=this.cols||ny>=this.rows;
    const hitsBody=this.snakeSet.has(key) && !(tail && tail.x===nx && tail.y===ny && !willGrow);
    if(hitsWall||hitsBody){
      this.alive=false;
      const crashReward=hitsWall?-R.wallPenalty:-R.selfPenalty;
      if(hitsWall) breakdown.wallPenalty+=crashReward;
      else breakdown.selfPenalty+=crashReward;
      breakdown.total+=crashReward;
      this.lastCrash=hitsWall?'wall':'self';
      return {state:this.getState(),reward:crashReward,done:true,ateFruit:false};
    }
    let spaceReward=0;
    if((R.trapPenalty??0)!==0 || (R.spaceGainBonus??0)!==0){
      const space=this.freeSpaceFrom(nx,ny,!willGrow);
      const need=this.snake.length+2;
      const denom=Math.max(1,need);
      if(space<need){
        spaceReward-=R.trapPenalty*(1+(need-space)/denom);
      }else if(R.spaceGainBonus){
        const curSpace=this.freeSpaceFrom(this.snake[0].x,this.snake[0].y,true);
        if(space>curSpace){
          spaceReward+=R.spaceGainBonus*Math.min(1,(space-curSpace)/denom);
        }
      }
    }
    for(let i=0;i<this.visit.length;i++) this.visit[i]*=0.995;
    this.snake.unshift({x:nx,y:ny});
    let r=-R.stepPenalty;
    breakdown.stepPenalty-=R.stepPenalty;
    r+=spaceReward;
    if(spaceReward>0) breakdown.spaceGainBonus+=spaceReward;
    else if(spaceReward<0) breakdown.trapPenalty+=spaceReward;
    if(a!==0){
      r-=R.turnPenalty;
      breakdown.turnPenalty-=R.turnPenalty;
    }
    this.actionHist.push(a);
    if(this.actionHist.length>6) this.actionHist.shift();
    if(this.actionHist.length>=4){
      const last4=this.actionHist.slice(-4).join(',');
      if(LOOP_PATTERNS.has(last4)){
        r-=R.loopPenalty;
        this.loopHits++;
        breakdown.loopPenalty-=R.loopPenalty;
      }
    }
    const vidx=this.idx(nx,ny);
    const revisitPenalty=this.visit[vidx]*R.revisitPenalty;
    r-=revisitPenalty;
    this.revisitAccum+=revisitPenalty;
    if(revisitPenalty) breakdown.revisitPenalty-=revisitPenalty;
    let ateFruit=false;
    if(nx===this.fruit.x && ny===this.fruit.y){
      ateFruit=true;
      r+=R.fruitReward;
      breakdown.fruitReward+=R.fruitReward;
      this.snakeSet.add(`${nx},${ny}`);
      this.spawnFruit();
      this.timeToFruitAccum+=this.stepsSinceFruit;
      this.timeToFruitCount++;
      this.stepsSinceFruit=0;
      this.episodeFruit++;
    }else{
      const tail=this.snake.pop();
      this.snakeSet.delete(`${tail.x},${tail.y}`);
      this.snakeSet.add(`${nx},${ny}`);
      this.visit[vidx]=Math.min(1,this.visit[vidx]+0.3);
      const pd=Math.abs(h.x-this.fruit.x)+Math.abs(h.y-this.fruit.y);
      const nd=Math.abs(nx-this.fruit.x)+Math.abs(ny-this.fruit.y);
      if(nd<pd){
        r+=R.approachBonus;
        breakdown.approachBonus+=R.approachBonus;
      }else if(nd>pd){
        r-=R.retreatPenalty;
        breakdown.retreatPenalty-=R.retreatPenalty;
      }
    }
    if(this.snake.length>this.maxLength){
      const gain=this.snake.length-this.maxLength;
      this.maxLength=this.snake.length;
      if(R.growthBonus){
        const bonus=R.growthBonus*gain;
        r+=bonus;
        breakdown.growthBonus+=bonus;
      }
    }
    const slack=this.computeSlack();
    const slackDelta=this.prevSlack-slack;
    if(R.compactWeight!==0){
      const compactReward=slackDelta*R.compactWeight;
      if(compactReward!==0){
        r+=compactReward;
        breakdown.compactness+=compactReward;
      }
    }
    this.prevSlack=slack;
    if(this.stepsSinceFruit>this.cols*this.rows*2){
      this.alive=false;
      r-=R.timeoutPenalty;
      this.lastCrash='timeout';
      this.rewardBreakdown.timeoutPenalty-=R.timeoutPenalty;
      this.rewardBreakdown.total+=r;
      return {state:this.getState(),reward:r,done:true,ateFruit:false};
    }
    this.rewardBreakdown.total+=r;
    return {state:this.getState(),reward:r,done:false,ateFruit};
  }
  getEpisodeBreakdown(){
    const src=this.rewardBreakdown||{};
    const copy={total:src.total??0};
    REWARD_COMPONENT_KEYS.forEach(key=>{ copy[key]=src[key]??0; });
    return copy;
  }
  getVisit(x,y){
    if(x<0||y<0||x>=this.cols||y>=this.rows) return 1;
    return this.visit[this.idx(x,y)]||0;
  }
  getState(){
    const h=this.snake[0];
    const L={x:-this.dir.y,y:this.dir.x}, R={x:this.dir.y,y:-this.dir.x};
    const block=(dx,dy)=>{
      const x=h.x+dx,y=h.y+dy;
      return (x<0||y<0||x>=this.cols||y>=this.rows||this.snakeSet.has(`${x},${y}`))?1:0;
    };
    const danger=[block(this.dir.x,this.dir.y),block(L.x,L.y),block(R.x,R.y)];
    const dir=[this.dir.y===-1?1:0,this.dir.y===1?1:0,this.dir.x===-1?1:0,this.dir.x===1?1:0];
    const fruit=[this.fruit.y<h.y?1:0,this.fruit.y>h.y?1:0,this.fruit.x<h.x?1:0,this.fruit.x>h.x?1:0];
    const dists=[h.y/(this.rows-1),(this.rows-1-h.y)/(this.rows-1),h.x/(this.cols-1),(this.cols-1-h.x)/(this.cols-1)];
    const dx=this.fruit.x-h.x, dy=this.fruit.y-h.y, len=Math.hypot(dx,dy)||1;
    const crowd=[
      this.getVisit(h.x, h.y-1),
      this.getVisit(h.x, h.y+1),
      this.getVisit(h.x-1, h.y),
      this.getVisit(h.x+1, h.y),
    ];
    return Float32Array.from([...danger,...dir,...fruit,...dists,dy/len,dx/len,...crowd]);
  }
}

class VecSnakeEnv{
  constructor(count=1,{cols=20,rows=20,rewardConfig={}}={}){
    this.cols=cols;
    this.rows=rows;
    this.rewardConfig={...REWARD_DEFAULTS,...rewardConfig};
    this.envCount=Math.max(1,count|0);
    this.envs=Array.from({length:this.envCount},()=>new SnakeEnv(this.cols,this.rows,this.rewardConfig));
  }
  getEnv(index=0){
    if(!this.envs.length) return null;
    const idx=((index%this.envCount)+this.envCount)%this.envCount;
    return this.envs[idx];
  }
  configure({count=this.envCount,cols=this.cols,rows=this.rows,rewardConfig=this.rewardConfig}={}){
    this.envCount=Math.max(1,count|0);
    this.cols=cols;
    this.rows=rows;
    this.rewardConfig={...REWARD_DEFAULTS,...rewardConfig};
    this.envs=Array.from({length:this.envCount},()=>new SnakeEnv(this.cols,this.rows,this.rewardConfig));
    return this.resetAll();
  }
  setCount(count){
    return this.configure({count});
  }
  setSize(cols,rows){
    return this.configure({cols,rows});
  }
  setRewardConfig(cfg={}){
    this.rewardConfig={...this.rewardConfig,...cfg};
    this.envs.forEach(env=>env.setRewardConfig(this.rewardConfig));
  }
  resetEnv(index){
    const env=this.getEnv(index);
    if(!env) return null;
    env.setRewardConfig(this.rewardConfig);
    return env.reset();
  }
  resetAll(){
    return this.envs.map(env=>{
      env.setRewardConfig(this.rewardConfig);
      return env.reset();
    });
  }
  step(actions){
    if(!Array.isArray(actions)||actions.length!==this.envCount){
      throw new Error(`Expected ${this.envCount} actions but received ${actions?.length}`);
    }
    const nextStates=new Array(this.envCount);
    const rewards=new Array(this.envCount);
    const dones=new Array(this.envCount);
    const ateFruit=new Array(this.envCount);
    for(let i=0;i<this.envCount;i++){
      const res=this.envs[i].step(actions[i]);
      nextStates[i]=res.state;
      rewards[i]=res.reward;
      dones[i]=!!res.done;
      ateFruit[i]=!!res.ateFruit;
    }
    return {nextStates,rewards,dones,ateFruit};
  }
}

/* ---------------- Replay buffer helpers ---------------- */
class NStepAccumulator{
  constructor(n=1,gamma=0.99){ this.setConfig(n,gamma); }
  setConfig(n,gamma){
    this.n=Math.max(1,n|0);
    this.gamma=gamma;
    this.queue=[];
  }
  push(step){
    const item={
      s:Float32Array.from(step.s),
      a:step.a|0,
      r:+step.r,
      ns:Float32Array.from(step.ns),
      d:!!step.d,
    };
    this.queue.push(item);
    const ready=[];
    while(this.queue.length>=this.n){
      ready.push(this.build());
      this.queue.shift();
      if(ready[ready.length-1].d){
        this.queue.length=0;
        return ready;
      }
    }
    if(item.d){
      ready.push(...this.flush());
    }
    return ready;
  }
  build(){
    let reward=0;
    let discount=1;
    let done=false;
    let nextState=this.queue[0].ns;
    const limit=Math.min(this.n,this.queue.length);
    for(let i=0;i<limit;i++){
      const step=this.queue[i];
      reward+=discount*step.r;
      discount*=this.gamma;
      nextState=step.ns;
      if(step.d){
        done=true;
        break;
      }
    }
    const first=this.queue[0];
    return {s:first.s,a:first.a,r:reward,ns:nextState,d:done};
  }
  flush(){
    const out=[];
    while(this.queue.length){
      out.push(this.build());
      this.queue.shift();
    }
    return out;
  }
}
class ReplayBuffer{
  constructor(cap=50000,opts={}){
    this.cap=Math.max(1,cap|0);
    this.buf=[];
    this.pos=0;
    this.alpha=opts.alpha??0.6;
    this.beta=opts.beta??0.4;
    this.betaIncrement=opts.betaIncrement??0.000002;
    this.priorityEps=opts.priorityEps??0.001;
    this.priorities=new Float32Array(this.cap);
    this.maxPriority=this.priorityEps;
  }
  size(){return this.buf.length;}
  setCapacity(cap){
    const newCap=Math.max(1,cap|0);
    if(newCap===this.cap) return;
    this.cap=newCap;
    this.buf=this.buf.slice(-this.cap);
    this.pos=Math.min(this.pos,this.cap-1);
    this.priorities=new Float32Array(this.cap);
    this.maxPriority=this.priorityEps;
  }
  setAlpha(val){ this.alpha=Math.max(0.01,+val||0.01); }
  setBeta(val){ this.beta=Math.min(1,Math.max(0,+val||0)); }
  setPriorityEps(val){
    this.priorityEps=Math.max(1e-6,+val||1e-6);
    for(let i=0;i<this.buf.length;i++){
      if(this.priorities[i]<this.priorityEps) this.priorities[i]=this.priorityEps;
    }
  }
  setBetaIncrement(val){ this.betaIncrement=Math.max(0,+val||0); }
  push(sample){
    const entry={...sample};
    if(this.buf.length<this.cap){
      this.buf.push(entry);
      const lastIdx=this.buf.length-1;
      this.priorities[lastIdx]=this.maxPriority;
    }else{
      const idx=this.pos%this.cap;
      this.buf[idx]=entry;
      this.priorities[idx]=this.maxPriority;
    }
    this.pos=(this.pos+1)%this.cap;
  }
  sample(batchSize){
    if(!this.buf.length) return null;
    const size=Math.min(batchSize,this.buf.length);
    const priorities=this.priorities.slice(0,this.buf.length);
    const probs=priorities.map(p=>Math.pow(p,this.alpha));
    const sum=probs.reduce((a,b)=>a+b,0)||1;
    const normalized=probs.map(p=>p/sum);
    const batch=[];
    const idxs=[];
    const weights=[];
    let beta=this.beta;
    this.beta=Math.min(1,this.beta+this.betaIncrement);
    const maxWeight=Math.pow(this.buf.length, -beta);
    for(let i=0;i<size;i++){
      const r=Math.random();
      let acc=0;
      let index=0;
      for(let j=0;j<normalized.length;j++){
        acc+=normalized[j];
        if(r<=acc){ index=j; break; }
      }
      batch.push(this.buf[index]);
      idxs.push(index);
      const w=Math.pow(this.buf.length*normalized[index], -beta);
      weights.push(w/maxWeight);
    }
    return {batch,idxs,weights};
  }
  updatePriorities(idxs,priorities){
    idxs.forEach((idx,i)=>{
      const p=Math.max(this.priorityEps,priorities[i]);
      this.priorities[idx]=p;
      this.maxPriority=Math.max(this.maxPriority,p);
    });
  }
  toJSON(){
    return {
      cap:this.cap,
      buf:this.buf.map(item=>({
        s:Array.from(item.s),
        a:item.a,
        r:item.r,
        ns:Array.from(item.ns),
        d:item.d,
      })),
      pos:this.pos,
      alpha:this.alpha,
      beta:this.beta,
      betaIncrement:this.betaIncrement,
      priorityEps:this.priorityEps,
      priorities:Array.from(this.priorities),
      maxPriority:this.maxPriority,
    };
  }
  static fromJSON(json={},cap,opts={}){
    const buffer=new ReplayBuffer(cap??json.cap,opts);
    if(Array.isArray(json.buf)){
      buffer.buf=json.buf.map(item=>({
        s:Float32Array.from(item.s),
        a:item.a,
        r:item.r,
        ns:Float32Array.from(item.ns),
        d:item.d,
      }));
      buffer.priorities=new Float32Array(buffer.cap);
      json.priorities?.forEach((p,i)=>{
        if(i<buffer.priorities.length) buffer.priorities[i]=p;
      });
      buffer.pos=json.pos??0;
      buffer.maxPriority=json.maxPriority??buffer.priorityEps;
    }
    return buffer;
  }
}

/* ---------------- Agents ---------------- */
class DQNAgent{
  constructor(sDim,aDim,cfg={}){
    this.kind='dqn';
    this.sDim=sDim;
    this.aDim=aDim;
    this.envCount=Math.max(1,cfg.envCount??1);
    this.gamma=cfg.gamma??0.98;
    this.lr=cfg.lr??0.0005;
    this.batch=cfg.batch??128;
    this.priorityEps=cfg.priorityEps??0.001;
    this.layers=Array.isArray(cfg.layers)?cfg.layers.slice():[256,256,128];
    this.dueling=cfg.dueling!==undefined?!!cfg.dueling:true;
    this.double=cfg.double!==undefined?!!cfg.double:true;
    this.learnRepeats=cfg.learnRepeats??2;
    this.buffer=new ReplayBuffer(cfg.bufferSize??50000,{
      alpha:cfg.priorityAlpha??0.6,
      beta:cfg.priorityBeta??0.4,
      betaIncrement:cfg.priorityBetaIncrement??0.000002,
      priorityEps:this.priorityEps,
    });
    this.priorityEps=this.buffer.priorityEps;
    this.epsStart=cfg.epsStart??1.0;
    this.epsEnd=cfg.epsEnd??0.12;
    this.epsDecay=cfg.epsDecay??80000;
    this.nStep=cfg.nStep??3;
    this.nStepBuffers=Array.from({length:this.envCount},()=>new NStepAccumulator(this.nStep,this.gamma));
    this.trainStep=cfg.trainStep??0;
    this.optimizer=tf.train.adam(this.lr);
    this.online=this.build();
    this.target=this.build();
    this.syncTarget();
    this.updateEpsilon(this.trainStep);
  }
  build(){
    const input=tf.input({shape:[this.sDim]});
    let x=input;
    this.layers.forEach(units=>{
      x=tf.layers.dense({units,activation:'relu',kernelInitializer:'heNormal'}).apply(x);
    });
    let q;
    if(this.dueling){
      const adv=tf.layers.dense({units:128,activation:'relu',kernelInitializer:'heNormal'}).apply(x);
      const advOut=tf.layers.dense({units:this.aDim,activation:'linear'}).apply(adv);
      const val=tf.layers.dense({units:128,activation:'relu',kernelInitializer:'heNormal'}).apply(x);
      const valOut=tf.layers.dense({units:1,activation:'linear'}).apply(val);
      q=tf.layers.add().apply([advOut,valOut]);
    }else{
      q=tf.layers.dense({units:this.aDim,activation:'linear'}).apply(x);
    }
    return tf.model({inputs:input,outputs:q});
  }
  setGamma(val){
    this.gamma=val;
    this.nStepBuffers.forEach(buf=>buf.setConfig(this.nStep,this.gamma));
  }
  setLearningRate(val){
    this.lr=val;
    this.optimizer=tf.train.adam(this.lr);
  }
  setEpsilonSchedule({start,end,decay}={}){
    if(start!==undefined) this.epsStart=start;
    if(end!==undefined) this.epsEnd=end;
    if(decay!==undefined) this.epsDecay=decay;
    this.updateEpsilon(this.trainStep);
  }
  setNStep(val){
    const n=Math.max(1,val|0);
    if(n===this.nStep)return;
    this.nStep=n;
    this.nStepBuffers.forEach(buf=>buf.setConfig(this.nStep,this.gamma));
  }
  recordTransition(...args){
    if(typeof args[0]==='number' && args.length>=6){
      const [envIndex,s,a,r,ns,d]=args;
      this._recordForEnv(envIndex,s,a,r,ns,d);
      return;
    }
    const [s,a,r,ns,d]=args;
    this._recordForEnv(0,s,a,r,ns,d);
  }
  _recordForEnv(envIndex,s,a,r,ns,d){
    const idx=((envIndex|0)%this.envCount+this.envCount)%this.envCount;
    const buf=this.nStepBuffers[idx];
    if(!buf) return;
    const ready=buf.push({s,a,r,ns,d});
    if(ready.length) ready.forEach(t=>this.buffer.push(t));
    if(d){
      const tail=buf.flush();
      if(tail.length) tail.forEach(t=>this.buffer.push(t));
    }
  }
  setEnvCount(count){
    const next=Math.max(1,count|0);
    if(next===this.envCount) return;
    this.envCount=next;
    this.nStepBuffers=Array.from({length:this.envCount},()=>new NStepAccumulator(this.nStep,this.gamma));
  }
  drainPending(envIndex){
    if(envIndex===undefined){
      this.nStepBuffers.forEach(buf=>{
        const tail=buf.flush();
        if(tail.length) tail.forEach(t=>this.buffer.push(t));
      });
      return;
    }
    const idx=((envIndex|0)%this.envCount+this.envCount)%this.envCount;
    const buf=this.nStepBuffers[idx];
    if(!buf) return;
    const tail=buf.flush();
    if(tail.length) tail.forEach(t=>this.buffer.push(t));
  }
  syncTarget(){
    this.target.setWeights(this.online.getWeights());
  }
  updateEpsilon(step){
    const t=Math.min(1,step/this.epsDecay);
    this.epsilon=this.epsStart*(1-t)+this.epsEnd*t;
    return this.epsilon;
  }
  act(s){
    if(Math.random()<this.epsilon) return (Math.random()*this.aDim)|0;
    return tf.tidy(()=>{
      return this.online.predict(tf.tensor2d([s],[1,this.sDim])).argMax(1).dataSync()[0];
    });
  }
  greedyAction(s){
    return tf.tidy(()=>{
      return this.online.predict(tf.tensor2d([s],[1,this.sDim])).argMax(1).dataSync()[0];
    });
  }
  async learn(){
    if(this.buffer.size()<this.batch) return null;
    const sample=this.buffer.sample(this.batch);
    if(!sample||!sample.batch.length) return null;
    const {batch,idxs,weights}=sample;
    const S=tf.tensor2d(batch.map(x=>x.s),[batch.length,this.sDim]);
    const NS=tf.tensor2d(batch.map(x=>x.ns),[batch.length,this.sDim]);
    const A=tf.tensor1d(batch.map(x=>x.a),'int32');
    const R=tf.tensor1d(batch.map(x=>x.r));
    const D=tf.tensor1d(batch.map(x=>x.d?1:0));
    const W=tf.tensor1d(weights);
    let tdErrors;
    const lossTensor=await this.optimizer.minimize(()=>{
      const q=this.online.apply(S);
      const oneHot=tf.oneHot(A,this.aDim);
      const qPred=tf.sum(q.mul(oneHot),1);
      const qNextTarget=this.target.apply(NS);
      let qNext;
      if(this.double){
        const qNextOnline=this.online.apply(NS);
        const aPrime=tf.argMax(qNextOnline,1);
        const mask=tf.oneHot(aPrime,this.aDim);
        qNext=tf.sum(qNextTarget.mul(mask),1);
      }else{
        qNext=tf.max(qNextTarget,1);
      }
      const target=R.add(qNext.mul(tf.scalar(this.gamma)).mul(tf.scalar(1).sub(D)));
      tdErrors=tf.keep(target.sub(qPred));
      const absErr=tdErrors.abs();
      const quadratic=tf.minimum(absErr,tf.scalar(1));
      const linear=absErr.sub(quadratic);
      const losses=quadratic.square().mul(0.5).add(linear);
      return losses.mul(W).mean();
    },true);
    const loss=lossTensor.dataSync()[0];
    lossTensor.dispose();
    const absTd=tdErrors.abs();
    const tdArray=absTd.dataSync();
    absTd.dispose();
    tdErrors.dispose();
    S.dispose(); NS.dispose(); A.dispose(); R.dispose(); D.dispose(); W.dispose();
    this.buffer.updatePriorities(idxs,tdArray);
    this.trainStep++;
    return loss;
  }
  async finishEpisode(){
    return null;
  }
  async exportState(){
    const weights=await Promise.all(this.online.getWeights().map(async w=>({
      shape:w.shape,
      dtype:w.dtype,
      data:typedArrayToBase64(await w.data()),
    })));
    return {
      version:4,
      kind:'dqn',
      sDim:this.sDim,
      aDim:this.aDim,
      config:{
        gamma:this.gamma,
        lr:this.lr,
        batch:this.batch,
        bufferSize:this.buffer.cap,
        epsStart:this.epsStart,
        epsEnd:this.epsEnd,
        epsDecay:this.epsDecay,
        nStep:this.nStep,
        priorityAlpha:this.buffer.alpha,
        priorityBeta:this.buffer.beta,
        priorityBetaIncrement:this.buffer.betaIncrement,
        priorityEps:this.buffer.priorityEps,
        dueling:this.dueling,
        double:this.double,
        layers:this.layers,
        envCount:this.envCount,
        learnRepeats:this.learnRepeats,
      },
      trainStep:this.trainStep,
      epsilon:this.epsilon,
      buffer:this.buffer.toJSON(),
      weights,
    };
  }
  async importState(state){
    if(!state) throw new Error('Invalid state');
    if(state.sDim && state.sDim!==this.sDim) throw new Error('State-dimension matchar inte');
    if(state.aDim && state.aDim!==this.aDim) throw new Error('Action-dimension matchar inte');
    const cfg=state.config??{};
    this.layers=Array.isArray(cfg.layers)?cfg.layers.slice():this.layers;
    this.dueling=cfg.dueling!==undefined?!!cfg.dueling:this.dueling;
    this.double=cfg.double!==undefined?!!cfg.double:this.double;
    this.learnRepeats=cfg.learnRepeats??this.learnRepeats;
    this.setGamma(cfg.gamma??this.gamma);
    this.setLearningRate(cfg.lr??this.lr);
    this.batch=cfg.batch??this.batch;
    this.buffer=ReplayBuffer.fromJSON(state.buffer,cfg.bufferSize,{
      alpha:cfg.priorityAlpha,
      beta:cfg.priorityBeta,
      betaIncrement:cfg.priorityBetaIncrement,
      priorityEps:cfg.priorityEps,
    });
    this.priorityEps=this.buffer.priorityEps;
    this.setEnvCount(cfg.envCount??this.envCount);
    this.epsStart=cfg.epsStart??this.epsStart;
    this.epsEnd=cfg.epsEnd??this.epsEnd;
    this.epsDecay=cfg.epsDecay??this.epsDecay;
    this.nStep=cfg.nStep??this.nStep;
    this.nStepBuffers=Array.from({length:this.envCount},()=>new NStepAccumulator(this.nStep,this.gamma));
    this.trainStep=state.trainStep??0;
    this.online.dispose();
    this.target.dispose();
    this.online=this.build();
    this.target=this.build();
    if(Array.isArray(state.weights)){
      const tensors=state.weights.map(w=>tf.tensor(base64ToTypedArray(w.data,w.dtype),w.shape,w.dtype));
      this.online.setWeights(tensors);
      tensors.forEach(t=>t.dispose());
    }
    this.syncTarget();
    this.updateEpsilon(this.trainStep);
  }
  setEntropy(){}
}
class PolicyGradientAgent{
  constructor(sDim,aDim,cfg={}){
    this.kind='policy';
    this.sDim=sDim;
    this.aDim=aDim;
    this.gamma=cfg.gamma??0.99;
    this.lr=cfg.lr??0.0008;
    this.entropy=cfg.entropy??0.01;
    this.optimizer=tf.train.adam(this.lr);
    this.model=this.build();
    this.trajectory=[];
    this.learnRepeats=0;
  }
  build(){
    const input=tf.input({shape:[this.sDim]});
    let x=tf.layers.dense({units:256,activation:'relu',kernelInitializer:'heNormal'}).apply(input);
    x=tf.layers.dense({units:256,activation:'relu',kernelInitializer:'heNormal'}).apply(x);
    const out=tf.layers.dense({units:this.aDim,activation:'softmax'}).apply(x);
    return tf.model({inputs:input,outputs:out});
  }
  setGamma(val){ this.gamma=val; }
  setLearningRate(val){ this.lr=val; this.optimizer=tf.train.adam(this.lr); }
  setEntropy(val){ this.entropy=val; }
  act(s){
    return tf.tidy(()=>{
      const probs=this.model.predict(tf.tensor2d([s],[1,this.sDim])).dataSync();
      const r=Math.random();
      let acc=0;
      for(let i=0;i<probs.length;i++){
        acc+=probs[i];
        if(r<=acc) return i;
      }
      return probs.length-1;
    });
  }
  greedyAction(s){
    return tf.tidy(()=>{
      const probs=this.model.predict(tf.tensor2d([s],[1,this.sDim])).dataSync();
      let best=0,max=-Infinity;
      probs.forEach((p,i)=>{ if(p>max){max=p;best=i;} });
      return best;
    });
  }
  recordTransition(s,a,r,ns,d){
    this.trajectory.push({s:Float32Array.from(s),a,r});
  }
  drainPending(){ this.trajectory.length=0; }
  async learn(){ return null; }
  async finishEpisode(){
    if(!this.trajectory.length) return null;
    const returns=[];
    let g=0;
    for(let i=this.trajectory.length-1;i>=0;i--){
      g=this.trajectory[i].r+this.gamma*g;
      returns[i]=g;
    }
    const states=tf.tensor2d(this.trajectory.map(t=>t.s),[this.trajectory.length,this.sDim]);
    const actions=tf.tensor1d(this.trajectory.map(t=>t.a),'int32');
    const returnsTensor=tf.tensor1d(returns);
    const entropyCoeff=this.entropy;
    const normalized=standardize1D(returnsTensor);
    const lossTensor=await this.optimizer.minimize(()=>tf.tidy(()=>{
      const probs=this.model.apply(states);
      const mask=tf.oneHot(actions,this.aDim);
      const selected=probs.mul(mask).sum(1).add(1e-8);
      const logProbs=tf.log(selected);
      const policyLoss=tf.neg(logProbs.mul(normalized));
      const entropy=probs.mul(tf.log(probs.add(1e-8))).sum(1).neg();
      return policyLoss.sub(entropy.mul(entropyCoeff)).mean();
    }),true);
    const loss=lossTensor.dataSync()[0];
    lossTensor.dispose();
    states.dispose();
    actions.dispose();
    returnsTensor.dispose();
    normalized.dispose();
    this.trajectory.length=0;
    return loss;
  }
  async exportState(){
    const weights=await Promise.all(this.model.getWeights().map(async w=>({
      shape:w.shape,
      dtype:w.dtype,
      data:typedArrayToBase64(await w.data()),
    })));
    return {
      version:4,
      kind:'policy',
      sDim:this.sDim,
      aDim:this.aDim,
      config:{
        gamma:this.gamma,
        lr:this.lr,
        entropy:this.entropy,
      },
      weights,
    };
  }
  async importState(state){
    if(!state) throw new Error('Invalid state');
    if(state.sDim && state.sDim!==this.sDim) throw new Error('State-dimension matchar inte');
    if(state.aDim && state.aDim!==this.aDim) throw new Error('Action-dimension matchar inte');
    const cfg=state.config??{};
    this.setGamma(cfg.gamma??this.gamma);
    this.setLearningRate(cfg.lr??this.lr);
    this.setEntropy(cfg.entropy??this.entropy);
    this.model.dispose();
    this.model=this.build();
    if(Array.isArray(state.weights)){
      const tensors=state.weights.map(w=>tf.tensor(base64ToTypedArray(w.data,w.dtype),w.shape,w.dtype));
      this.model.setWeights(tensors);
      tensors.forEach(t=>t.dispose());
    }
  }
}
class A2CAgent{
  constructor(sDim,aDim,cfg={}){
    this.kind='a2c';
    this.sDim=sDim;
    this.aDim=aDim;
    this.gamma=cfg.gamma??0.99;
    this.lr=cfg.lr??0.0006;
    this.entropyCoef=cfg.entropy??0.005;
    this.valueCoef=cfg.valueCoef??0.5;
    this.optimizer=tf.train.adam(this.lr);
    this.model=this.build();
    this.trajectory=[];
    this.learnRepeats=0;
  }
  build(){
    const input=tf.input({shape:[this.sDim]});
    let x=tf.layers.dense({units:256,activation:'relu',kernelInitializer:'heNormal'}).apply(input);
    x=tf.layers.dense({units:256,activation:'relu',kernelInitializer:'heNormal'}).apply(x);
    const policy=tf.layers.dense({units:this.aDim,activation:'softmax'}).apply(x);
    const value=tf.layers.dense({units:1,activation:'linear'}).apply(x);
    return tf.model({inputs:input,outputs:[policy,value]});
  }
  setGamma(val){ this.gamma=val; }
  setLearningRate(val){ this.lr=val; this.optimizer=tf.train.adam(this.lr); }
  setEntropy(val){ this.entropyCoef=val; }
  setValueCoef(val){ this.valueCoef=val; }
  act(s){
    return tf.tidy(()=>{
      const [policy,value]=this.model.predict(tf.tensor2d([s],[1,this.sDim]));
      value.dispose();
      const probs=policy.dataSync();
      const r=Math.random();
      let acc=0;
      let chosen=probs.length-1;
      for(let i=0;i<probs.length;i++){
        acc+=probs[i];
        if(r<=acc){ chosen=i; break; }
      }
      policy.dispose();
      return chosen;
    });
  }
  greedyAction(s){
    return tf.tidy(()=>{
      const [policy,value]=this.model.predict(tf.tensor2d([s],[1,this.sDim]));
      value.dispose();
      const probs=policy.dataSync();
      let best=0,max=-Infinity;
      probs.forEach((p,i)=>{ if(p>max){max=p;best=i;} });
      policy.dispose();
      return best;
    });
  }
  recordTransition(s,a,r,ns,d){
    this.trajectory.push({s:Float32Array.from(s),a,r,ns:Float32Array.from(ns),d});
  }
  drainPending(){ this.trajectory.length=0; }
  async learn(){ return null; }
  predictValue(state){
    const input=tf.tensor2d([state],[1,this.sDim]);
    const [policy,value]=this.model.predict(input);
    policy.dispose();
    const val=value.dataSync()[0];
    value.dispose();
    input.dispose();
    return val;
  }
  async finishEpisode(){
    const len=this.trajectory.length;
    if(!len) return null;
    const returns=new Array(len);
    let nextValue=0;
    if(!this.trajectory[len-1].d){
      nextValue=this.predictValue(this.trajectory[len-1].ns);
    }
    for(let i=len-1;i>=0;i--){
      const step=this.trajectory[i];
      nextValue=step.r+this.gamma*nextValue*(step.d?0:1);
      returns[i]=nextValue;
      if(step.d) nextValue=0;
    }
    const states=tf.tensor2d(this.trajectory.map(t=>t.s),[len,this.sDim]);
    const actions=tf.tensor1d(this.trajectory.map(t=>t.a),'int32');
    const returnsTensor=tf.tensor1d(returns);
    const entropyCoef=this.entropyCoef;
    const valueCoef=this.valueCoef;
    const lossTensor=await this.optimizer.minimize(()=>tf.tidy(()=>{
      const [policy,value]=this.model.apply(states);
      const mask=tf.oneHot(actions,this.aDim);
      const probs=policy.mul(mask).sum(1).add(1e-8);
      const logProbs=tf.log(probs);
      const values=value.reshape([len]);
      const rawAdv=returnsTensor.sub(values);
      const advMean=rawAdv.mean();
      const advStd=rawAdv.sub(advMean).square().mean().sqrt().add(1e-6);
      const advantages=rawAdv.sub(advMean).div(advStd);
      const actorLoss=tf.neg(logProbs.mul(advantages));
      const criticLoss=rawAdv.square().mul(0.5);
      const entropy=policy.mul(tf.log(policy.add(1e-8))).sum(1).neg();
      const total=actorLoss.add(criticLoss.mul(valueCoef)).sub(entropy.mul(entropyCoef));
      return total.mean();
    }),true);
    const loss=lossTensor.dataSync()[0];
    lossTensor.dispose();
    states.dispose();
    actions.dispose();
    returnsTensor.dispose();
    this.trajectory.length=0;
    return loss;
  }
  async exportState(){
    const weights=await Promise.all(this.model.getWeights().map(async w=>({
      shape:w.shape,
      dtype:w.dtype,
      data:typedArrayToBase64(await w.data()),
    })));
    return {
      version:4,
      kind:'a2c',
      sDim:this.sDim,
      aDim:this.aDim,
      config:{
        gamma:this.gamma,
        lr:this.lr,
        entropy:this.entropyCoef,
        valueCoef:this.valueCoef,
      },
      weights,
    };
  }
  async importState(state){
    if(!state) throw new Error('Invalid state');
    if(state.sDim && state.sDim!==this.sDim) throw new Error('State-dimension matchar inte');
    if(state.aDim && state.aDim!==this.aDim) throw new Error('Action-dimension matchar inte');
    const cfg=state.config??{};
    this.setGamma(cfg.gamma??this.gamma);
    this.setLearningRate(cfg.lr??this.lr);
    this.setEntropy(cfg.entropy??this.entropyCoef);
    this.setValueCoef(cfg.valueCoef??this.valueCoef);
    this.model.dispose();
    this.model=this.build();
    if(Array.isArray(state.weights)){
      const tensors=state.weights.map(w=>tf.tensor(base64ToTypedArray(w.data,w.dtype),w.shape,w.dtype));
      this.model.setWeights(tensors);
      tensors.forEach(t=>t.dispose());
    }
  }
}
class PPOAgent{
  constructor(sDim,aDim,cfg={}){
    this.kind='ppo';
    this.sDim=sDim;
    this.aDim=aDim;
    this.gamma=cfg.gamma??0.99;
    this.lam=cfg.lambda??0.95;
    this.lr=cfg.lr??0.0003;
    this.entropyCoef=cfg.entropy??0.003;
    this.valueCoef=cfg.valueCoef??0.5;
    this.clip=cfg.clip??0.2;
    this.batchSize=cfg.batch??256;
    this.epochs=cfg.epochs??4;
    this.optimizer=tf.train.adam(this.lr);
    this.model=this.build();
    this.trajectory=[];
    this.learnRepeats=0;
    this.lastActInfo=null;
  }
  build(){
    const input=tf.input({shape:[this.sDim]});
    let x=tf.layers.dense({units:256,activation:'relu',kernelInitializer:'heNormal'}).apply(input);
    x=tf.layers.dense({units:256,activation:'relu',kernelInitializer:'heNormal'}).apply(x);
    const policy=tf.layers.dense({units:this.aDim,activation:'softmax'}).apply(x);
    const value=tf.layers.dense({units:1,activation:'linear'}).apply(x);
    return tf.model({inputs:input,outputs:[policy,value]});
  }
  setGamma(val){ this.gamma=val; }
  setLearningRate(val){ this.lr=val; this.optimizer=tf.train.adam(this.lr); }
  setEntropy(val){ this.entropyCoef=val; }
  setValueCoef(val){ this.valueCoef=val; }
  setClip(val){ this.clip=val; }
  setLambda(val){ this.lam=val; }
  setBatch(val){ this.batchSize=Math.max(16,val|0); }
  setEpochs(val){ this.epochs=Math.max(1,val|0); }
  act(s){
    const input=tf.tensor2d([s],[1,this.sDim]);
    const [policy,value]=this.model.predict(input);
    const probs=policy.dataSync();
    const val=value.dataSync()[0];
    let r=Math.random();
    let action=probs.length-1;
    for(let i=0;i<probs.length;i++){
      r-=probs[i];
      if(r<=0){ action=i; break; }
    }
    const prob=Math.max(probs[action]??0,1e-8);
    this.lastActInfo={
      logProb:Math.log(prob),
      value:val,
    };
    policy.dispose();
    value.dispose();
    input.dispose();
    return action;
  }
  greedyAction(s){
    const input=tf.tensor2d([s],[1,this.sDim]);
    const [policy,value]=this.model.predict(input);
    value.dispose();
    const probs=policy.dataSync();
    let best=0,max=-Infinity;
    probs.forEach((p,i)=>{ if(p>max){max=p;best=i;} });
    policy.dispose();
    input.dispose();
    return best;
  }
  recordTransition(s,a,r,ns,d){
    const info=this.lastActInfo||this.evaluateAction(s,a);
    this.trajectory.push({
      s:Float32Array.from(s),
      a,
      r,
      ns:Float32Array.from(ns),
      d,
      value:info.value,
      logProb:info.logProb,
    });
    this.lastActInfo=null;
  }
  drainPending(){
    this.trajectory.length=0;
    this.lastActInfo=null;
  }
  async learn(){ return null; }
  evaluateAction(state,action){
    const input=tf.tensor2d([state],[1,this.sDim]);
    const [policy,value]=this.model.predict(input);
    const probs=policy.dataSync();
    const val=value.dataSync()[0];
    policy.dispose();
    value.dispose();
    input.dispose();
    const prob=Math.max(probs[action]??0,1e-8);
    return {logProb:Math.log(prob),value:val};
  }
  predictValue(state){
    const input=tf.tensor2d([state],[1,this.sDim]);
    const [,value]=this.model.predict(input);
    const val=value.dataSync()[0];
    value.dispose();
    input.dispose();
    return val;
  }
  async finishEpisode(){
    const len=this.trajectory.length;
    if(!len) return null;
    const values=this.trajectory.map(t=>t.value);
    let nextValue=0;
    if(!this.trajectory[len-1].d){
      nextValue=this.predictValue(this.trajectory[len-1].ns);
    }
    const advantages=new Array(len);
    const returns=new Array(len);
    let gae=0;
    for(let i=len-1;i>=0;i--){
      const step=this.trajectory[i];
      const value=values[i];
      const nextVal=(i===len-1)?nextValue:values[i+1];
      const nonTerminal=step.d?0:1;
      const delta=step.r+this.gamma*nextVal*nonTerminal-value;
      gae=delta+this.gamma*this.lam*nonTerminal*gae;
      advantages[i]=gae;
      returns[i]=gae+value;
    }
    const states=tf.tensor2d(this.trajectory.map(t=>t.s),[len,this.sDim]);
    const actions=tf.tensor1d(this.trajectory.map(t=>t.a),'int32');
    const oldLog=tf.tensor1d(this.trajectory.map(t=>t.logProb));
    const returnsTensor=tf.tensor1d(returns);
    const advRaw=tf.tensor1d(advantages);
    const advantagesTensor=standardize1D(advRaw);
    advRaw.dispose();
    const idxs=[...Array(len).keys()];
    let lastLoss=null;
    for(let epoch=0;epoch<this.epochs;epoch++){
      shuffleInPlace(idxs);
      for(let start=0;start<len;start+=this.batchSize){
        const slice=idxs.slice(start,Math.min(len,start+this.batchSize));
        if(!slice.length) continue;
        const batchIdx=tf.tensor1d(slice,'int32');
        const lossTensor=await this.optimizer.minimize(()=>tf.tidy(()=>{
          const batchStates=tf.gather(states,batchIdx);
          const batchActions=tf.gather(actions,batchIdx);
          const batchReturns=tf.gather(returnsTensor,batchIdx);
          const batchOldLog=tf.gather(oldLog,batchIdx);
          const batchAdv=tf.gather(advantagesTensor,batchIdx);
          const [policy,value]=this.model.apply(batchStates);
          const probs=policy.mul(tf.oneHot(batchActions,this.aDim)).sum(1).add(1e-8);
          const logProbs=tf.log(probs);
          const ratio=tf.exp(logProbs.sub(batchOldLog));
          const clipped=ratio.clipByValue(1-this.clip,1+this.clip);
          const actor=tf.minimum(ratio.mul(batchAdv),clipped.mul(batchAdv)).neg();
          const values=value.reshape([slice.length]);
          const critic=batchReturns.sub(values).square().mul(0.5*this.valueCoef);
          const entropy=policy.mul(tf.log(policy.add(1e-8))).sum(1).neg();
          return actor.add(critic).sub(entropy.mul(this.entropyCoef)).mean();
        }),true);
        lastLoss=lossTensor.dataSync()[0];
        lossTensor.dispose();
        batchIdx.dispose();
        await tf.nextFrame();
      }
    }
    states.dispose();
    actions.dispose();
    oldLog.dispose();
    returnsTensor.dispose();
    advantagesTensor.dispose();
    this.trajectory.length=0;
    return lastLoss;
  }
  async exportState(){
    const weights=await Promise.all(this.model.getWeights().map(async w=>({
      shape:w.shape,
      dtype:w.dtype,
      data:typedArrayToBase64(await w.data()),
    })));
    return {
      version:4,
      kind:'ppo',
      sDim:this.sDim,
      aDim:this.aDim,
      config:{
        gamma:this.gamma,
        lr:this.lr,
        entropy:this.entropyCoef,
        valueCoef:this.valueCoef,
        clip:this.clip,
        lambda:this.lam,
        batch:this.batchSize,
        epochs:this.epochs,
      },
      weights,
    };
  }
  async importState(state){
    if(!state) throw new Error('Invalid state');
    if(state.sDim && state.sDim!==this.sDim) throw new Error('State-dimension matchar inte');
    if(state.aDim && state.aDim!==this.aDim) throw new Error('Action-dimension matchar inte');
    const cfg=state.config??{};
    this.setGamma(cfg.gamma??this.gamma);
    this.setLearningRate(cfg.lr??this.lr);
    this.setEntropy(cfg.entropy??this.entropyCoef);
    this.setValueCoef(cfg.valueCoef??this.valueCoef);
    this.setClip(cfg.clip??this.clip);
    this.setLambda(cfg.lambda??this.lam);
    this.setBatch(cfg.batch??this.batchSize);
    this.setEpochs(cfg.epochs??this.epochs);
    this.model.dispose();
    this.model=this.build();
    if(Array.isArray(state.weights)){
      const tensors=state.weights.map(w=>tf.tensor(base64ToTypedArray(w.data,w.dtype),w.shape,w.dtype));
      this.model.setWeights(tensors);
      tensors.forEach(t=>t.dispose());
    }
  }
}
function shuffleInPlace(arr){
  for(let i=arr.length-1;i>0;i--){
    const j=(Math.random()*(i+1))|0;
    [arr[i],arr[j]]=[arr[j],arr[i]];
  }
}
function standardize1D(t){
  return tf.tidy(()=>{
    const mean=t.mean();
    const variance=t.sub(mean).square().mean();
    const std=variance.sqrt().add(1e-6);
    return t.sub(mean).div(std);
  });
}

function createRewardTelemetry(max=1200){
  const capacity=Math.max(10,max|0);
  const keys=[...REWARD_COMPONENT_KEYS,'total'];
  const store=Object.fromEntries(keys.map(key=>[key,[]]));
  return {
    record(breakdown){
      if(!breakdown) return;
      keys.forEach(key=>{
        const arr=store[key];
        const value=+breakdown[key]||0;
        arr.push(value);
        if(arr.length>capacity) arr.shift();
      });
    },
    summary(){
      const rows=REWARD_COMPONENTS.map(comp=>{
        const arr=store[comp.key];
        const last=arr.length?arr[arr.length-1]:0;
        const avg100=movingAverage(arr,100);
        const avg500=movingAverage(arr,500);
        const trend=avg100-movingAverage(arr,100,100);
        return {...comp,last,avg100,avg500,trend};
      });
      const totalArr=store.total;
      const total={
        key:'total',
        label:'Net reward',
        last:totalArr.length?totalArr[totalArr.length-1]:0,
        avg100:movingAverage(totalArr,100),
        avg500:movingAverage(totalArr,500),
        trend:movingAverage(totalArr,100)-movingAverage(totalArr,100,100),
      };
      const absSum=rows.reduce((acc,row)=>acc+Math.abs(row.avg100||0),0);
      rows.forEach(row=>{
        row.share=absSum?Math.abs(row.avg100)/absSum:0;
      });
      rows.sort((a,b)=>Math.abs(b.avg100)-Math.abs(a.avg100));
      return {rows,total};
    },
    toJSON(){
      const out={};
      keys.forEach(key=>{ out[key]=Array.from(store[key]); });
      return out;
    },
    fromJSON(data){
      keys.forEach(key=>{
        const arr=Array.isArray(data?.[key])?data[key].map(v=>+v||0):[];
        store[key].length=0;
        const slice=arr.slice(-capacity);
        slice.forEach(v=>store[key].push(v));
      });
    },
    reset(){
      keys.forEach(key=>{ store[key].length=0; });
    },
    getHistory(){
      return store;
    },
  };
}

/* ---------------- Rendering helpers ---------------- */
const board=document.getElementById('board');
const bctx=board.getContext('2d');
board.setAttribute('aria-hidden','false');
let COLS=20,ROWS=20,CELL=board.width/COLS;
let envCount=1;
let vecEnv=new VecSnakeEnv(envCount,{cols:COLS,rows:ROWS,rewardConfig});
let renderIndex=0;
let env=vecEnv.getEnv(renderIndex);

function snapshotEnv(environment){
  return {
    snake:environment.snake.map(p=>({x:p.x,y:p.y})),
    fruit:environment.fruit?{x:environment.fruit.x,y:environment.fruit.y}:{x:-1,y:-1},
  };
}
const cloneState=state=>({
  snake:state.snake.map(p=>({x:p.x,y:p.y})),
  fruit:{x:state.fruit.x,y:state.fruit.y},
});
const BG_COLOR='#101532';
const GRID_COLOR='rgba(135,143,210,0.16)';
const HEAD_GRADIENT=['#f472b6','#c084fc'];
const BODY_GRADIENT=['#8b5cf6','#6366f1'];
const HEAD_GLOW='rgba(244,114,182,0.65)';
const BODY_GLOW='rgba(99,102,241,0.5)';
let lastDrawnState=snapshotEnv(env);
let renderQueue=[];
let currentAnim=null;
let renderActive=false;
let renderToken=0;
let watching=false;
let liveViewHidden=false;
let renderSuspended=false;
const MAX_RENDER_QUEUE=240;

function queueLimit(){ return watching?MAX_RENDER_QUEUE*2:MAX_RENDER_QUEUE; }
function setImmediateState(environment){
  const state=snapshotEnv(environment);
  if(renderToken){
    cancelAnimationFrame(renderToken);
    renderToken=0;
  }
  renderActive=false;
  renderQueue.length=0;
  currentAnim=null;
  lastDrawnState=cloneState(state);
  if(!renderSuspended) drawFrame(state,state,1);
}
function enqueueRenderFrame(from,to,duration){
  if(renderSuspended){
    lastDrawnState=cloneState(to);
    return;
  }
  const entry={from:cloneState(from),to:cloneState(to),start:null,duration:Math.max(16,duration||80)};
  renderQueue.push(entry);
  const limit=queueLimit();
  if(renderQueue.length>limit){
    const latest=renderQueue[renderQueue.length-1];
    renderQueue=[{from:cloneState(lastDrawnState),to:cloneState(latest.to),start:null,duration:Math.max(40,duration)}];
    currentAnim=null;
  }
  if(!renderActive){
    renderActive=true;
    renderToken=requestAnimationFrame(stepRender);
  }
}
const easeProgress=t=>{
  if(t<=0) return 0;
  if(t>=1) return 1;
  return t<0.5?4*t*t*t:1-Math.pow(-2*t+2,3)/2;
};
function stepRender(ts){
  if(renderSuspended){
    renderQueue.length=0;
    currentAnim=null;
    renderActive=false;
    renderToken=0;
    return;
  }
  if(!currentAnim){
    currentAnim=renderQueue.shift();
    if(!currentAnim){
      renderActive=false;
      renderToken=0;
      drawFrame(lastDrawnState,lastDrawnState,1);
      return;
    }
  }
  if(currentAnim.start===null) currentAnim.start=ts;
  const duration=currentAnim.duration||80;
  const progress=duration<=0?1:Math.min(1,(ts-currentAnim.start)/duration);
  drawFrame(currentAnim.from,currentAnim.to,easeProgress(progress));
  if(progress>=1){
    lastDrawnState=cloneState(currentAnim.to);
    currentAnim=null;
  }
  renderToken=requestAnimationFrame(stepRender);
}
const waitAnimationFrame=()=>new Promise(res=>requestAnimationFrame(res));
async function waitForRenderCapacity(limit=Math.max(10,Math.floor(queueLimit()*0.6))){
  while(renderQueue.length>limit){
    await waitAnimationFrame();
  }
}
async function waitForRenderIdle(){
  while(renderQueue.length>0||currentAnim){
    await waitAnimationFrame();
  }
}
function drawFrame(from,to,t){
  if(renderSuspended) return;
  bctx.fillStyle=BG_COLOR;
  bctx.fillRect(0,0,board.width,board.height);
  drawGrid();
  const sameFruit=from.fruit.x===to.fruit.x&&from.fruit.y===to.fruit.y;
  if(from.fruit.x>=0&&!sameFruit) drawFruit(from.fruit,1-t);
  if(to.fruit.x>=0) drawFruit(to.fruit,sameFruit?1:t);
  const fromSnake=from.snake;
  const toSnake=to.snake;
  const grew=toSnake.length>fromSnake.length;
  const shrank=toSnake.length<fromSnake.length;
  const offset=shrank?fromSnake.length-toSnake.length:0;
  const segments=toSnake.map((seg,i)=>{
    let start;
    if(grew){
      start=i===0?fromSnake[0]:fromSnake[i-1]??fromSnake[fromSnake.length-1];
    }else if(shrank){
      start=fromSnake[i+offset]??fromSnake[fromSnake.length-1];
    }else{
      start=fromSnake[i]??fromSnake[fromSnake.length-1];
    }
    const sx=(start?.x??seg.x);
    const sy=(start?.y??seg.y);
    return {x:sx+(seg.x-sx)*t,y:sy+(seg.y-sy)*t};
  });
  drawSnakeSegments(segments);
}
function drawGrid(){
  bctx.save();
  bctx.strokeStyle=GRID_COLOR;
  bctx.lineWidth=1;
  bctx.shadowBlur=0;
  for(let x=0;x<=COLS;x++){
    const px=x*CELL;
    bctx.beginPath();
    bctx.moveTo(px,0);
    bctx.lineTo(px,board.height);
    bctx.stroke();
  }
  for(let y=0;y<=ROWS;y++){
    const py=y*CELL;
    bctx.beginPath();
    bctx.moveTo(0,py);
    bctx.lineTo(board.width,py);
    bctx.stroke();
  }
  bctx.restore();
}
function drawFruit(fruit,alpha=1){
  if(fruit.x<0||fruit.y<0) return;
  const cx=(fruit.x+0.5)*CELL;
  const cy=(fruit.y+0.5)*CELL;
  const radius=CELL*0.35;
  const gradient=bctx.createRadialGradient(cx,cy,radius*0.2,cx,cy,radius);
  gradient.addColorStop(0,`rgba(255,214,102,${alpha})`);
  gradient.addColorStop(0.65,`rgba(253,149,102,${alpha})`);
  gradient.addColorStop(1,`rgba(236,72,153,${alpha})`);
  bctx.save();
  bctx.fillStyle=gradient;
  bctx.shadowBlur=14;
  bctx.shadowColor='rgba(236,72,153,0.45)';
  bctx.beginPath();
  bctx.arc(cx,cy,radius,0,Math.PI*2);
  bctx.fill();
  bctx.restore();
}
function drawSnakeSegments(segments){
  if(!segments.length) return;
  bctx.save();
  segments.forEach((seg,i)=>{
    const colors=i===0?HEAD_GRADIENT:BODY_GRADIENT;
    const glow=i===0?HEAD_GLOW:BODY_GLOW;
    const size=CELL*0.74;
    const offset=(CELL-size)/2;
    const radius=Math.min(size*0.45,12);
    const x=seg.x*CELL+offset;
    const y=seg.y*CELL+offset;
    const gradient=bctx.createLinearGradient(x,y,x,y+size);
    gradient.addColorStop(0,colors[0]);
    gradient.addColorStop(1,colors[1]);
    bctx.shadowBlur=i===0?18:12;
    bctx.shadowColor=glow;
    bctx.fillStyle=gradient;
    drawRoundedRect(x,y,size,size,radius);
    bctx.shadowBlur=0;
    const highlight=bctx.createRadialGradient(x+size*0.35,y+size*0.35,0,x+size*0.35,y+size*0.35,size*0.9);
    highlight.addColorStop(0,'rgba(255,255,255,0.32)');
    highlight.addColorStop(1,'rgba(255,255,255,0)');
    bctx.fillStyle=highlight;
    drawRoundedRect(x,y,size,size,radius);
  });
  bctx.restore();
}
function drawRoundedRect(x,y,w,h,r){
  const radius=Math.max(2,Math.min(r,Math.min(w,h)/2));
  bctx.beginPath();
  bctx.moveTo(x+radius,y);
  bctx.arcTo(x+w,y,x+w,y+h,radius);
  bctx.arcTo(x+w,y+h,x,y+h,radius);
  bctx.arcTo(x,y+h,x,y,radius);
  bctx.arcTo(x,y,x+w,y,radius);
  bctx.closePath();
  bctx.fill();
}

/* ---------------- App state ---------------- */
const playbackModes={
  cinematic:{label:'Smooth realtime',frameMs:110,stepsPerFrame:1,renderEvery:1,queueTarget:60},
  fast:{label:'Fast',frameMs:60,stepsPerFrame:3,renderEvery:1,queueTarget:90},
  turbo:{label:'Turbo',frameMs:30,stepsPerFrame:6,renderEvery:2,queueTarget:120},
  watch:{label:'Watch',frameMs:120,stepsPerFrame:1,renderEvery:1,queueTarget:60},
};
const AGENT_PRESETS={
  dueling:{
    label:'Dueling Double DQN',
    badge:'Dueling DQN',
    type:'dqn',
    defaults:{
      gamma:0.98,lr:0.0005,
      epsStart:1.0,epsEnd:0.12,epsDecay:80000,
      batch:128,bufferSize:50000,targetSync:2000,
      nStep:3,priorityAlpha:0.6,priorityBeta:0.4,
      layers:[256,256,128],dueling:true,double:true,learnRepeats:2,
    },
    description:'Prioritized replay, n-step returns, and dueling heads provide stable, sample-efficient DQN training.',
    create:(sDim,aDim,cfg)=>new DQNAgent(sDim,aDim,{
      ...cfg,
      dueling:true,
      double:true,
      layers:cfg.layers??[256,256,128],
      learnRepeats:cfg.learnRepeats??2,
    }),
  },
  vanilla:{
    label:'Classic DQN',
    badge:'Vanilla DQN',
    type:'dqn',
    defaults:{
      gamma:0.97,lr:0.00025,
      epsStart:1.0,epsEnd:0.12,epsDecay:80000,
      batch:64,bufferSize:40000,targetSync:1500,
      nStep:1,priorityAlpha:0.4,priorityBeta:0.4,
      layers:[128,128],dueling:false,double:false,learnRepeats:1,
    },
    description:'A simpler DQN without dueling/double — perfect for understanding the base behaviour.',
    create:(sDim,aDim,cfg)=>new DQNAgent(sDim,aDim,{
      ...cfg,
      dueling:false,
      double:false,
      layers:cfg.layers??[128,128],
      learnRepeats:cfg.learnRepeats??1,
    }),
  },
  policy:{
    label:'Policy Gradient (REINFORCE)',
    badge:'Policy Grad',
    type:'policy',
    defaults:{
      gamma:0.99,lr:0.0008,entropy:0.01,
    },
    description:'Monte Carlo policy gradient with entropy regularisation for steady exploration.',
    create:(sDim,aDim,cfg)=>new PolicyGradientAgent(sDim,aDim,cfg),
  },
  a2c:{
    label:'Advantage Actor-Critic',
    badge:'A2C',
    type:'a2c',
    defaults:{
      gamma:0.99,lr:0.0006,entropy:0.005,valueCoef:0.5,
    },
    description:'Shared network that trains both policy and value function for faster convergence.',
    create:(sDim,aDim,cfg)=>new A2CAgent(sDim,aDim,cfg),
  },
  ppo:{
    label:'Proximal Policy Optimization',
    badge:'PPO',
    type:'ppo',
    defaults:{
      gamma:0.99,lr:0.0003,entropy:0.003,valueCoef:0.5,clip:0.2,lambda:0.95,batch:256,epochs:4,
    },
    description:'Clipped policy gradient with GAE for stable updates even in long episodes.',
    create:(sDim,aDim,cfg)=>new PPOAgent(sDim,aDim,cfg),
  },
};

const STAGE_AGENT_ALIASES={ddqn:'dueling',rainbow:'dueling',sac:'ppo'};
const STAGE_PRESETS={
  dqn_stage1:{
    label:'DQN Stage 1 – Warmup',
    agent:'vanilla',
    gamma:0.97,lr:0.00025,
    epsStart:1.0,epsEnd:0.2,epsDecay:60000,
    batchSize:64,bufferSize:40000,targetSync:1500,
    nStep:1,hiddenSizes:[128,128],
    rewardConfig:{fruitReward:12,stepPenalty:0.01,timeoutPenalty:5},
  },
  dqn_stage2:{
    label:'DQN Stage 2 – Exploration Shaping',
    agent:'vanilla',
    gamma:0.975,lr:0.00022,
    epsStart:1.0,epsEnd:0.18,epsDecay:80000,
    batchSize:96,bufferSize:60000,targetSync:1800,
    nStep:2,hiddenSizes:[160,160],
    rewardConfig:{fruitReward:14,stepPenalty:0.0095,timeoutPenalty:4.5},
  },
  dqn_stage3:{
    label:'DQN Stage 3 – Midgame Stabilisation',
    agent:'vanilla',
    gamma:0.98,lr:0.00018,
    epsStart:0.95,epsEnd:0.14,epsDecay:95000,
    batchSize:128,bufferSize:90000,targetSync:2000,
    nStep:3,hiddenSizes:[192,192],
    rewardConfig:{fruitReward:18,stepPenalty:0.008,timeoutPenalty:4},
  },
  dqn_stage4:{
    label:'DQN Stage 4 – Ramp Up',
    agent:'vanilla',
    gamma:0.988,lr:0.00013,
    epsStart:0.9,epsEnd:0.1,epsDecay:115000,
    batchSize:160,bufferSize:140000,targetSync:2400,
    nStep:4,hiddenSizes:[224,224,128],
    rewardConfig:{fruitReward:22,stepPenalty:0.007,timeoutPenalty:3.5},
  },
  dqn_stage5:{
    label:'DQN Stage 5 – Endgame',
    agent:'vanilla',
    gamma:0.993,lr:0.0001,
    epsStart:0.85,epsEnd:0.08,epsDecay:140000,
    batchSize:192,bufferSize:180000,targetSync:2800,
    nStep:5,hiddenSizes:[256,256,128],
    rewardConfig:{fruitReward:24,stepPenalty:0.006,timeoutPenalty:3},
  },
  ddqn_stage1:{
    label:'DDQN Stage 1 – Warmup',
    agent:'ddqn',
    gamma:0.975,lr:0.00028,
    epsStart:1.0,epsEnd:0.22,epsDecay:70000,
    batchSize:96,bufferSize:50000,targetSync:1600,
    nStep:2,hiddenSizes:[160,160,96],
    priorityAlpha:0.5,priorityBeta:0.4,
    rewardConfig:{fruitReward:13,stepPenalty:0.009,timeoutPenalty:4.8},
  },
  ddqn_stage2:{
    label:'DDQN Stage 2 – Momentum',
    agent:'ddqn',
    gamma:0.982,lr:0.00022,
    epsStart:0.95,epsEnd:0.17,epsDecay:90000,
    batchSize:128,bufferSize:80000,targetSync:1900,
    nStep:3,hiddenSizes:[192,192,128],
    priorityAlpha:0.55,priorityBeta:0.48,
    rewardConfig:{fruitReward:16,stepPenalty:0.0085,timeoutPenalty:4.2},
  },
  ddqn_stage3:{
    label:'DDQN Stage 3 – Midgame Control',
    agent:'ddqn',
    gamma:0.988,lr:0.00018,
    epsStart:0.9,epsEnd:0.14,epsDecay:105000,
    batchSize:160,bufferSize:110000,targetSync:2200,
    nStep:4,hiddenSizes:[224,224,128],
    priorityAlpha:0.6,priorityBeta:0.55,
    rewardConfig:{fruitReward:19,stepPenalty:0.0075,timeoutPenalty:3.8},
  },
  ddqn_stage4:{
    label:'DDQN Stage 4 – Refinement',
    agent:'ddqn',
    gamma:0.992,lr:0.00012,
    epsStart:0.85,epsEnd:0.1,epsDecay:125000,
    batchSize:192,bufferSize:150000,targetSync:2600,
    nStep:4,hiddenSizes:[256,256,160],
    priorityAlpha:0.65,priorityBeta:0.6,
    rewardConfig:{fruitReward:22,stepPenalty:0.0068,timeoutPenalty:3.4},
  },
  ddqn_stage5:{
    label:'DDQN Stage 5 – Endgame',
    agent:'ddqn',
    gamma:0.995,lr:0.0001,
    epsStart:0.8,epsEnd:0.08,epsDecay:150000,
    batchSize:224,bufferSize:200000,targetSync:3000,
    nStep:5,hiddenSizes:[256,256,192],
    priorityAlpha:0.7,priorityBeta:0.65,
    rewardConfig:{fruitReward:24,stepPenalty:0.006,timeoutPenalty:3},
  },
  rainbow_stage1:{
    label:'Rainbow Stage 1 – Warmup',
    agent:'rainbow',
    gamma:0.975,lr:0.00025,
    epsStart:1.0,epsEnd:0.18,epsDecay:80000,
    batchSize:128,bufferSize:70000,targetSync:1700,
    nStep:3,hiddenSizes:[192,192,128],
    priorityAlpha:0.6,priorityBeta:0.45,learnRepeats:2,
    rewardConfig:{fruitReward:15,stepPenalty:0.009,timeoutPenalty:4.5},
  },
  rainbow_stage2:{
    label:'Rainbow Stage 2 – Expansion',
    agent:'rainbow',
    gamma:0.982,lr:0.0002,
    epsStart:0.95,epsEnd:0.15,epsDecay:95000,
    batchSize:160,bufferSize:100000,targetSync:2100,
    nStep:4,hiddenSizes:[224,224,128],
    priorityAlpha:0.65,priorityBeta:0.5,learnRepeats:2,
    rewardConfig:{fruitReward:18,stepPenalty:0.008,timeoutPenalty:4},
  },
  rainbow_stage3:{
    label:'Rainbow Stage 3 – Midgame Pressure',
    agent:'rainbow',
    gamma:0.988,lr:0.00016,
    epsStart:0.9,epsEnd:0.12,epsDecay:115000,
    batchSize:192,bufferSize:130000,targetSync:2500,
    nStep:4,hiddenSizes:[256,256,128],
    priorityAlpha:0.7,priorityBeta:0.55,learnRepeats:3,
    rewardConfig:{fruitReward:20,stepPenalty:0.007,timeoutPenalty:3.6},
  },
  rainbow_stage4:{
    label:'Rainbow Stage 4 – Advanced Planning',
    agent:'rainbow',
    gamma:0.992,lr:0.00012,
    epsStart:0.85,epsEnd:0.1,epsDecay:135000,
    batchSize:224,bufferSize:160000,targetSync:2900,
    nStep:5,hiddenSizes:[288,288,160],
    priorityAlpha:0.75,priorityBeta:0.6,learnRepeats:3,
    rewardConfig:{fruitReward:22,stepPenalty:0.0065,timeoutPenalty:3.3},
  },
  rainbow_stage5:{
    label:'Rainbow Stage 5 – Endgame',
    agent:'rainbow',
    gamma:0.995,lr:0.0001,
    epsStart:0.8,epsEnd:0.08,epsDecay:160000,
    batchSize:256,bufferSize:200000,targetSync:3300,
    nStep:5,hiddenSizes:[320,320,192],
    priorityAlpha:0.8,priorityBeta:0.65,learnRepeats:4,
    rewardConfig:{fruitReward:24,stepPenalty:0.006,timeoutPenalty:3},
  },
  ppo_stage1:{
    label:'PPO Stage 1 – Warmup',
    agent:'ppo',
    gamma:0.965,lr:0.0003,
    lam:0.92,clipRatio:0.3,stepsPerEpoch:2048,
    trainIters:3,minibatchSize:128,
    entropy:0.01,valueCoef:0.45,
    rewardConfig:{fruitReward:14,stepPenalty:0.01,timeoutPenalty:5},
  },
  ppo_stage2:{
    label:'PPO Stage 2 – Stabilise',
    agent:'ppo',
    gamma:0.975,lr:0.00022,
    lam:0.94,clipRatio:0.24,stepsPerEpoch:3072,
    trainIters:4,minibatchSize:192,
    entropy:0.008,valueCoef:0.5,
    rewardConfig:{fruitReward:16,stepPenalty:0.009,timeoutPenalty:4.5},
  },
  ppo_stage3:{
    label:'PPO Stage 3 – Midgame',
    agent:'ppo',
    gamma:0.985,lr:0.00018,
    lam:0.96,clipRatio:0.2,stepsPerEpoch:4096,
    trainIters:5,minibatchSize:256,
    entropy:0.006,valueCoef:0.55,
    rewardConfig:{fruitReward:18,stepPenalty:0.008,timeoutPenalty:4},
  },
  ppo_stage4:{
    label:'PPO Stage 4 – Ramp Up',
    agent:'ppo',
    gamma:0.992,lr:0.00014,
    lam:0.97,clipRatio:0.18,stepsPerEpoch:5120,
    trainIters:6,minibatchSize:320,
    entropy:0.005,valueCoef:0.6,
    rewardConfig:{fruitReward:21,stepPenalty:0.007,timeoutPenalty:3.5},
  },
  ppo_stage5:{
    label:'PPO Stage 5 – Endgame',
    agent:'ppo',
    gamma:0.995,lr:0.0001,
    lam:0.98,clipRatio:0.15,stepsPerEpoch:6144,
    trainIters:6,minibatchSize:384,
    entropy:0.004,valueCoef:0.65,
    rewardConfig:{fruitReward:24,stepPenalty:0.006,timeoutPenalty:3},
  },
  sac_stage1:{
    label:'SAC Stage 1 – Warmup',
    agent:'sac',
    gamma:0.965,lr:0.0003,
    lam:0.9,clipRatio:0.28,stepsPerEpoch:2048,
    trainIters:3,minibatchSize:128,
    entropy:0.02,valueCoef:0.35,
    rewardConfig:{fruitReward:15,stepPenalty:0.0095,timeoutPenalty:4.8},
  },
  sac_stage2:{
    label:'SAC Stage 2 – Exploration Balance',
    agent:'sac',
    gamma:0.975,lr:0.00022,
    lam:0.93,clipRatio:0.22,stepsPerEpoch:3072,
    trainIters:4,minibatchSize:192,
    entropy:0.016,valueCoef:0.4,
    rewardConfig:{fruitReward:17,stepPenalty:0.0085,timeoutPenalty:4.2},
  },
  sac_stage3:{
    label:'SAC Stage 3 – Midgame Control',
    agent:'sac',
    gamma:0.985,lr:0.00016,
    lam:0.95,clipRatio:0.18,stepsPerEpoch:4096,
    trainIters:5,minibatchSize:256,
    entropy:0.013,valueCoef:0.45,
    rewardConfig:{fruitReward:19,stepPenalty:0.0075,timeoutPenalty:3.8},
  },
  sac_stage4:{
    label:'SAC Stage 4 – Precision',
    agent:'sac',
    gamma:0.992,lr:0.00012,
    lam:0.97,clipRatio:0.16,stepsPerEpoch:5120,
    trainIters:6,minibatchSize:320,
    entropy:0.01,valueCoef:0.5,
    rewardConfig:{fruitReward:22,stepPenalty:0.0068,timeoutPenalty:3.4},
  },
  sac_stage5:{
    label:'SAC Stage 5 – Endgame',
    agent:'sac',
    gamma:0.995,lr:0.0001,
    lam:0.98,clipRatio:0.14,stepsPerEpoch:6144,
    trainIters:6,minibatchSize:384,
    entropy:0.008,valueCoef:0.55,
    rewardConfig:{fruitReward:24,stepPenalty:0.006,timeoutPenalty:3},
  },
};

const STAGE_PRESET_PLACEHOLDER='Select stage preset';

function resolveAgentKey(rawAgent,fallback='dueling'){
  if(rawAgent&&AGENT_PRESETS[rawAgent]) return rawAgent;
  if(rawAgent&&STAGE_AGENT_ALIASES[rawAgent]) return STAGE_AGENT_ALIASES[rawAgent];
  return fallback;
}

function getStageAgentKey(preset,fallback='dueling'){
  if(!preset||typeof preset!=='object') return fallback;
  return resolveAgentKey(preset.agent,fallback);
}

const ui={
  trainState:document.getElementById('trainState'),
  algoBadge:document.getElementById('algoBadge'),
  epsReadout:document.getElementById('epsReadout'),
  gammaBadge:document.getElementById('gammaBadge'),
  lrBadge:document.getElementById('lrBadge'),
  playbackLabel:document.getElementById('playbackLabel'),
  playbackButtons:Array.from(document.querySelectorAll('#playbackGroup .pill')),
  gridSize:document.getElementById('gridSize'),
  gridLabel:document.getElementById('gridLabel'),
  btnToggleLiveView:document.getElementById('btnToggleLiveView'),
  btnTrain:document.getElementById('btnTrain'),
  btnPause:document.getElementById('btnPause'),
  btnStep:document.getElementById('btnStep'),
  btnWatch:document.getElementById('btnWatch'),
  btnReset:document.getElementById('btnReset'),
  btnCheckpointToggle:document.getElementById('btnCheckpointToggle'),
  btnSave:document.getElementById('btnSave'),
  btnLoad:document.getElementById('btnLoad'),
  btnClear:document.getElementById('btnClear'),
  modeButtons:Array.from(document.querySelectorAll('#modeGroup .pill')),
  algoSelect:document.getElementById('algoSelect'),
  algoDescription:document.getElementById('algoDescription'),
  aiAutoTuneToggle:document.getElementById('aiAutoTuneToggle'),
  aiIntervalSlider:document.getElementById('aiIntervalSlider'),
  aiIntervalReadout:document.getElementById('aiIntervalReadout'),
  aiRunLimit:document.getElementById('aiRunLimit'),
  aiRunLimitHint:document.getElementById('aiRunLimitHint'),
  aiStyleButtons:Array.from(document.querySelectorAll('#aiStyleGroup .pill')),
  gamma:document.getElementById('gamma'),
  gammaReadout:document.getElementById('gammaReadout'),
  lr:document.getElementById('lr'),
  lrReadout:document.getElementById('lrReadout'),
  envCount:document.getElementById('envCount'),
  envCountReadout:document.getElementById('envCountReadout'),
  epsStart:document.getElementById('epsStart'),
  epsStartReadout:document.getElementById('epsStartReadout'),
  epsEnd:document.getElementById('epsEnd'),
  epsEndReadout:document.getElementById('epsEndReadout'),
  epsDecay:document.getElementById('epsDecay'),
  epsDecayReadout:document.getElementById('epsDecayReadout'),
  batchSize:document.getElementById('batchSize'),
  batchReadout:document.getElementById('batchReadout'),
  bufferSize:document.getElementById('bufferSize'),
  bufferReadout:document.getElementById('bufferReadout'),
  targetSync:document.getElementById('targetSync'),
  targetSyncReadout:document.getElementById('targetSyncReadout'),
  nStep:document.getElementById('nStep'),
  nStepReadout:document.getElementById('nStepReadout'),
  priorityAlpha:document.getElementById('priorityAlpha'),
  alphaReadout:document.getElementById('alphaReadout'),
  priorityBeta:document.getElementById('priorityBeta'),
  betaReadout:document.getElementById('betaReadout'),
  pgEntropy:document.getElementById('pgEntropy'),
  pgEntropyReadout:document.getElementById('pgEntropyReadout'),
  acEntropy:document.getElementById('acEntropy'),
  acEntropyReadout:document.getElementById('acEntropyReadout'),
  acValueCoef:document.getElementById('acValueCoef'),
  acValueCoefReadout:document.getElementById('acValueCoefReadout'),
  ppoEntropy:document.getElementById('ppoEntropy'),
  ppoEntropyReadout:document.getElementById('ppoEntropyReadout'),
  ppoClip:document.getElementById('ppoClip'),
  ppoClipReadout:document.getElementById('ppoClipReadout'),
  ppoLambda:document.getElementById('ppoLambda'),
  ppoLambdaReadout:document.getElementById('ppoLambdaReadout'),
  ppoBatch:document.getElementById('ppoBatch'),
  ppoBatchReadout:document.getElementById('ppoBatchReadout'),
  ppoEpochs:document.getElementById('ppoEpochs'),
  ppoEpochsReadout:document.getElementById('ppoEpochsReadout'),
  ppoValueCoef:document.getElementById('ppoValueCoef'),
  ppoValueCoefReadout:document.getElementById('ppoValueCoefReadout'),
  rewardStep:document.getElementById('rewardStep'),
  rewardStepReadout:document.getElementById('rewardStepReadout'),
  rewardTurn:document.getElementById('rewardTurn'),
  rewardTurnReadout:document.getElementById('rewardTurnReadout'),
  rewardApproach:document.getElementById('rewardApproach'),
  rewardApproachReadout:document.getElementById('rewardApproachReadout'),
  rewardRetreat:document.getElementById('rewardRetreat'),
  rewardRetreatReadout:document.getElementById('rewardRetreatReadout'),
  rewardLoop:document.getElementById('rewardLoop'),
  rewardLoopReadout:document.getElementById('rewardLoopReadout'),
  rewardRevisit:document.getElementById('rewardRevisit'),
  rewardRevisitReadout:document.getElementById('rewardRevisitReadout'),
  rewardWall:document.getElementById('rewardWall'),
  rewardWallReadout:document.getElementById('rewardWallReadout'),
  rewardSelf:document.getElementById('rewardSelf'),
  rewardSelfReadout:document.getElementById('rewardSelfReadout'),
  rewardTimeout:document.getElementById('rewardTimeout'),
  rewardTimeoutReadout:document.getElementById('rewardTimeoutReadout'),
  rewardTrap:document.getElementById('rewardTrap'),
  rewardTrapReadout:document.getElementById('rewardTrapReadout'),
  rewardSpace:document.getElementById('rewardSpace'),
  rewardSpaceReadout:document.getElementById('rewardSpaceReadout'),
  rewardFruit:document.getElementById('rewardFruit'),
  rewardFruitReadout:document.getElementById('rewardFruitReadout'),
  rewardGrowth:document.getElementById('rewardGrowth'),
  rewardGrowthReadout:document.getElementById('rewardGrowthReadout'),
  rewardCompact:document.getElementById('rewardCompact'),
  rewardCompactReadout:document.getElementById('rewardCompactReadout'),
  kEpisodes:document.getElementById('kEpisodes'),
  kAvgRw:document.getElementById('kAvgRw'),
  kBest:document.getElementById('kBest'),
  kFruitRate:document.getElementById('kFruitRate'),
  rewardTelemetryBody:document.getElementById('rewardTelemetryBody'),
  rewardTelemetrySummary:document.getElementById('rewardTelemetrySummary'),
  rewardTelemetryPanel:document.getElementById('rewardTelemetryPanel'),
  progressChartPanel:document.getElementById('progressChartPanel'),
  progressChartBody:document.getElementById('progressChartBody'),
  progressChartToggle:document.getElementById('progressChartToggle'),
  progressChartCanvas:document.getElementById('progressChartCanvas'),
  progressChartSvg:document.getElementById('progressChartSvg'),
  progressChartGrid:document.getElementById('progressChartGrid'),
  progressRewardPath:document.getElementById('progressRewardPath'),
  progressFruitPath:document.getElementById('progressFruitPath'),
  progressChartEmpty:document.getElementById('progressChartEmpty'),
  progressChartLegend:document.getElementById('progressChartLegend'),
  progressChartMeta:document.getElementById('progressChartMeta'),
  progressChartRange:document.getElementById('progressChartRange'),
  tabTraining:document.getElementById('tabTraining'),
  tabGuide:document.getElementById('tabGuide'),
  trainingView:document.getElementById('trainingView'),
  guideView:document.getElementById('guideView'),
  autoLogPanel:document.getElementById('autoLogPanel'),
  autoLogStream:document.getElementById('autoLogStream'),
  autoLogClear:document.getElementById('autoLogClear'),
  fileLoader:document.getElementById('fileLoader'),
  advancedPanel:document.getElementById('advancedPanel'),
  advancedSections:{
    dqn:document.querySelector('[data-config="dqn"]'),
    policy:document.querySelector('[data-config="policy"]'),
    a2c:document.querySelector('[data-config="a2c"]'),
    ppo:document.querySelector('[data-config="ppo"]'),
  },
};

let presetSelectEl=null;
let stageSelectEl=null;
let currentStagePresetKey='';
let stagePresetField=null;

let agent=null;
let stateDim=env?.getState()?.length||0;
let actionDim=3;
let currentAlgoKey='dueling';
let playbackMode='cinematic';
let training=false;
let trainingToken=0;
let checkpointDirHandle=null;
let checkpointEnabled=false;
let checkpointFileHandle=null;
let checkpointSupportWarned=false;
let checkpointEpisodeInterval=500;
let lastFrame=0;
let targetSyncSteps=2000;
let episode=0,totalSteps=0,bestLen=0;
const rwHist=[],fruitHist=[],lossHist=[];
const progressPoints=[];
const PROGRESS_POINTS_MAX=120;
const PROGRESS_CHART_WIDTH=360;
const PROGRESS_CHART_HEIGHT=160;
const rewardTelemetry=createRewardTelemetry(1200);
let contexts=[];
let renderTick=0;
let trainingMode='manual';
let autoPilot=null;
const autoLogEntries=[];
const MAX_AUTO_LOG_ENTRIES=24;
let lastAutoMetrics=null;
let lastAutoSummaryEpisode=0;
const AUTO_TUNING_STYLES={
  calm:{key:'calm',label:'Lugn',magnitude:0.65,cooldown:1.6,eval:1.3,start:1.3},
  balanced:{key:'balanced',label:'Medel',magnitude:1,cooldown:1,eval:1,start:1},
  aggressive:{key:'aggressive',label:'Aggressiv',magnitude:1.45,cooldown:0.6,eval:0.75,start:0.8},
};
const DEFAULT_AUTO_STYLE='balanced';
const aiEpisodeHistory=[];
let aiAnalysisInterval=500;
let aiAutoTuneEnabled=false;
let aiAutoTuneStyle=DEFAULT_AUTO_STYLE;
let autoRunLimit=0;
let autoRunStopEpisode=null;
let aiTuner=null;
function avg(arr,n){
  if(!arr.length) return 0;
  const slice=arr.slice(-n);
  return slice.reduce((a,b)=>a+b,0)/slice.length;
}
function clamp(value,min,max){
  return Math.min(max,Math.max(min,value));
}
function movingAverage(arr=[],window,offset=0){
  if(!Array.isArray(arr)||!arr.length||window<=0) return 0;
  const end=Math.max(0,arr.length-offset);
  if(end<=0) return 0;
  const start=Math.max(0,end-window);
  const slice=arr.slice(start,end);
  if(!slice.length) return 0;
  return slice.reduce((a,b)=>a+b,0)/slice.length;
}
function stddev(arr=[]){
  if(!Array.isArray(arr)||!arr.length) return 0;
  const mean=arr.reduce((a,b)=>a+b,0)/arr.length;
  const variance=arr.reduce((a,b)=>a+(b-mean)**2,0)/arr.length;
  return Math.sqrt(variance);
}
function formatMetric(value,decimals=2){
  if(value===null||value===undefined||Number.isNaN(value)) return '—';
  const num=+value;
  return num.toFixed(decimals);
}
function formatPercent(value,decimals=1){
  if(value===null||value===undefined||Number.isNaN(value)) return '—';
  return `${(+value*100).toFixed(decimals)}%`;
}
function formatSigned(value,decimals=2){
  if(value===null||value===undefined||Number.isNaN(value)) return '—';
  const num=+value;
  const out=num.toFixed(decimals);
  return num>0?`+${out}`:out;
}
const AUTO_REASON_LABELS={
  stagnation:'stagnation',
  recovery:'recovery',
  regression:'regression',
  loss_ratio:'loss ratio',
  slow_fruit:'slow fruit',
  step_drag:'step drag',
  retreat_load:'retreat load',
  fruit_support:'fruit support',
  loop_penalty:'loop penalty',
  loop_relax:'loop relax',
  revisit_penalty:'revisit penalty',
  revisit_relax:'revisit relax',
  recover:'recover',
  self_penalty:'self penalty',
};
const REWARD_DECIMALS={
  loopPenalty:2,
  revisitPenalty:3,
  selfPenalty:1,
  approachBonus:3,
  retreatPenalty:3,
  stepPenalty:3,
  turnPenalty:3,
  fruitReward:1,
  growthBonus:1,
  trapPenalty:3,
  spaceGainBonus:3,
  timeoutPenalty:1,
};
function describeRewardDetail(adj){
  if(!adj||typeof adj!=='object') return 'Reward';
  const key=adj.key;
  const label=REWARD_LABELS[key]||key||'Reward';
  if(adj.value===undefined) return label;
  const decimals=key in REWARD_DECIMALS?REWARD_DECIMALS[key]:3;
  return `${label} → ${formatMetric(adj.value,decimals)}`;
}
function humanizeAutoReason(reason){
  if(!reason) return '';
  const text=AUTO_REASON_LABELS[reason]||reason;
  return text.charAt(0).toUpperCase()+text.slice(1);
}
function buildMetricsLine(metrics){
  if(!metrics) return '';
  const parts=[];
  if(metrics.maFruit100!==undefined) parts.push(`ma100 ${formatMetric(metrics.maFruit100,1)}`);
  if(metrics.maFruit500!==undefined) parts.push(`ma500 ${formatMetric(metrics.maFruit500,1)}`);
  if(metrics.fruitSlope!==undefined) parts.push(`trend ${formatSigned(metrics.fruitSlope,2)}`);
  if(metrics.lossRatio!==undefined) parts.push(`lossσ/μ ${formatMetric(metrics.lossRatio,2)}`);
  return parts.join(' • ');
}
function resetAutoLog(){
  autoLogEntries.length=0;
  if(ui.autoLogStream) ui.autoLogStream.innerHTML='';
  lastAutoMetrics=null;
  lastAutoSummaryEpisode=autoPilot?.episode||0;
}
function updateAutoLogVisibility(){
  if(!ui.autoLogPanel) return;
  const shouldShow=trainingMode==='auto'||aiAutoTuneEnabled;
  ui.autoLogPanel.classList.toggle('hidden',!shouldShow);
}
function logAutoEvent({title='',detail='',metrics=null,tone='info',episode=null}={}){
  if(!ui.autoLogStream) return;
  const entry=document.createElement('div');
  entry.className=`auto-log__entry auto-log__entry--${tone}`;
  if(episode!==null&&episode!==undefined){
    const tag=document.createElement('div');
    tag.className='auto-log__tag';
    tag.textContent=`Episod ${episode}`;
    entry.appendChild(tag);
  }
  if(title){
    const titleEl=document.createElement('div');
    titleEl.className='auto-log__title';
    titleEl.textContent=title;
    entry.appendChild(titleEl);
  }
  if(detail){
    const detailEl=document.createElement('div');
    detailEl.className='auto-log__detail';
    detailEl.textContent=detail;
    entry.appendChild(detailEl);
  }
  const metricsLine=buildMetricsLine(metrics);
  if(metricsLine){
    const metricsEl=document.createElement('div');
    metricsEl.className='auto-log__metrics';
    metricsEl.textContent=metricsLine;
    entry.appendChild(metricsEl);
  }
  ui.autoLogStream.prepend(entry);
  autoLogEntries.unshift(entry);
  while(autoLogEntries.length>MAX_AUTO_LOG_ENTRIES){
    const old=autoLogEntries.pop();
    old?.remove();
  }
}
function describeAutoAdjustment(adj={}){
  const res={title:'Auto adjustment',detail:'',tone:'info'};
  if(!adj||typeof adj!=='object') return res;
  switch(adj.type){
    case 'board':
      res.title='Curriculum';
      res.detail=`Board → ${adj.size}×${adj.size}`;
      res.tone='board';
      break;
    case 'epsilon':{
      res.title='Exploration';
      const parts=[];
      if(adj.end!==undefined) parts.push(`ε end → ${formatMetric(adj.end,2)}`);
      if(adj.decay!==undefined) parts.push(`decay → ${Math.round(+adj.decay)}`);
      res.detail=parts.join(' • ');
      res.tone='epsilon';
      break;
    }
    case 'lr':
      res.title='Learning rate';
      res.detail=`LR → ${formatMetric(adj.value,4)}`;
      res.tone='lr';
      break;
    case 'reward':{
      res.tone='reward';
      res.detail=describeRewardDetail(adj);
      break;
    }
    default:
      res.title='Auto adjustment';
      res.detail=adj.type?`${adj.type}`:'';
      res.tone='info';
  }
  const reason=humanizeAutoReason(adj.reason);
  if(reason){
    res.detail=res.detail?`${res.detail} — ${reason}`:reason;
  }
  return res;
}
function logAutoAdjustments(adjustments=[]){
  if(trainingMode!=='auto') return;
  if(!Array.isArray(adjustments)||!adjustments.length) return;
  adjustments.forEach(adj=>{
    const metrics=adj.metrics||lastAutoMetrics;
    const episodeNumber=adj.episode??autoPilot?.episode??0;
    const {title,detail,tone}=describeAutoAdjustment(adj);
    logAutoEvent({title,detail,metrics,tone,episode:episodeNumber});
    if(metrics) lastAutoMetrics=metrics;
    lastAutoSummaryEpisode=episodeNumber;
  });
}
function logAutoSummary(metrics,episodeNumber){
  if(trainingMode!=='auto') return;
  if(!metrics) return;
  logAutoEvent({title:'Auto check-in',detail:'No new adjustments',metrics,tone:'summary',episode:episodeNumber});
}

function round(value,decimals=3){
  const num=Number(value);
  if(!Number.isFinite(num)) return 0;
  const factor=10**decimals;
  return Math.round(num*factor)/factor;
}

function updateAiIntervalReadout(){
  if(!ui.aiIntervalSlider||!ui.aiIntervalReadout) return;
  const raw=+ui.aiIntervalSlider.value||aiAnalysisInterval||500;
  aiAnalysisInterval=Math.max(100,Math.min(5000,Math.round(raw/100)*100));
  checkpointEpisodeInterval=aiAnalysisInterval;
  ui.aiIntervalReadout.textContent=`${aiAnalysisInterval} ep`;
  if(aiTuner) aiTuner.setInterval(aiAnalysisInterval);
}

function setAiAutoStyle(style,{announce=true}={}){
  const preset=AUTO_TUNING_STYLES[style]||AUTO_TUNING_STYLES[DEFAULT_AUTO_STYLE];
  aiAutoTuneStyle=preset.key;
  ui.aiStyleButtons?.forEach(btn=>{
    btn.classList.toggle('active',btn.dataset.style===aiAutoTuneStyle);
  });
  autoPilot?.setTuningStyle?.(aiAutoTuneStyle);
  if(announce){
    logAutoEvent({
      title:'Auto-läge uppdaterat',
      detail:`${preset.label} justeringar`,
      tone:'ai',
      episode:autoPilot?.episode||episode||0,
    });
  }
  updateAiRunLimitHint();
}

function updateAiRunLimitHint(reachedTarget=false){
  if(!ui.aiRunLimitHint) return;
  if(autoRunLimit<=0){
    ui.aiRunLimitHint.textContent='0 = obegränsat';
    return;
  }
  const preset=AUTO_TUNING_STYLES[aiAutoTuneStyle]||AUTO_TUNING_STYLES[DEFAULT_AUTO_STYLE];
  if(reachedTarget){
    ui.aiRunLimitHint.textContent=`Mål nått – ${preset.label} läge pausade efter ${autoRunLimit} episoder`;
    return;
  }
  if(trainingMode==='auto' && training && autoRunStopEpisode){
    const remaining=Math.max(0,autoRunStopEpisode-episode);
    ui.aiRunLimitHint.textContent=`Pausar efter ${autoRunLimit} episoder (${remaining} återstår)`;
  }else{
    ui.aiRunLimitHint.textContent=`Pausar efter ${autoRunLimit} episoder`;
  }
}

function scheduleAutoRunTarget(){
  if(trainingMode==='auto' && training && autoRunLimit>0){
    autoRunStopEpisode=episode+autoRunLimit;
  }else{
    autoRunStopEpisode=null;
  }
  updateAiRunLimitHint();
}

function applyAiRunLimitFromUI(){
  if(!ui.aiRunLimit) return;
  const parsed=Math.max(0,Math.floor(+ui.aiRunLimit.value||0));
  autoRunLimit=parsed;
  if(parsed>0){
    ui.aiRunLimit.value=`${parsed}`;
  }else{
    ui.aiRunLimit.value='';
  }
  if(training && trainingMode==='auto' && autoRunLimit>0){
    scheduleAutoRunTarget();
  }else{
    autoRunStopEpisode=null;
    updateAiRunLimitHint();
  }
}

function getHyperparameterSnapshot(){
  const snapshot={
    gamma:+ui.gamma?.value||0,
    lr:+ui.lr?.value||0,
    envCount,
    epsilon:agent?.epsilon??null,
    agentKind:agent?.kind||null,
  };
  if(agent?.kind==='dqn'){
    Object.assign(snapshot,{
      epsStart:+ui.epsStart?.value||0,
      epsEnd:+ui.epsEnd?.value||0,
      epsDecay:+ui.epsDecay?.value||0,
      batch:+ui.batchSize?.value||0,
      bufferSize:+ui.bufferSize?.value||0,
      targetSync:+ui.targetSync?.value||0,
      nStep:+ui.nStep?.value||0,
      priorityAlpha:+ui.priorityAlpha?.value||0,
      priorityBeta:+ui.priorityBeta?.value||0,
    });
  }else if(agent?.kind==='policy'){
    Object.assign(snapshot,{
      entropy:+ui.pgEntropy?.value||0,
    });
  }else if(agent?.kind==='a2c'){
    Object.assign(snapshot,{
      entropy:+ui.acEntropy?.value||0,
      valueCoef:+ui.acValueCoef?.value||0,
    });
  }else if(agent?.kind==='ppo'){
    Object.assign(snapshot,{
      entropy:+ui.ppoEntropy?.value||0,
      valueCoef:+ui.ppoValueCoef?.value||0,
      clip:+ui.ppoClip?.value||0,
      lambda:+ui.ppoLambda?.value||0,
      batch:+ui.ppoBatch?.value||0,
      epochs:+ui.ppoEpochs?.value||0,
    });
  }
  return snapshot;
}

function compactEpisodeSummary(entry,includeDetail=false){
  if(!entry) return null;
  const summary={
    episode:entry.episode??null,
    reward:round(Number(entry.reward)||0,3),
    fruits:Number(entry.fruits)||0,
    steps:Number(entry.steps)||0,
    crash:entry.crash||'none',
  };
  if(includeDetail){
    summary.loopHits=Number(entry.loopHits)||0;
    if(entry.revisitPenalty!==undefined) summary.revisitPenalty=round(Number(entry.revisitPenalty)||0,3);
    if(entry.timeToFruitAvg!==undefined&&entry.timeToFruitAvg!==null){
      summary.timeToFruitAvg=round(Number(entry.timeToFruitAvg)||0,3);
    }
  }
  return summary;
}

function aggregateEpisodeWindow(windowSize){
  const size=Math.min(windowSize,aiEpisodeHistory.length);
  if(!size) return null;
  const slice=aiEpisodeHistory.slice(-size);
  let rewardSum=0,fruitSum=0,stepsSum=0,loopsSum=0,timeToFruitSum=0,timeToFruitCount=0;
  const rewardValues=[];
  const fruitValues=[];
  const crashCounts={};
  const breakdownTotals={};
  let breakdownCount=0;
  slice.forEach(item=>{
    const reward=Number(item.reward)||0;
    const fruits=Number(item.fruits)||0;
    const steps=Number(item.steps)||0;
    const loops=Number(item.loopHits)||0;
    rewardValues.push(reward);
    fruitValues.push(fruits);
    rewardSum+=reward;
    fruitSum+=fruits;
    stepsSum+=steps;
    loopsSum+=loops;
    const crashKey=item.crash||'none';
    crashCounts[crashKey]=(crashCounts[crashKey]||0)+1;
    if(item.timeToFruitAvg!==null&&item.timeToFruitAvg!==undefined){
      const timeVal=Number(item.timeToFruitAvg)||0;
      timeToFruitSum+=timeVal;
      timeToFruitCount++;
    }
    if(item.breakdown){
      breakdownCount++;
      Object.entries(item.breakdown).forEach(([key,val])=>{
        breakdownTotals[key]=(breakdownTotals[key]||0)+(+val||0);
      });
    }
  });
  const rewardAvg=size?rewardSum/size:0;
  const fruitAvg=size?fruitSum/size:0;
  const stepsAvg=size?stepsSum/size:0;
  const loopsAvg=size?loopsSum/size:0;
  const rewardVariance=rewardValues.length?rewardValues.reduce((acc,val)=>acc+(val-rewardAvg)**2,0)/rewardValues.length:0;
  const fruitVariance=fruitValues.length?fruitValues.reduce((acc,val)=>acc+(val-fruitAvg)**2,0)/fruitValues.length:0;
  const rewardStd=Math.sqrt(rewardVariance);
  const fruitStd=Math.sqrt(fruitVariance);
  const fruitRate=stepsSum>0?fruitSum/stepsSum:0;
  const fillRate=(COLS*ROWS)?fruitAvg/(COLS*ROWS):0;
  const half=Math.floor(slice.length/2);
  let rewardTrend=0,fruitTrend=0;
  if(half>0){
    const first=slice.slice(0,half);
    const last=slice.slice(-half);
    const firstReward=first.reduce((acc,item)=>acc+((Number(item.reward)||0)),0)/half;
    const lastReward=last.reduce((acc,item)=>acc+((Number(item.reward)||0)),0)/half;
    const firstFruit=first.reduce((acc,item)=>acc+((Number(item.fruits)||0)),0)/half;
    const lastFruit=last.reduce((acc,item)=>acc+((Number(item.fruits)||0)),0)/half;
    rewardTrend=lastReward-firstReward;
    fruitTrend=lastFruit-firstFruit;
  }
  const stats={
    rewardAvg:round(rewardAvg,3),
    rewardStd:round(rewardStd,3),
    fruitAvg:round(fruitAvg,3),
    fruitStd:round(fruitStd,3),
    stepsAvg:Math.round(stepsAvg),
    loopsAvg:round(loopsAvg,3),
    fruitRate:round(fruitRate,4),
    fillRate:round(fillRate,4),
    rewardTrend:round(rewardTrend,3),
    fruitTrend:round(fruitTrend,3),
  };
  if(timeToFruitCount>0){
    stats.timeToFruit=round(timeToFruitSum/timeToFruitCount,3);
  }
  const breakdownAvg=breakdownCount?Object.fromEntries(Object.entries(breakdownTotals).map(([key,val])=>[key,round(val/breakdownCount,4)])):undefined;
  return {
    window:size,
    firstEpisode:slice[0]?.episode??null,
    lastEpisode:slice[slice.length-1]?.episode??null,
    stats,
    crash:crashCounts,
    rewardBreakdown:breakdownAvg,
  };
}

function buildAITelemetrySnapshot(intervalOverride){
  const candidate=Number(intervalOverride);
  const requestedInterval=Number.isFinite(candidate)&&candidate>0?Math.floor(candidate):aiAnalysisInterval;
  const lookback=Math.min(requestedInterval,aiEpisodeHistory.length);
  if(!lookback) return null;
  const hyperSnapshot=getHyperparameterSnapshot();
  const intervalSummary=aggregateEpisodeWindow(lookback);
  const rollupSummary=aggregateEpisodeWindow(Math.min(ROLLUP_WINDOW,aiEpisodeHistory.length));
  const recentWindow=Math.min(RECENT_EPISODES_MAX,lookback);
  const recentEpisodes=aiEpisodeHistory.slice(-recentWindow).map(entry=>compactEpisodeSummary(entry));
  const latestEpisode=compactEpisodeSummary(aiEpisodeHistory[aiEpisodeHistory.length-1],true);
  return {
    intervalEpisodes:intervalSummary?.window??lookback,
    meta:{
      episode,
      interval:intervalSummary?.window??lookback,
      board:{cols:COLS,rows:ROWS},
      envs:envCount,
      algo:currentAlgoKey,
      agent:agent?.kind||null,
      best:bestLen,
    },
    stats:intervalSummary?.stats||{},
    crash:intervalSummary?.crash||{},
    rewardBreakdown:intervalSummary?.rewardBreakdown,
    rollup:rollupSummary?{
      window:rollupSummary.window,
      firstEpisode:rollupSummary.firstEpisode,
      lastEpisode:rollupSummary.lastEpisode,
      stats:rollupSummary.stats,
      crash:rollupSummary.crash,
      rewardBreakdown:rollupSummary.rewardBreakdown,
    }:null,
    rewardConfig:{...rewardConfig},
    hyper:hyperSnapshot,
    currentConfig:{
      reward:{...rewardConfig},
      hyper:hyperSnapshot,
    },
    recentEpisodes,
    latestEpisode,
  };
}

function applyAITunerRewardConfig(newConfig={}){
  const result={changes:[],config:null};
  if(!newConfig||typeof newConfig!=='object') return result;
  const merged={...rewardConfig};
  Object.entries(newConfig).forEach(([key,value])=>{
    if(!(key in merged)) return;
    const num=Number(value);
    if(!Number.isFinite(num)) return;
    let clamped=num;
    const inputId=REWARD_INPUT_IDS[key];
    const input=ui[inputId];
    if(input){
      if(input.min!==''&&input.min!==undefined) clamped=Math.max(clamped,+input.min);
      if(input.max!==''&&input.max!==undefined) clamped=Math.min(clamped,+input.max);
    }
    const current=merged[key];
    if(Math.abs(current-clamped)<1e-6) return;
    merged[key]=clamped;
    result.changes.push({key,oldValue:current,newValue:clamped});
  });
  if(result.changes.length){
    applyRewardConfigToUI(merged);
    result.config={...merged};
  }
  return result;
}

function applyAITunerHyperparameters(updates={}){
  const result={changes:[],hyper:null};
  if(!updates||typeof updates!=='object') return result;
  const setValue=(id,value,keyLabel)=>{
    const input=ui[id];
    if(!input) return;
    let num=Number(value);
    if(!Number.isFinite(num)) return;
    if(input.min!==''&&input.min!==undefined) num=Math.max(num,+input.min);
    if(input.max!==''&&input.max!==undefined) num=Math.min(num,+input.max);
    const current=+input.value;
    if(Math.abs(current-num)<1e-6) return;
    input.value=`${num}`;
    result.changes.push({key:keyLabel,oldValue:current,newValue:num});
  };
  if(updates.gamma!==undefined) setValue('gamma',updates.gamma,'gamma');
  if(updates.lr!==undefined) setValue('lr',updates.lr,'lr');
  if(updates.epsStart!==undefined) setValue('epsStart',updates.epsStart,'epsStart');
  if(updates.epsEnd!==undefined) setValue('epsEnd',updates.epsEnd,'epsEnd');
  if(updates.epsDecay!==undefined) setValue('epsDecay',updates.epsDecay,'epsDecay');
  if(updates.batch!==undefined){
    if(agent?.kind==='ppo') setValue('ppoBatch',updates.batch,'ppoBatch');
    else setValue('batchSize',updates.batch,'batchSize');
  }
  if(updates.batchSize!==undefined) setValue('batchSize',updates.batchSize,'batchSize');
  if(updates.bufferSize!==undefined) setValue('bufferSize',updates.bufferSize,'bufferSize');
  if(updates.targetSync!==undefined) setValue('targetSync',updates.targetSync,'targetSync');
  if(updates.nStep!==undefined) setValue('nStep',updates.nStep,'nStep');
  if(updates.priorityAlpha!==undefined) setValue('priorityAlpha',updates.priorityAlpha,'priorityAlpha');
  if(updates.priorityBeta!==undefined) setValue('priorityBeta',updates.priorityBeta,'priorityBeta');
  if(updates.entropy!==undefined){
    if(agent?.kind==='policy') setValue('pgEntropy',updates.entropy,'pgEntropy');
    else if(agent?.kind==='a2c') setValue('acEntropy',updates.entropy,'acEntropy');
    else if(agent?.kind==='ppo') setValue('ppoEntropy',updates.entropy,'ppoEntropy');
  }
  if(updates.valueCoef!==undefined){
    if(agent?.kind==='a2c') setValue('acValueCoef',updates.valueCoef,'acValueCoef');
    else if(agent?.kind==='ppo') setValue('ppoValueCoef',updates.valueCoef,'ppoValueCoef');
  }
  if(updates.clip!==undefined&&agent?.kind==='ppo') setValue('ppoClip',updates.clip,'ppoClip');
  if(updates.lambda!==undefined&&agent?.kind==='ppo') setValue('ppoLambda',updates.lambda,'ppoLambda');
  if(updates.epochs!==undefined&&agent?.kind==='ppo') setValue('ppoEpochs',updates.epochs,'ppoEpochs');
  if(result.changes.length){
    updateReadouts();
    applyConfigToAgent();
    result.hyper=getHyperparameterSnapshot();
  }
  return result;
}

function logAITunerEvent({title='',detail='',tone='ai',metrics=null,episodeNumber=null}={}){
  logAutoEvent({title,detail,metrics,tone,episode:episodeNumber??episode});
}

function bindUI(){
  ui.playbackButtons.forEach(btn=>{
    btn.addEventListener('click',()=>{
      setPlaybackMode(btn.dataset.speed);
    });
  });
  ui.modeButtons.forEach(btn=>{
    btn.addEventListener('click',()=>{
      setTrainingMode(btn.dataset.mode);
    });
  });
  ui.gridSize.addEventListener('input',()=>{
    updateGridLabel();
    const desiredSize=Math.max(8,(+ui.gridSize.value)|0);
    if(desiredSize!==COLS){
      resetEnvironment(desiredSize,true);
    }
  });
  ui.btnReset.addEventListener('click',()=>resetEnvironment(+ui.gridSize.value,true));
  ui.btnTrain.addEventListener('click',startTraining);
  ui.btnPause.addEventListener('click',stopTraining);
  ui.btnStep.addEventListener('click',async()=>{ await playSingleEpisode(); });
  ui.btnWatch.addEventListener('click',watchSmoothEpisode);
  ui.btnToggleLiveView?.addEventListener('click',()=>{
    setLiveViewHidden(!liveViewHidden);
  });
  ui.btnCheckpointToggle?.addEventListener('click',handleCheckpointToggle);
  ui.btnSave.addEventListener('click',saveTrainingToFile);
  ui.btnLoad.addEventListener('click',()=>ui.fileLoader?.click());
  ui.btnClear.addEventListener('click',()=>{
    for(const k in localStorage){
      if(k.includes('tensorflowjs')) localStorage.removeItem(k);
    }
    flash('Cleared local storage');
  });
  ui.autoLogClear?.addEventListener('click',()=>{
    resetAutoLog();
  });
  if(ui.progressChartToggle){
    ui.progressChartToggle.addEventListener('click',()=>{
      const collapsed=!ui.progressChartPanel?.classList.contains('collapsed');
      setProgressChartCollapsed(collapsed);
    });
    setProgressChartCollapsed(false);
  }
  ui.aiIntervalSlider?.addEventListener('input',()=>{
    updateAiIntervalReadout();
  });
  ui.aiRunLimit?.addEventListener('change',()=>{
    applyAiRunLimitFromUI();
  });
  ui.aiStyleButtons?.forEach(btn=>{
    btn.addEventListener('click',()=>{
      setAiAutoStyle(btn.dataset.style||DEFAULT_AUTO_STYLE);
    });
  });
  ui.aiAutoTuneToggle?.addEventListener('change',()=>{
    if(ui.aiAutoTuneToggle.checked){
      const code=prompt('Ange säkerhetskod för att aktivera AI Auto-Tune:');
      if(code!=='1234554321'){
        ui.aiAutoTuneToggle.checked=false;
        aiAutoTuneEnabled=false;
        flash('Fel kod. AI Auto-Tune förblir avstängd.',true);
        return;
      }
    }
    aiAutoTuneEnabled=ui.aiAutoTuneToggle.checked;
    if(aiTuner) aiTuner.setEnabled(aiAutoTuneEnabled);
    const detail=aiAutoTuneEnabled?'Aktiverad':'Avstängd';
    logAITunerEvent({title:'AI Auto-Tune',detail,tone:'ai',episodeNumber:episode});
    updateAutoLogVisibility();
  });
  ui.fileLoader?.addEventListener('change',async ev=>{
    const [file]=ev.target.files||[];
    if(file) await loadTrainingFromFile(file);
    ev.target.value='';
  });
  ui.algoSelect.addEventListener('change',()=>{
    if(watching) return;
    const wasTraining=training;
    if(wasTraining) stopTraining();
    instantiateAgent(ui.algoSelect.value);
  });
  const updateAndApply=()=>{ updateReadouts(); applyConfigToAgent(); };
  ['gamma','lr','epsStart','epsEnd','epsDecay','batchSize','bufferSize','targetSync','nStep','priorityAlpha','priorityBeta','pgEntropy','acEntropy','acValueCoef','ppoEntropy','ppoClip','ppoLambda','ppoBatch','ppoEpochs','ppoValueCoef']
    .forEach(id=>ui[id]?.addEventListener('input',updateAndApply));
  ui.envCount?.addEventListener('input',()=>{
    updateReadouts();
    applyEnvCountFromUI();
  });
  const rewardIds=['rewardStep','rewardTurn','rewardApproach','rewardRetreat','rewardLoop','rewardRevisit','rewardWall','rewardSelf','rewardTimeout','rewardTrap','rewardSpace','rewardFruit','rewardGrowth','rewardCompact'];
  const updateRewards=()=>{ updateRewardReadouts(); applyRewardsToEnv(); };
  rewardIds.forEach(id=>ui[id]?.addEventListener('input',updateRewards));
  ui.tabTraining.addEventListener('click',()=>setActiveTab('training'));
  ui.tabGuide.addEventListener('click',()=>setActiveTab('guide'));
  updateReadouts();
  updateGridLabel();
  applyRewardsToEnv();
  updateControlAvailability();
  setTrainingMode(trainingMode);
  updateAutoLogVisibility();
  updateAiIntervalReadout();
  setAiAutoStyle(aiAutoTuneStyle,{announce:false});
  updateAiRunLimitHint();
  if(ui.aiAutoTuneToggle) ui.aiAutoTuneToggle.checked=aiAutoTuneEnabled;
  updateCheckpointToggleUI();
  updateProgressChart();
}
function setActiveTab(tab){
  const showGuide=tab==='guide';
  ui.tabTraining.classList.toggle('active',!showGuide);
  ui.tabGuide.classList.toggle('active',showGuide);
  ui.trainingView.classList.toggle('hidden',showGuide);
  ui.guideView.classList.toggle('hidden',!showGuide);
}
function updateGridLabel(){
  const val=+ui.gridSize.value;
  ui.gridLabel.textContent=`${val}×${val}`;
}
function updateControlAvailability(){
  if(ui.btnStep){
    ui.btnStep.disabled=trainingMode==='auto'||envCount>1;
  }
}
function createContextSlot(index,state){
  return {
    envIndex:index,
    state:Float32Array.from(state),
    totalReward:0,
    fruits:0,
    steps:0,
    needsReset:false,
  };
}
function seedContexts(forceReset=true){
  if(!vecEnv) return;
  const states=forceReset?vecEnv.resetAll():vecEnv.envs.map(env=>Float32Array.from(env.getState()));
  contexts=states.map((state,idx)=>createContextSlot(idx,state));
  env=vecEnv.getEnv(renderIndex)||vecEnv.getEnv(0);
  if(env) setImmediateState(env);
}
function ensureContextPool(){
  if(!vecEnv) return;
  if(contexts.length!==envCount){
    seedContexts(true);
  }
}
function reconfigureEnvironment({count=envCount,size=COLS,force=false}={}){
  let desiredCount=Math.max(1,count|0);
  if(trainingMode==='auto'){
    desiredCount=Math.max(12,desiredCount);
  }
  const desiredSize=Math.max(8,(+size)|0);
  const needInit=!vecEnv;
  const changedCount=desiredCount!==envCount;
  const changedSize=force||desiredSize!==COLS||needInit;
  envCount=desiredCount;
  if(!vecEnv){
    vecEnv=new VecSnakeEnv(envCount,{cols:desiredSize,rows:desiredSize,rewardConfig});
  }else if(changedCount||changedSize){
    vecEnv.configure({count:envCount,cols:desiredSize,rows:desiredSize,rewardConfig});
  }else{
    vecEnv.setRewardConfig(rewardConfig);
  }
  renderIndex=Math.min(renderIndex,envCount-1);
  env=vecEnv.getEnv(renderIndex)||vecEnv.getEnv(0);
  COLS=desiredSize;
  ROWS=desiredSize;
  CELL=board.width/COLS;
  stateDim=env?.getState()?.length||stateDim;
  seedContexts(true);
  agent?.setEnvCount?.(envCount);
  renderTick=0;
  updateControlAvailability();
  if(ui.envCount && ui.envCount.value!==`${envCount}`){
    ui.envCount.value=`${envCount}`;
  }
  updateBadgeMetrics();
}
function resetEnvironment(size=COLS,force=false){
  const wasTraining=training;
  if(wasTraining) stopTraining();
  reconfigureEnvironment({size,force:true});
  if(wasTraining&&!watching) startTraining();
}
function applyEnvCountFromUI(){
  const desired=Math.max(1,+ui.envCount.value||1);
  if(desired===envCount) return;
  const wasTraining=training;
  if(wasTraining) stopTraining();
  reconfigureEnvironment({count:desired,size:+ui.gridSize.value,force:true});
  if(wasTraining&&!watching) startTraining();
}
function setTrainingMode(mode){
  let next=mode==='auto'?'auto':'manual';
  const wasTraining=training;
  const prevMode=trainingMode;
  if(next==='auto' && agent?.kind!=='dqn'){
    flash('Auto mode requires a DQN agent',true);
    next='manual';
  }
  if(wasTraining) stopTraining();
  trainingMode=next;
  ui.modeButtons.forEach(btn=>btn.classList.toggle('active',btn.dataset.mode===trainingMode));
  if(trainingMode==='auto'){
    const firstActivation=prevMode!=='auto';
    autoPilot=new BrowserAutoPilot({rewardConfig:{...rewardConfig}});
    autoPilot.setAgent(agent);
    autoPilot.setTuningStyle(aiAutoTuneStyle);
    const desiredCount=Math.max(12,envCount);
    ui.envCount.value=`${desiredCount}`;
    if(firstActivation){
      ui.gridSize.value='10';
      updateGridLabel();
      reconfigureEnvironment({count:desiredCount,size:10,force:true});
    }else{
      reconfigureEnvironment({count:desiredCount,size:+ui.gridSize.value,force:true});
    }
    updateReadouts();
    const stageIdx=autoPilot.boardStages.findIndex(stage=>stage.size===COLS);
    if(stageIdx>=0) autoPilot.stageIndex=stageIdx;
    lastAutoMetrics=null;
    lastAutoSummaryEpisode=autoPilot?.episode||0;
    autoPilot.lastEvaluationEpisode=autoPilot.episode||0;
    if(firstActivation){
      resetAutoLog();
      updateAutoLogVisibility();
      logAutoEvent({
        title:'Auto mode activated',
        detail:`${envCount} environments • ${COLS}×${ROWS}`,
        tone:'info',
        episode:autoPilot?.episode||0,
      });
    }else{
      updateAutoLogVisibility();
    }
    ui.envCount.disabled=true;
  }else{
    autoPilot=null;
    ui.envCount.disabled=agent?.kind!=='dqn';
    lastAutoMetrics=null;
    lastAutoSummaryEpisode=0;
    updateAutoLogVisibility();
    autoRunStopEpisode=null;
  }
  if(trainingMode==='auto') ui.envCount.disabled=true;
  updateControlAvailability();
  updateReadouts();
  updateAiRunLimitHint();
  if(wasTraining&&!watching) startTraining();
}
function setPlaybackMode(mode){
  if(!playbackModes[mode]) mode='cinematic';
  playbackMode=mode;
  ui.playbackButtons.forEach(btn=>{
    btn.classList.toggle('active',btn.dataset.speed===mode);
  });
  ui.playbackLabel.textContent=liveViewHidden?'Rendering paused':playbackModes[mode].label;
}
function setLiveViewHidden(hidden){
  hidden=!!hidden;
  if(hidden===liveViewHidden) return;
  liveViewHidden=hidden;
  if(ui.btnToggleLiveView){
    ui.btnToggleLiveView.textContent=hidden?'Show live view':'Hide live view';
    ui.btnToggleLiveView.setAttribute('aria-pressed',hidden?'true':'false');
  }
  board.classList.toggle('hidden',hidden);
  board.setAttribute('aria-hidden',hidden?'true':'false');
  if(hidden){
    renderSuspended=true;
    if(renderToken){
      cancelAnimationFrame(renderToken);
      renderToken=0;
    }
    renderActive=false;
    renderQueue.length=0;
    currentAnim=null;
    ui.playbackLabel.textContent='Rendering paused';
  }else{
    renderSuspended=false;
    setImmediateState(env);
    setPlaybackMode(playbackMode);
  }
}
function applyConfigToAgent(){
  if(!agent) return;
  const shared={
    gamma:+ui.gamma.value,
    lr:+ui.lr.value,
  };
  agent.setEnvCount?.(envCount);
  agent.setGamma(shared.gamma);
  agent.setLearningRate(shared.lr);
  if(agent.kind==='dqn'){
    agent.setEpsilonSchedule?.({
      start:+ui.epsStart.value,
      end:+ui.epsEnd.value,
      decay:+ui.epsDecay.value,
    });
    agent.batch=+ui.batchSize.value;
    agent.buffer.setCapacity(+ui.bufferSize.value);
    agent.buffer.setAlpha(+ui.priorityAlpha.value);
    agent.buffer.setBeta(+ui.priorityBeta.value);
    agent.setNStep(+ui.nStep.value);
    targetSyncSteps=+ui.targetSync.value||2000;
  }else if(agent.kind==='policy'){
    agent.setEntropy(+ui.pgEntropy.value);
    targetSyncSteps=Infinity;
  }else if(agent.kind==='a2c'){
    agent.setEntropy(+ui.acEntropy.value);
    agent.setValueCoef(+ui.acValueCoef.value);
    targetSyncSteps=Infinity;
  }else if(agent.kind==='ppo'){
    agent.setEntropy(+ui.ppoEntropy.value);
    agent.setValueCoef(+ui.ppoValueCoef.value);
    agent.setClip(+ui.ppoClip.value);
    agent.setLambda(+ui.ppoLambda.value);
    agent.setBatch(+ui.ppoBatch.value);
    agent.setEpochs(+ui.ppoEpochs.value);
    targetSyncSteps=Infinity;
  }
  updateBadgeMetrics();
}
function updateReadouts(){
  updateBadgeMetrics();
  updateRewardReadouts();
}
function updateBadgeMetrics(){
  ui.gammaReadout.textContent=(+ui.gamma.value).toFixed(3);
  ui.gammaBadge.textContent=(+ui.gamma.value).toFixed(3);
  ui.lrReadout.textContent=(+ui.lr.value).toFixed(4);
  ui.lrBadge.textContent=(+ui.lr.value).toFixed(4);
  if(ui.envCountReadout){
    ui.envCountReadout.textContent=`${(+ui.envCount.value)|0}`;
  }
  if(agent?.kind==='dqn'){
    ui.epsReadout.textContent=(agent.epsilon??1).toFixed(2);
  }else{
    ui.epsReadout.textContent='—';
  }
  ui.epsStartReadout.textContent=(+ui.epsStart.value).toFixed(2);
  ui.epsEndReadout.textContent=(+ui.epsEnd.value).toFixed(2);
  ui.epsDecayReadout.textContent=`${(+ui.epsDecay.value)|0}`;
  ui.batchReadout.textContent=`${(+ui.batchSize.value)|0}`;
  ui.bufferReadout.textContent=`${(+ui.bufferSize.value)|0}`;
  ui.targetSyncReadout.textContent=`${(+ui.targetSync.value)|0}`;
  ui.nStepReadout.textContent=`${(+ui.nStep.value)|0}`;
  ui.alphaReadout.textContent=(+ui.priorityAlpha.value).toFixed(2);
  ui.betaReadout.textContent=(+ui.priorityBeta.value).toFixed(2);
  ui.pgEntropyReadout.textContent=(+ui.pgEntropy.value).toFixed(3);
  ui.acEntropyReadout.textContent=(+ui.acEntropy.value).toFixed(3);
  ui.acValueCoefReadout.textContent=(+ui.acValueCoef.value).toFixed(2);
  ui.ppoEntropyReadout.textContent=(+ui.ppoEntropy.value).toFixed(3);
  ui.ppoClipReadout.textContent=(+ui.ppoClip.value).toFixed(2);
  ui.ppoLambdaReadout.textContent=(+ui.ppoLambda.value).toFixed(2);
  ui.ppoBatchReadout.textContent=`${(+ui.ppoBatch.value)|0}`;
  ui.ppoEpochsReadout.textContent=`${(+ui.ppoEpochs.value)|0}`;
  ui.ppoValueCoefReadout.textContent=(+ui.ppoValueCoef.value).toFixed(2);
}
function updateRewardReadouts(){
  if(!ui.rewardStep) return;
  ui.rewardStepReadout.textContent=(+ui.rewardStep.value).toFixed(3);
  ui.rewardTurnReadout.textContent=(+ui.rewardTurn.value).toFixed(3);
  ui.rewardApproachReadout.textContent=(+ui.rewardApproach.value).toFixed(3);
  ui.rewardRetreatReadout.textContent=(+ui.rewardRetreat.value).toFixed(3);
  ui.rewardLoopReadout.textContent=(+ui.rewardLoop.value).toFixed(2);
  ui.rewardRevisitReadout.textContent=(+ui.rewardRevisit.value).toFixed(3);
  ui.rewardWallReadout.textContent=(+ui.rewardWall.value).toFixed(1);
  ui.rewardSelfReadout.textContent=(+ui.rewardSelf.value).toFixed(1);
  ui.rewardTimeoutReadout.textContent=(+ui.rewardTimeout.value).toFixed(1);
  ui.rewardTrapReadout.textContent=(+ui.rewardTrap.value).toFixed(2);
  ui.rewardSpaceReadout.textContent=(+ui.rewardSpace.value).toFixed(2);
  ui.rewardFruitReadout.textContent=(+ui.rewardFruit.value).toFixed(1);
  ui.rewardGrowthReadout.textContent=(+ui.rewardGrowth.value).toFixed(1);
  ui.rewardCompactReadout.textContent=(+ui.rewardCompact.value).toFixed(3);
}
function getRewardConfigFromUI(){
  return {
    stepPenalty:+ui.rewardStep.value,
    turnPenalty:+ui.rewardTurn.value,
    approachBonus:+ui.rewardApproach.value,
    retreatPenalty:+ui.rewardRetreat.value,
    loopPenalty:+ui.rewardLoop.value,
    revisitPenalty:+ui.rewardRevisit.value,
    wallPenalty:+ui.rewardWall.value,
    selfPenalty:+ui.rewardSelf.value,
    timeoutPenalty:+ui.rewardTimeout.value,
    trapPenalty:+ui.rewardTrap.value,
    spaceGainBonus:+ui.rewardSpace.value,
    fruitReward:+ui.rewardFruit.value,
    growthBonus:+ui.rewardGrowth.value,
    compactWeight:+ui.rewardCompact.value,
  };
}
function applyRewardsToEnv(){
  rewardConfig=getRewardConfigFromUI();
  vecEnv?.setRewardConfig(rewardConfig);
  env=vecEnv?.getEnv(renderIndex)||env;
  autoPilot?.setRewardConfig?.({...rewardConfig});
}
function applyRewardConfigToUI(config={}){
  if(config.stepPenalty!==undefined) ui.rewardStep.value=config.stepPenalty;
  if(config.turnPenalty!==undefined) ui.rewardTurn.value=config.turnPenalty;
  if(config.approachBonus!==undefined) ui.rewardApproach.value=config.approachBonus;
  if(config.retreatPenalty!==undefined) ui.rewardRetreat.value=config.retreatPenalty;
  if(config.loopPenalty!==undefined) ui.rewardLoop.value=config.loopPenalty;
  if(config.revisitPenalty!==undefined) ui.rewardRevisit.value=config.revisitPenalty;
  if(config.wallPenalty!==undefined) ui.rewardWall.value=config.wallPenalty;
  if(config.selfPenalty!==undefined) ui.rewardSelf.value=config.selfPenalty;
  if(config.timeoutPenalty!==undefined) ui.rewardTimeout.value=config.timeoutPenalty;
  if(config.trapPenalty!==undefined) ui.rewardTrap.value=config.trapPenalty;
  if(config.spaceGainBonus!==undefined) ui.rewardSpace.value=config.spaceGainBonus;
  if(config.fruitReward!==undefined) ui.rewardFruit.value=config.fruitReward;
  if(config.growthBonus!==undefined) ui.rewardGrowth.value=config.growthBonus;
  if(config.compactWeight!==undefined) ui.rewardCompact.value=config.compactWeight;
  updateRewardReadouts();
  applyRewardsToEnv();
}
class BrowserAutoPilot{
  constructor({rewardConfig={}}={}){
    this.history=[];
    this.lossHistory=[];
    this.episode=0;
    this.rewardConfig={...rewardConfig};
    this.boardStages=[
      {size:10,threshold:0},
      {size:14,threshold:60},
      {size:18,threshold:120},
      {size:20,threshold:200},
    ];
    this.stageIndex=0;
    this.lastAdjust={};
    this.bestFruit=0;
    this.agent=null;
    this.evaluationIntervalBase=50;
    this.evaluationInterval=this.evaluationIntervalBase;
    this.minEpisodesForAdjustBase=200;
    this.minEpisodesForAdjust=this.minEpisodesForAdjustBase;
    this.lastEvaluationEpisode=0;
    this.tuningStyle=DEFAULT_AUTO_STYLE;
    this.magnitudeFactor=1;
    this.cooldownFactor=1;
    this.setTuningStyle(DEFAULT_AUTO_STYLE);
  }
  setTuningStyle(style=DEFAULT_AUTO_STYLE){
    const preset=AUTO_TUNING_STYLES[style]||AUTO_TUNING_STYLES[DEFAULT_AUTO_STYLE];
    this.tuningStyle=preset.key;
    this.magnitudeFactor=preset.magnitude;
    this.cooldownFactor=preset.cooldown;
    this.evaluationInterval=Math.max(20,Math.round(this.evaluationIntervalBase*preset.eval));
    this.minEpisodesForAdjust=Math.max(100,Math.round(this.minEpisodesForAdjustBase*preset.start));
  }
  getTuningStyle(){
    return this.tuningStyle;
  }
  setAgent(agent){
    this.agent=agent;
  }
  setRewardConfig(cfg={}){
    this.rewardConfig={...cfg};
  }
  getRewardConfig(){
    return {...this.rewardConfig};
  }
  _scaledCooldown(base){
    return Math.max(1,Math.round(base*this.cooldownFactor));
  }
  _scaleAdd(delta){
    return delta*this.magnitudeFactor;
  }
  _scaleMultiplier(base){
    if(base===1) return 1;
    if(base>1) return 1+(base-1)*this.magnitudeFactor;
    return 1-(1-base)*this.magnitudeFactor;
  }
  recordEpisode({
    fruits=0,
    reward=0,
    steps=0,
    loss=null,
    loopHits=0,
    revisitPenalty=0,
    crash=null,
    timeToFruitTotal=0,
    timeToFruitCount=0,
    rewardBreakdown=null,
  }={}){
    this.history.push({
      fruits,
      reward,
      steps,
      loopHits,
      revisitPenalty,
      crash,
      timeToFruitTotal,
      timeToFruitCount,
      breakdown:rewardBreakdown?{...rewardBreakdown}:null,
    });
    if(this.history.length>6000) this.history.shift();
    if(loss!==null && loss!==undefined){
      this.lossHistory.push(loss);
      if(this.lossHistory.length>6000) this.lossHistory.shift();
    }
    this.episode++;
  }
  _canAdjust(key,cooldown=500){
    const window=this._scaledCooldown(cooldown);
    const last=this.lastAdjust[key]??-Infinity;
    if(this.episode-last<window) return false;
    this.lastAdjust[key]=this.episode;
    return true;
  }
  _averageBreakdown(window=200){
    const recent=this.history.slice(-window).filter(item=>item.breakdown);
    if(!recent.length) return null;
    const totals=Object.fromEntries(REWARD_COMPONENT_KEYS.map(key=>[key,0]));
    recent.forEach(item=>{
      const data=item.breakdown;
      REWARD_COMPONENT_KEYS.forEach(key=>{
        totals[key]+=data[key]??0;
      });
    });
    const scale=1/recent.length;
    REWARD_COMPONENT_KEYS.forEach(key=>{
      totals[key]*=scale;
    });
    return totals;
  }
  getMetrics(){
    const fruits=this.history.map(item=>item.fruits);
    const ma100=movingAverage(fruits,100);
    const ma500=movingAverage(fruits,500);
    const prev100=movingAverage(fruits,100,100);
    const fruitSlope=ma100-prev100;
    const improvement2000=movingAverage(fruits,2000)-movingAverage(fruits,2000,2000);
    const stepsHist=this.history.map(item=>item.steps||0);
    const avgEpisodeLen100=movingAverage(stepsHist,100);
    this.bestFruit=Math.max(this.bestFruit,ma100||0);
    const regression=this.bestFruit>0 && ma100<this.bestFruit*0.75;
    const window500=this.history.slice(-500);
    const totalSteps500=window500.reduce((sum,item)=>sum+(item.steps||0),0);
    const loopHits500=window500.reduce((sum,item)=>sum+(item.loopHits||0),0);
    const revisitPenalty500=window500.reduce((sum,item)=>sum+(item.revisitPenalty||0),0);
    const crashSelfEpisodes=window500.filter(item=>item.crash==='self').length;
    const timeToFruitTotal=window500.reduce((sum,item)=>sum+(item.timeToFruitTotal||0),0);
    const timeToFruitCount=window500.reduce((sum,item)=>sum+(item.timeToFruitCount||0),0);
    const loopHitRate=totalSteps500>0?loopHits500/totalSteps500:0;
    const revisitRate=totalSteps500>0?revisitPenalty500/Math.max(1,totalSteps500):0;
    const crashRateSelf=window500.length?crashSelfEpisodes/window500.length:0;
    const timeToFruitAvg=timeToFruitCount>0?timeToFruitTotal/timeToFruitCount:0;
    const lossValues=this.lossHistory.slice(-200);
    const lossMean=lossValues.length?lossValues.reduce((a,b)=>a+b,0)/lossValues.length:0;
    const lossStd=stddev(lossValues);
    const lossRatio=lossMean>0?lossStd/lossMean:0;
    return {
      maFruit100:ma100,
      maFruit500:ma500,
      fruitSlope,
      improvement2000,
      regression,
      lossMean,
      lossStd,
      lossRatio,
      avgEpisodeLen100,
      loopHitRate,
      revisitRate,
      crashRateSelf,
      timeToFruitAvg,
    };
  }
  maybeAdjust({agent}={}){
    const actor=agent||this.agent;
    const metrics=this.getMetrics();
    const adjustments=[];
    const nextStage=this.boardStages[this.stageIndex+1];
    if(nextStage && metrics.maFruit500>nextStage.threshold && this._canAdjust('board',200)){
      this.stageIndex++;
      adjustments.push({type:'board',size:this.boardStages[this.stageIndex].size});
    }
    if(!actor||actor.kind!=='dqn'){
      return {adjustments,metrics};
    }
    if(this.episode<this.minEpisodesForAdjust){
      return {adjustments,metrics};
    }
    if(this.episode-this.lastEvaluationEpisode<this.evaluationInterval){
      return {adjustments,metrics};
    }
    this.lastEvaluationEpisode=this.episode;
    const breakdownAvg=this._averageBreakdown(240);
    if(metrics.fruitSlope<=0 && metrics.improvement2000<2 && this._canAdjust('epsilon-up',500)){
      const newEnd=clamp(actor.epsEnd+this._scaleAdd(0.03),0.01,0.3);
      const newDecay=clamp(actor.epsDecay*this._scaleMultiplier(1.2),5000,200000);
      actor.setEpsilonSchedule?.({end:newEnd,decay:newDecay});
      adjustments.push({type:'epsilon',end:newEnd,decay:newDecay,reason:'stagnation'});
    }
    if(metrics.regression && this._canAdjust('regression',800)){
      const newEnd=clamp(actor.epsEnd+this._scaleAdd(0.02),0.01,0.3);
      actor.setEpsilonSchedule?.({end:newEnd});
      const newLr=Math.max(0.0002,actor.lr*this._scaleMultiplier(0.8));
      actor.setLearningRate(newLr);
      adjustments.push({type:'lr',value:newLr,reason:'regression'});
    }else if(metrics.fruitSlope>0 && actor.epsEnd>0.12 && this._canAdjust('epsilon-down',800)){
      const newEnd=clamp(actor.epsEnd-this._scaleAdd(0.02),0.01,0.3);
      const newDecay=clamp(actor.epsDecay*this._scaleMultiplier(0.9),5000,200000);
      actor.setEpsilonSchedule?.({end:newEnd,decay:newDecay});
      adjustments.push({type:'epsilon',end:newEnd,decay:newDecay,reason:'recovery'});
    }
    if(metrics.lossRatio>0.85 && this._canAdjust('lr-down',1000)){
      const newLr=Math.max(0.0002,actor.lr*this._scaleMultiplier(0.85));
      actor.setLearningRate(newLr);
      adjustments.push({type:'lr',value:newLr,reason:'loss_ratio'});
    }else if(metrics.lossRatio<0.3 && this._canAdjust('lr-up',1200)){
      const newLr=Math.min(0.0005,actor.lr*this._scaleMultiplier(1.05));
      actor.setLearningRate(newLr);
      adjustments.push({type:'lr',value:newLr,reason:'recover'});
    }
    this._adjustRewards(actor,metrics,adjustments,breakdownAvg);
    return {adjustments,metrics};
  }
  _adjustRewards(actor,metrics,adjustments,breakdownAvg){
    if(!metrics) return;
    const rewardConfig=this.rewardConfig||{};
    if(metrics.loopHitRate>0.01 && metrics.fruitSlope<=0 && this._canAdjust('reward-loop',500)){
      rewardConfig.loopPenalty=clamp((rewardConfig.loopPenalty??0.5)+this._scaleAdd(0.05),0,1);
      rewardConfig.compactWeight=0;
      adjustments.push({type:'reward',key:'loopPenalty',value:rewardConfig.loopPenalty,reason:'loop_penalty'});
    }
    if(metrics.revisitRate>0.01 && this._canAdjust('reward-revisit',500)){
      rewardConfig.revisitPenalty=clamp((rewardConfig.revisitPenalty??0.05)+this._scaleAdd(0.005),0,0.1);
      const newEnd=clamp((actor?.epsEnd??0.12)+this._scaleAdd(0.02),0.01,0.3);
      actor?.setEpsilonSchedule?.({end:newEnd});
      adjustments.push({type:'reward',key:'revisitPenalty',value:rewardConfig.revisitPenalty,reason:'revisit_penalty'});
      adjustments.push({type:'epsilon',end:newEnd,reason:'revisit_penalty'});
    }
    if(metrics.crashRateSelf>0.4 && this._canAdjust('reward-self',500)){
      rewardConfig.selfPenalty=clamp((rewardConfig.selfPenalty??25.5)+this._scaleAdd(1),0,30);
      rewardConfig.turnPenalty=clamp((rewardConfig.turnPenalty??0.001)-this._scaleAdd(0.0002),0,0.02);
      adjustments.push({type:'reward',key:'selfPenalty',value:rewardConfig.selfPenalty,reason:'self_penalty'});
    }
    let approachAdjusted=false;
    let retreatAdjusted=false;
    if(metrics.timeToFruitAvg>200 && metrics.loopHitRate<0.005 && metrics.revisitRate<0.005 && this._canAdjust('reward-fruit',500)){
      rewardConfig.approachBonus=clamp((rewardConfig.approachBonus??0.03)+this._scaleAdd(0.005),0,0.1);
      rewardConfig.retreatPenalty=clamp((rewardConfig.retreatPenalty??0.03)+this._scaleAdd(0.005),0,0.1);
      adjustments.push({type:'reward',key:'approachBonus',value:rewardConfig.approachBonus,reason:'slow_fruit'});
      adjustments.push({type:'reward',key:'retreatPenalty',value:rewardConfig.retreatPenalty,reason:'slow_fruit'});
      approachAdjusted=true;
      retreatAdjusted=true;
    }
    if(breakdownAvg){
      const avgFruit=breakdownAvg.fruitReward??0;
      const avgStep=breakdownAvg.stepPenalty??0;
      const avgRetreat=breakdownAvg.retreatPenalty??0;
      const avgApproach=breakdownAvg.approachBonus??0;
      if(avgStep<0 && Math.abs(avgStep)>Math.max(0.5,Math.abs(avgFruit))*1.3 && this._canAdjust('reward-step-down',900)){
        rewardConfig.stepPenalty=clamp((rewardConfig.stepPenalty??0.01)*this._scaleMultiplier(0.9),0.001,0.05);
        adjustments.push({type:'reward',key:'stepPenalty',value:rewardConfig.stepPenalty,reason:'step_drag'});
      }
      if(!retreatAdjusted && Math.abs(avgRetreat)>(Math.abs(avgApproach)+0.5) && this._canAdjust('reward-retreat-down',900)){
        rewardConfig.retreatPenalty=Math.max(0,(rewardConfig.retreatPenalty??0.03)-this._scaleAdd(0.005));
        adjustments.push({type:'reward',key:'retreatPenalty',value:rewardConfig.retreatPenalty,reason:'retreat_load'});
      }
      if(avgFruit<3 && metrics.timeToFruitAvg>160 && this._canAdjust('reward-fruit-extra',1200)){
        rewardConfig.fruitReward=clamp((rewardConfig.fruitReward??10)+this._scaleAdd(1),0,30);
        adjustments.push({type:'reward',key:'fruitReward',value:rewardConfig.fruitReward,reason:'fruit_support'});
      }
      if(!approachAdjusted && avgApproach<1 && metrics.timeToFruitAvg>150 && this._canAdjust('reward-approach-boost',1200)){
        rewardConfig.approachBonus=clamp((rewardConfig.approachBonus??0.03)+this._scaleAdd(0.005),0,0.1);
        adjustments.push({type:'reward',key:'approachBonus',value:rewardConfig.approachBonus,reason:'fruit_support'});
      }
    }
    this.rewardConfig={...rewardConfig};
  }
}
function updateAdvancedVisibility(){
  const type=AGENT_PRESETS[currentAlgoKey]?.type||'dqn';
  Object.entries(ui.advancedSections).forEach(([key,el])=>{
    if(!el) return;
    el.classList.toggle('hidden',key!==type);
  });
}
function instantiateAgent(key,opts={}){
  const {useCurrentUI=false,overrideDefaults=null}=opts;
  currentAlgoKey=AGENT_PRESETS[key]?key:'dueling';
  ui.algoSelect.value=currentAlgoKey;
  const preset=AGENT_PRESETS[currentAlgoKey];
  const appliedDefaults={...preset.defaults};
  if(overrideDefaults){
    if(Array.isArray(overrideDefaults.layers)) appliedDefaults.layers=overrideDefaults.layers.slice();
    if(overrideDefaults.learnRepeats!==undefined) appliedDefaults.learnRepeats=overrideDefaults.learnRepeats;
    if(overrideDefaults.dueling!==undefined) appliedDefaults.dueling=overrideDefaults.dueling;
    if(overrideDefaults.double!==undefined) appliedDefaults.double=overrideDefaults.double;
  }
  if(!useCurrentUI){
    applyPresetToUI(appliedDefaults);
  }
  agent=preset.create(stateDim,actionDim,{
    gamma:+ui.gamma.value,
    lr:+ui.lr.value,
    epsStart:+ui.epsStart.value,
    epsEnd:+ui.epsEnd.value,
    epsDecay:+ui.epsDecay.value,
    batch:+ui.batchSize.value,
    bufferSize:+ui.bufferSize.value,
    priorityAlpha:+ui.priorityAlpha.value,
    priorityBeta:+ui.priorityBeta.value,
    nStep:+ui.nStep.value,
    targetSync:+ui.targetSync.value,
    entropy:+ui.pgEntropy.value,
    valueCoef:+ui.acValueCoef.value,
    clip:+ui.ppoClip.value,
    lambda:+ui.ppoLambda.value,
    batch:+ui.ppoBatch.value,
    epochs:+ui.ppoEpochs.value,
    learnRepeats:appliedDefaults.learnRepeats,
    layers:appliedDefaults.layers,
    dueling:appliedDefaults.dueling,
    double:appliedDefaults.double,
  });
  agent.learnRepeats=(appliedDefaults.learnRepeats??preset.defaults.learnRepeats)??agent.learnRepeats??1;
  targetSyncSteps=agent.kind==='dqn'? (+ui.targetSync.value||2000):Infinity;
  updateAdvancedVisibility();
  ui.algoBadge.textContent=preset.badge||preset.label;
  ui.algoDescription.textContent=preset.description;
  if(typeof presetSelectEl!=='undefined'&&presetSelectEl&&presetSelectEl.value!==currentAlgoKey){
    presetSelectEl.value=currentAlgoKey;
  }
  updateBadgeMetrics();
  currentStagePresetKey='';
  refreshStagePresetOptions(currentAlgoKey,{preserveSelection:false});
  resetTrainingStats();
  if(agent.kind!=='dqn' && trainingMode==='auto'){
    setTrainingMode('manual');
  }
  if(agent.kind!=='dqn'){
    ui.envCount.value='1';
    ui.envCount.disabled=true;
    reconfigureEnvironment({count:1,size:+ui.gridSize.value,force:true});
  }else{
    ui.envCount.disabled=false;
    const desiredCount=Math.max(1,+ui.envCount.value||envCount);
    reconfigureEnvironment({count:desiredCount,size:+ui.gridSize.value,force:true});
  }
  autoPilot?.setAgent?.(agent);
  updateControlAvailability();
  updateReadouts();
}
function applyPresetToUI(config){
  if(config.gamma!==undefined) ui.gamma.value=config.gamma;
  if(config.lr!==undefined) ui.lr.value=config.lr;
  if(config.epsStart!==undefined) ui.epsStart.value=config.epsStart;
  if(config.epsEnd!==undefined) ui.epsEnd.value=config.epsEnd;
  if(config.epsDecay!==undefined) ui.epsDecay.value=config.epsDecay;
  if(config.batch!==undefined) ui.batchSize.value=config.batch;
  if(config.bufferSize!==undefined) ui.bufferSize.value=config.bufferSize;
  if(config.targetSync!==undefined) ui.targetSync.value=config.targetSync;
  if(config.nStep!==undefined) ui.nStep.value=config.nStep;
  if(config.priorityAlpha!==undefined) ui.priorityAlpha.value=config.priorityAlpha;
  if(config.priorityBeta!==undefined) ui.priorityBeta.value=config.priorityBeta;
  if(config.entropy!==undefined){
    ui.pgEntropy.value=config.entropy;
    ui.acEntropy.value=config.entropy;
    ui.ppoEntropy.value=config.entropy;
  }
  if(config.valueCoef!==undefined){
    ui.acValueCoef.value=config.valueCoef;
    ui.ppoValueCoef.value=config.valueCoef;
  }
  if(config.clip!==undefined) ui.ppoClip.value=config.clip;
  if(config.lambda!==undefined) ui.ppoLambda.value=config.lambda;
  if(config.batch!==undefined) ui.ppoBatch.value=config.batch;
  if(config.epochs!==undefined) ui.ppoEpochs.value=config.epochs;
  updateReadouts();
}
function applyPreset(preset,options={}){
  if(!preset) return;
  const {preserveProgress=false}=options;
  const agentKey=getStageAgentKey(preset,currentAlgoKey);
  const overrides={};
  if(Array.isArray(preset.hiddenSizes)) overrides.layers=preset.hiddenSizes.slice();
  if(preset.learnRepeats!==undefined) overrides.learnRepeats=preset.learnRepeats;
  if(preset.dueling!==undefined) overrides.dueling=preset.dueling;
  if(preset.double!==undefined) overrides.double=preset.double;
  const {
    label,
    agent:_stageAgent,
    rewardConfig,
    hiddenSizes:_hiddenSizes,
    learnRepeats:_learnRepeats,
    lam,
    clipRatio,
    stepsPerEpoch:_stepsPerEpoch,
    trainIters,
    minibatchSize,
    batchSize,
    ...rest
  }=preset;
  const config={...rest};
  if(batchSize!==undefined) config.batch=batchSize;
  if(minibatchSize!==undefined) config.batch=minibatchSize;
  if(lam!==undefined) config.lambda=lam;
  if(clipRatio!==undefined) config.clip=clipRatio;
  if(trainIters!==undefined) config.epochs=trainIters;
  // remove properties not handled by UI
  delete config.agent;
  delete config.label;
  delete config.rewardConfig;
  delete config.hiddenSizes;
  delete config.learnRepeats;
  delete config.lam;
  delete config.clipRatio;
  delete config.stepsPerEpoch;
  delete config.trainIters;
  delete config.minibatchSize;
  delete config.batchSize;
  applyPresetToUI(config);
  if(rewardConfig) applyRewardConfigToUI({...rewardConfig});
  applyRewardsToEnv();
  const overrideArg=Object.keys(overrides).length?overrides:null;
  const targetPreset=AGENT_PRESETS[agentKey];
  currentAlgoKey=agentKey;
  ui.algoSelect.value=agentKey;
  if(presetSelectEl) presetSelectEl.value=agentKey;
  const hasProgress=episode>0||totalSteps>0;
  const canPreserve=preserveProgress&&agent&&targetPreset&&currentAlgoKey===agentKey&&hasProgress;
  if(canPreserve){
    if(overrideArg&&overrideArg.learnRepeats!==undefined) agent.learnRepeats=overrideArg.learnRepeats;
    ui.algoBadge.textContent=label||targetPreset.badge||targetPreset.label;
    applyConfigToAgent();
    updateReadouts();
    return;
  }
  instantiateAgent(agentKey,{useCurrentUI:true,overrideDefaults:overrideArg});
  if(label) ui.algoBadge.textContent=label;
  applyConfigToAgent();
  updateReadouts();
}

function refreshStagePresetOptions(agentKey,{preserveSelection=true}={}){
  if(!stageSelectEl) return;
  const fallback=agentKey&&AGENT_PRESETS[agentKey]?agentKey:'dueling';
  const targetAgent=resolveAgentKey(agentKey,fallback);
  const previousValue=preserveSelection?(stageSelectEl.value||currentStagePresetKey):'';
  stageSelectEl.innerHTML='';
  const placeholder=document.createElement('option');
  placeholder.value='';
  placeholder.textContent=STAGE_PRESET_PLACEHOLDER;
  stageSelectEl.appendChild(placeholder);
  const entries=[];
  for(const [key,preset] of Object.entries(STAGE_PRESETS)){
    if(resolveAgentKey(preset.agent,targetAgent)===targetAgent){
      entries.push({key,label:preset.label});
    }
  }
  const hasEntries=entries.length>0;
  if(stagePresetField){
    stagePresetField.classList.toggle('hidden',!hasEntries);
  }
  if(!hasEntries){
    placeholder.textContent='No stage presets available';
    stageSelectEl.disabled=true;
    currentStagePresetKey='';
    stageSelectEl.value='';
    return;
  }
  stageSelectEl.disabled=false;
  const availableKeys=new Set();
  entries.forEach(({key,label})=>{
    const option=document.createElement('option');
    option.value=key;
    option.textContent=label;
    stageSelectEl.appendChild(option);
    availableKeys.add(key);
  });
  if(preserveSelection&&previousValue&&availableKeys.has(previousValue)){
    stageSelectEl.value=previousValue;
    currentStagePresetKey=previousValue;
    return;
  }
  if(currentStagePresetKey&&availableKeys.has(currentStagePresetKey)){
    stageSelectEl.value=currentStagePresetKey;
    return;
  }
  currentStagePresetKey='';
  stageSelectEl.value='';
}
function resetTrainingStats(){
  episode=0;
  totalSteps=0;
  bestLen=0;
  rwHist.length=0;
  fruitHist.length=0;
  lossHist.length=0;
  progressPoints.length=0;
  rewardTelemetry.reset();
  updateStatsUI();
  updateRewardTelemetryUI();
  updateProgressChart();
  renderTick=0;
  contexts.forEach(ctx=>ctx.needsReset=true);
  if(autoPilot){
    autoPilot.history.length=0;
    autoPilot.lossHistory.length=0;
    autoPilot.episode=0;
    autoPilot.bestFruit=0;
    autoPilot.lastAdjust={};
    autoPilot.lastEvaluationEpisode=0;
    autoPilot.rewardConfig={...rewardConfig};
    autoPilot.setTuningStyle(aiAutoTuneStyle);
  }
  lastAutoMetrics=null;
  lastAutoSummaryEpisode=0;
}
function updateStatsUI(){
  ui.kEpisodes.textContent=episode;
  ui.kAvgRw.textContent=avg(rwHist,100).toFixed(2);
  ui.kBest.textContent=bestLen;
  ui.kFruitRate.textContent=avg(fruitHist,100).toFixed(2);
}
function setProgressChartCollapsed(collapsed){
  if(!ui.progressChartPanel||!ui.progressChartBody||!ui.progressChartToggle) return;
  ui.progressChartPanel.classList.toggle('collapsed',!!collapsed);
  if(collapsed){
    ui.progressChartBody.setAttribute('hidden','');
  }else{
    ui.progressChartBody.removeAttribute('hidden');
  }
  ui.progressChartToggle.setAttribute('aria-expanded',String(!collapsed));
  ui.progressChartToggle.textContent=collapsed?'Visa diagram':'Dölj diagram';
}
function updateProgressChart(){
  if(!ui.progressChartSvg) return;
  const data=progressPoints.slice(-PROGRESS_POINTS_MAX);
  const hasData=data.length>0;
  const elements=[['progressChartCanvas',!hasData],['progressChartLegend',!hasData],['progressChartMeta',!hasData]];
  elements.forEach(([key,shouldHide])=>{
    const el=ui[key];
    if(!el) return;
    el.classList.toggle('hidden',shouldHide);
  });
  if(ui.progressChartEmpty) ui.progressChartEmpty.classList.toggle('hidden',hasData);
  if(!hasData){
    ui.progressRewardPath?.setAttribute('d','');
    ui.progressFruitPath?.setAttribute('d','');
    if(ui.progressChartGrid) ui.progressChartGrid.innerHTML='';
    if(ui.progressChartRange) ui.progressChartRange.textContent='—';
    return;
  }
  const width=PROGRESS_CHART_WIDTH;
  const height=PROGRESS_CHART_HEIGHT;
  const rewardVals=data.map(p=>p.reward).filter(v=>Number.isFinite(v));
  const fruitVals=data.map(p=>p.fruit).filter(v=>Number.isFinite(v));
  const values=[...rewardVals,...fruitVals];
  let min=Math.min(...values);
  let max=Math.max(...values);
  if(!values.length||!Number.isFinite(min)||!Number.isFinite(max)){
    min=0;
    max=1;
  }
  if(min===max){
    const pad=Math.abs(min)||1;
    min-=pad*0.5;
    max+=pad*0.5;
  }
  const pad=(max-min)*0.08;
  min-=pad;
  max+=pad;
  const toX=index=>data.length>1?(index/(data.length-1))*width:width/2;
  const toY=value=>{
    const norm=(value-min)/(max-min||1);
    const y=height-(norm*height);
    return Math.min(height,Math.max(0,y));
  };
  const rewardPath=data.map((point,i)=>`${i===0?'M':'L'}${toX(i).toFixed(1)},${toY(point.reward).toFixed(1)}`).join(' ');
  const fruitPath=data.map((point,i)=>`${i===0?'M':'L'}${toX(i).toFixed(1)},${toY(point.fruit).toFixed(1)}`).join(' ');
  ui.progressRewardPath?.setAttribute('d',rewardPath);
  ui.progressFruitPath?.setAttribute('d',fruitPath);
  if(ui.progressChartGrid){
    const ticks=[max,(max+min)/2,min];
    ui.progressChartGrid.innerHTML=ticks.map(value=>{
      const y=toY(value);
      return `<line x1="0" y1="${y.toFixed(1)}" x2="${width}" y2="${y.toFixed(1)}"></line>`+
        `<text x="8" y="${y.toFixed(1)}" dominant-baseline="middle">${formatMetric(value,2)}</text>`;
    }).join('');
  }
  const first=data[0];
  const last=data[data.length-1];
  if(ui.progressChartRange){
    ui.progressChartRange.textContent=`${first.startEpisode}–${last.episode}`;
  }
}
function recordProgressPoint(){
  if(rwHist.length<100||fruitHist.length<100) return;
  const rewardAvg=avg(rwHist,100);
  const fruitAvg=avg(fruitHist,100);
  progressPoints.push({
    episode,
    startEpisode:Math.max(1,episode-99),
    reward:rewardAvg,
    fruit:fruitAvg,
  });
  if(progressPoints.length>PROGRESS_POINTS_MAX) progressPoints.shift();
  updateProgressChart();
}
function updateRewardTelemetryUI(){
  if(!ui.rewardTelemetryBody) return;
  const {rows,total}=rewardTelemetry.summary();
  const frag=document.createDocumentFragment();
  const valueClass=value=>{
    if(value>1e-3) return 'positive';
    if(value<-1e-3) return 'negative';
    return 'neutral';
  };
  rows.forEach(row=>{
    const tr=document.createElement('tr');
    const lastClass=valueClass(row.last);
    const avgClass=valueClass(row.avg100);
    const trendClass=valueClass(row.trend);
    tr.innerHTML=`
      <td>${row.label}</td>
      <td class="mono ${lastClass}">${formatMetric(row.last,2)}</td>
      <td class="mono ${avgClass}">${formatMetric(row.avg100,2)}</td>
      <td class="mono ${avgClass}">${formatMetric(row.avg500,2)}</td>
      <td class="mono share">${formatPercent(row.share,1)}</td>
      <td class="mono trend ${trendClass}">${formatSigned(row.trend,2)}</td>
    `;
    frag.appendChild(tr);
  });
  ui.rewardTelemetryBody.innerHTML='';
  ui.rewardTelemetryBody.appendChild(frag);
  if(ui.rewardTelemetrySummary){
    const netClass=valueClass(total.avg100);
    const trendClass=valueClass(total.trend);
    ui.rewardTelemetrySummary.textContent=`${formatMetric(total.avg100,2)} (trend ${formatSigned(total.trend,2)})`;
    ui.rewardTelemetrySummary.className=`mono ${netClass}`;
    ui.rewardTelemetrySummary.dataset.trend=trendClass;
  }
}
function flash(message,danger=false){
  ui.trainState.textContent=message;
  ui.trainState.style.background=danger?'var(--danger)':'#1a1f46';
  ui.trainState.style.borderColor=danger?'#ff8da4':'#2a2f61';
  setTimeout(()=>{
    ui.trainState.textContent=watching?'watching':(training?'training':'idle');
    ui.trainState.style.background='#1a1f46';
    ui.trainState.style.borderColor='#2a2f61';
  },1200);
}

/* ---------------- Training loop ---------------- */
async function finalizeContextEpisode(ctx,envIndex){
  agent.drainPending?.(envIndex);
  const loss=await agent.finishEpisode?.(ctx);
  if(loss!==null && loss!==undefined){
    lossHist.push(loss);
    if(lossHist.length>1000) lossHist.shift();
  }
  episode++;
  rwHist.push(ctx.totalReward);
  if(rwHist.length>1000) rwHist.shift();
  fruitHist.push(ctx.fruits);
  if(fruitHist.length>1000) fruitHist.shift();
  const envRef=vecEnv.getEnv(envIndex);
  if(envRef) bestLen=Math.max(bestLen,envRef.snake.length);
  const breakdown=envRef?.getEpisodeBreakdown?.();
  const loopHits=envRef?.loopHits??0;
  const revisitPenalty=envRef?.revisitAccum??0;
  const crashType=envRef?.lastCrash??null;
  const timeToFruitTotal=envRef?.timeToFruitAccum??0;
  const timeToFruitCount=envRef?.timeToFruitCount??0;
  const avgTimeToFruit=timeToFruitCount>0?timeToFruitTotal/timeToFruitCount:null;
  updateStatsUI();
  rewardTelemetry.record(breakdown);
  updateRewardTelemetryUI();
  if(episode%100===0) recordProgressPoint();
  if(checkpointEnabled && checkpointEpisodeInterval>0 && episode%checkpointEpisodeInterval===0){
    try{
      const snapshot=await buildAppState();
      await saveCheckpoint(snapshot);
    }catch(err){
      console.error('Periodic checkpoint save failed',err);
    }
  }
  const latestLoss=lossHist.length?lossHist[lossHist.length-1]:null;
  let adjustments=[];
  if(trainingMode==='auto' && autoPilot){
    autoPilot.setAgent(agent);
    autoPilot.recordEpisode({
      fruits:ctx.fruits,
      reward:ctx.totalReward,
      steps:ctx.steps,
      loss:latestLoss,
      loopHits,
      revisitPenalty,
      crash:crashType,
      timeToFruitTotal,
      timeToFruitCount,
      rewardBreakdown:breakdown,
    });
    const res=autoPilot.maybeAdjust({agent});
    const metrics=res?.metrics||null;
    const autoEpisode=autoPilot?.episode||0;
    if(metrics) lastAutoMetrics=metrics;
    if(res?.adjustments?.length){
      adjustments=res.adjustments.map(adj=>({
        ...adj,
        metrics,
        episode:autoEpisode,
      }));
      lastAutoSummaryEpisode=autoEpisode;
    }else if(metrics && autoEpisode-lastAutoSummaryEpisode>=200){
      logAutoSummary(metrics,autoEpisode);
      lastAutoSummaryEpisode=autoEpisode;
    }
  }
  if(trainingMode==='auto' && autoRunLimit>0){
    if(autoRunStopEpisode && episode>=autoRunStopEpisode){
      const preset=AUTO_TUNING_STYLES[aiAutoTuneStyle]||AUTO_TUNING_STYLES[DEFAULT_AUTO_STYLE];
      logAutoEvent({
        title:'Auto-run pausad',
        detail:`${autoRunLimit} episoder i ${preset.label.toLowerCase()} läge`,
        tone:'summary',
        episode:autoPilot?.episode||episode,
      });
      flash('Mål nått – AI Auto-Tune pausad.');
      stopTraining({reachedTarget:true});
      autoRunStopEpisode=null;
    }else if(autoRunStopEpisode){
      updateAiRunLimitHint();
    }
  }
  aiEpisodeHistory.push({
    episode,
    reward:ctx.totalReward,
    fruits:ctx.fruits,
    steps:ctx.steps,
    loopHits,
    revisitPenalty,
    crash:crashType||'none',
    timeToFruitAvg:avgTimeToFruit,
    breakdown:breakdown?{...breakdown}:null,
  });
  if(aiEpisodeHistory.length>6000) aiEpisodeHistory.shift();
  if(aiTuner) aiTuner.maybeTune({episode});
  ctx.totalReward=0;
  ctx.fruits=0;
  ctx.steps=0;
  ctx.needsReset=true;
  ctx.state=null;
  return adjustments;
}
async function applyAutoAdjustments(adjustments){
  if(!Array.isArray(adjustments)||!adjustments.length) return;
  let nextBoard=null;
  let rewardAdjusted=false;
  adjustments.forEach(adj=>{
    if(adj.type==='board'){
      nextBoard=adj.size;
    }else if(adj.type==='reward'){
      rewardAdjusted=true;
    }
  });
  if(nextBoard){
    ui.gridSize.value=`${nextBoard}`;
    updateGridLabel();
    reconfigureEnvironment({size:nextBoard,force:true});
    flash(`Curriculum: ${nextBoard}×${nextBoard}`);
  }
  if(rewardAdjusted){
    const cfg=autoPilot?.getRewardConfig?.()||{...rewardConfig};
    applyRewardConfigToUI(cfg);
  }
  if(agent?.kind==='dqn'){
    ui.epsStart.value=agent.epsStart.toFixed(2);
    ui.epsEnd.value=agent.epsEnd.toFixed(2);
    ui.epsDecay.value=`${Math.round(agent.epsDecay)}`;
    ui.lr.value=agent.lr.toFixed(4);
  }
  updateReadouts();
  logAutoAdjustments(adjustments);
}
async function performVectorStep(mode){
  ensureContextPool();
  if(!contexts.length) return false;
  contexts.forEach(ctx=>{
    if(ctx.needsReset || !ctx.state){
      const state=vecEnv.resetEnv(ctx.envIndex);
      ctx.state=Float32Array.from(state);
      ctx.totalReward=0;
      ctx.fruits=0;
      ctx.steps=0;
      ctx.needsReset=false;
      if(ctx.envIndex===renderIndex){
        env=vecEnv.getEnv(renderIndex)||env;
        if(env) setImmediateState(env);
      }
    }
  });
  env=vecEnv.getEnv(renderIndex)||env;
  const displayEnv=env;
  const shouldRender=(renderTick%mode.renderEvery===0);
  let before=null;
  if(shouldRender && displayEnv){
    before=snapshotEnv(displayEnv);
  }
  const actions=contexts.map(ctx=>agent.act(ctx.state));
  const {nextStates,rewards,dones,ateFruit}=vecEnv.step(actions);
  renderTick++;
  if(shouldRender && displayEnv){
    const after=snapshotEnv(displayEnv);
    enqueueRenderFrame(before,after,mode.frameMs);
    await waitForRenderCapacity(mode.queueTarget);
  }
  const pendingAdjustments=[];
  for(let i=0;i<contexts.length;i++){
    const ctx=contexts[i];
    const reward=rewards[i];
    const nextState=nextStates[i];
    const done=dones[i];
    const ate=ateFruit[i];
    if(agent.kind==='dqn'){
      agent.recordTransition(i,ctx.state,actions[i],reward,nextState,done);
    }else{
      agent.recordTransition(ctx.state,actions[i],reward,nextState,done);
    }
    ctx.state=nextState;
    ctx.totalReward+=reward;
    if(ate||reward>1) ctx.fruits++;
    ctx.steps++;
    totalSteps++;
    if(done){
      const adjustments=await finalizeContextEpisode(ctx,i);
      pendingAdjustments.push(...adjustments);
    }
  }
  const repeats=agent.learnRepeats??1;
  for(let i=0;i<repeats;i++){
    const loss=await agent.learn();
    if(loss!==null && loss!==undefined){
      lossHist.push(loss);
      if(lossHist.length>1000) lossHist.shift();
    }
  }
  if(agent.kind==='dqn' && targetSyncSteps>0 && totalSteps%targetSyncSteps===0){
    agent.syncTarget();
  }
  if(agent.updateEpsilon){
    const eps=agent.updateEpsilon(totalSteps);
    if(agent.kind==='dqn' && eps!==undefined){
      ui.epsReadout.textContent=eps.toFixed(2);
    }
  }
  if(totalSteps%32===0) await tf.nextFrame();
  if(pendingAdjustments.length){
    await applyAutoAdjustments(pendingAdjustments);
  }
  return true;
}
async function trainingLoop(ts){
  if(!training||watching){
    trainingToken=0;
    return;
  }
  const mode=playbackModes[playbackMode]||playbackModes.cinematic;
  if(ts-lastFrame<mode.frameMs){
    trainingToken=requestAnimationFrame(trainingLoop);
    return;
  }
  lastFrame=ts;
  for(let i=0;i<mode.stepsPerFrame;i++){
    const cont=await performVectorStep(mode);
    if(!cont||!training||watching) break;
  }
  if(training&&!watching){
    trainingToken=requestAnimationFrame(trainingLoop);
  }else{
    trainingToken=0;
  }
}
function startTraining(){
  if(training||watching||!agent) return;
  ensureContextPool();
  autoPilot?.setAgent?.(agent);
  training=true;
  ui.trainState.textContent='training';
  scheduleAutoRunTarget();
  lastFrame=0;
  if(!trainingToken) trainingToken=requestAnimationFrame(trainingLoop);
}
function stopTraining(opts={}){
  const {reachedTarget=false}=opts||{};
  if(!training) return;
  training=false;
  if(trainingToken){
    cancelAnimationFrame(trainingToken);
    trainingToken=0;
  }
  ui.trainState.textContent='idle';
  autoRunStopEpisode=null;
  updateAiRunLimitHint(reachedTarget);
}
async function playSingleEpisode(){
  if(training||watching) return;
  if(envCount>1){
    flash('Step mode requires envCount = 1',true);
    return;
  }
  ensureContextPool();
  const target=episode+1;
  const mode=playbackModes.cinematic;
  while(episode<target){
    await performVectorStep(mode);
  }
}
function previewDeath(env,action){
  const d=env.dir;
  const L={x:-d.y,y:d.x};
  const R={x:d.y,y:-d.x};
  const F=d;
  const dir=action===1?L:action===2?R:F;
  const h=env.snake[0];
  const nx=h.x+dir.x;
  const ny=h.y+dir.y;
  const tail=env.snake[env.snake.length-1];
  const willGrow=(nx===env.fruit.x && ny===env.fruit.y);
  const hitsWall=nx<0||ny<0||nx>=env.cols||ny>=env.rows;
  const key=`${nx},${ny}`;
  const hitsBody=env.snakeSet.has(key) && !(tail && tail.x===nx && tail.y===ny && !willGrow);
  return hitsWall||hitsBody;
}
async function watchSmoothEpisode(){
  if(watching||!agent) return;
  const wasTraining=training;
  if(wasTraining) stopTraining();
  if(liveViewHidden) setLiveViewHidden(false);
  watching=true;
  ui.trainState.textContent='watching';
  ui.btnWatch.disabled=true;
  try{
    await waitForRenderIdle();
    if(window.showDirectoryPicker){
      try{
        const checkpoint=await readLatestCheckpoint();
        await applyCheckpointData(checkpoint);
      }catch(err){
        console.warn('Failed to read latest-checkpoint',err);
      }
    }
    const desired=+ui.gridSize.value;
    if(env.cols!==desired||env.rows!==desired){
      resetEnvironment(desired,true);
    }
    let state=env.reset();
    setImmediateState(env);
    const mode=playbackModes.watch;
    const maxSteps=COLS*ROWS*6;
    let steps=0;
    let done=false;
    while(!done && steps<maxSteps){
      const before=snapshotEnv(env);
      let action=agent.greedyAction(state);
      if(previewDeath(env,action)){
        for(const alt of [0,1,2]){
          if(!previewDeath(env,alt)){ action=alt; break; }
        }
      }
      const {state:nextState,done:finished}=env.step(action);
      const after=snapshotEnv(env);
      enqueueRenderFrame(before,after,mode.frameMs);
      await waitForRenderCapacity(mode.queueTarget);
      await tf.nextFrame();
      state=nextState;
      done=finished;
      steps++;
    }
    await waitForRenderIdle();
    bestLen=Math.max(bestLen,env.snake.length);
    ui.kBest.textContent=bestLen;
  }finally{
    watching=false;
    ui.btnWatch.disabled=false;
    ui.trainState.textContent=wasTraining?'training':'idle';
    if(wasTraining) startTraining();
  }
}

/* ---------------- Save / load ---------------- */
function ensureFileAccessSupport(){
  const secureOk=window.isSecureContext||['localhost','127.0.0.1','::1'].includes(window.location.hostname);
  if(!('showSaveFilePicker' in window)){
    if(!checkpointSupportWarned){
      console.warn('File System Access API is unavailable in this browser; checkpoint saving disabled.');
      checkpointSupportWarned=true;
    }
    return false;
  }
  if(!secureOk){
    if(!checkpointSupportWarned){
      console.warn('Checkpoint saving requires HTTPS or localhost to access the File System Access API.');
      checkpointSupportWarned=true;
    }
    return false;
  }
  return true;
}

function updateCheckpointToggleUI(){
  if(!ui.btnCheckpointToggle) return;
  ui.btnCheckpointToggle.classList.toggle('active',checkpointEnabled);
  ui.btnCheckpointToggle.textContent=checkpointEnabled?'Disable Checkpoint Save':'Enable Checkpoint Save';
  ui.btnCheckpointToggle.setAttribute('aria-pressed',checkpointEnabled?'true':'false');
}

async function handleCheckpointToggle(){
  if(checkpointEnabled){
    checkpointEnabled=false;
    updateCheckpointToggleUI();
    return;
  }
  if(!ensureFileAccessSupport()){
    flash('File access unsupported',true);
    return;
  }
  try{
    if(!checkpointFileHandle){
      checkpointFileHandle=await window.showSaveFilePicker({
        suggestedName:'snake-training-checkpoint.json',
        types:[{
          description:'JSON files',
          accept:{'application/json':['.json']},
        }],
      });
    }
    if(checkpointFileHandle?.queryPermission){
      const status=await checkpointFileHandle.queryPermission({mode:'readwrite'});
      if(status==='denied') throw new Error('Permission denied');
      if(status==='prompt'){
        const granted=await checkpointFileHandle.requestPermission({mode:'readwrite'});
        if(granted!=='granted') throw new Error('Permission denied');
      }
    }else if(checkpointFileHandle?.requestPermission){
      const granted=await checkpointFileHandle.requestPermission({mode:'readwrite'});
      if(granted!=='granted') throw new Error('Permission denied');
    }
    checkpointEnabled=true;
    updateCheckpointToggleUI();
  }catch(err){
    if(err?.name==='AbortError'){
      console.warn('Checkpoint save selection was cancelled by the user.');
    }else{
      console.error('Failed to enable checkpoint saving',err);
      flash('Failed to enable checkpoint save',true);
    }
    checkpointEnabled=false;
    updateCheckpointToggleUI();
  }
}

async function saveCheckpoint(data){
  if(!checkpointEnabled||!checkpointFileHandle) return false;
  if(!ensureFileAccessSupport()) return false;
  try{
    if(checkpointFileHandle?.queryPermission){
      const status=await checkpointFileHandle.queryPermission({mode:'readwrite'});
      if(status==='denied') throw new Error('Permission denied');
      if(status==='prompt'){
        const granted=await checkpointFileHandle.requestPermission({mode:'readwrite'});
        if(granted!=='granted') throw new Error('Permission denied');
      }
    }else if(checkpointFileHandle?.requestPermission){
      const granted=await checkpointFileHandle.requestPermission({mode:'readwrite'});
      if(granted!=='granted') throw new Error('Permission denied');
    }
    const writable=await checkpointFileHandle.createWritable();
    const payload=JSON.stringify(data,null,2);
    await writable.write(payload);
    await writable.close();
    return true;
  }catch(err){
    console.error('Failed to save checkpoint',err);
    return false;
  }
}

async function buildAppState(){
  const agentState=await agent.exportState();
  return {
    version:4,
    createdAt:new Date().toISOString(),
    algo:currentAlgoKey,
    playback:playbackMode,
    mode:trainingMode,
    envCount,
    gridSize:+ui.gridSize.value,
    rewardConfig:{...rewardConfig},
    agent:agentState,
    meta:{
      episode,totalSteps,bestLen,
      envCount,
      boardSize:COLS,
      rwHist:Array.from(rwHist),
      fruitHist:Array.from(fruitHist),
      lossHist:Array.from(lossHist),
      rewardTelemetry:rewardTelemetry.toJSON(),
    },
  };
}
function applyMeta(meta={}){
  episode=+meta.episode||0;
  totalSteps=+meta.totalSteps||0;
  bestLen=+meta.bestLen||0;
  assignArray(rwHist,meta.rwHist,v=>+v||0);
  assignArray(fruitHist,meta.fruitHist,v=>+v||0);
  assignArray(lossHist,meta.lossHist,v=>+v||0);
  if(meta.rewardTelemetry){
    rewardTelemetry.fromJSON(meta.rewardTelemetry);
  }else{
    rewardTelemetry.reset();
  }
  updateRewardTelemetryUI();
  const hasMetaCount=typeof meta.envCount==='number';
  let nextCount=hasMetaCount?meta.envCount:envCount;
  if(trainingMode==='auto'){
    nextCount=Math.max(12,nextCount||12);
  }else{
    nextCount=Math.max(1,nextCount||1);
  }
  const nextSize=typeof meta.boardSize==='number'?meta.boardSize:+ui.gridSize.value;
  if(hasMetaCount){
    ui.envCount.value=`${nextCount}`;
  }
  if(typeof meta.boardSize==='number'){
    ui.gridSize.value=`${meta.boardSize}`;
    updateGridLabel();
  }
  reconfigureEnvironment({count:nextCount,size:nextSize,force:true});
  if(trainingMode==='auto' && autoPilot){
    const stageIdx=autoPilot.boardStages.findIndex(stage=>stage.size===COLS);
    if(stageIdx>=0) autoPilot.stageIndex=stageIdx;
  }
  updateStatsUI();
  updateReadouts();
}
async function saveTrainingToFile(){
  if(!agent) return;
  const resume=training;
  if(resume) stopTraining();
  try{
    await waitForRenderIdle();
    const state=await buildAppState();
    const blob=new Blob([JSON.stringify(state,null,2)],{type:'application/json'});
    const stamp=new Date().toISOString().replace(/[:.]/g,'-');
    const url=URL.createObjectURL(blob);
    const a=document.createElement('a');
    a.href=url;
    a.download=`snake-training-${stamp}.json`;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    setTimeout(()=>URL.revokeObjectURL(url),1000);
    flash('Saved to file');
  }catch(err){
    console.error(err);
    flash('Failed to save',true);
  }finally{
    if(resume&&!watching) startTraining();
  }
}
async function ensureCheckpointDirectory(){
  if(!window.showDirectoryPicker) throw new Error('File access is not supported in this browser');
  if(!checkpointDirHandle){
    checkpointDirHandle=await window.showDirectoryPicker({mode:'read'});
  }
  return checkpointDirHandle;
}
async function readLatestCheckpoint(){
  const handle=await ensureCheckpointDirectory();
  const latest=await handle.getDirectoryHandle('latest');
  const fileHandle=await latest.getFileHandle('checkpoint.json');
  const file=await fileHandle.getFile();
  const text=await file.text();
  return JSON.parse(text);
}
async function applyCheckpointData(data){
  if(!data||!data.agent) throw new Error('Invalid checkpoint');
  const kind=(data.agent.kind==='dqn')?'dueling':(AGENT_PRESETS[data.agent.kind]?data.agent.kind:'dueling');
  const preset=AGENT_PRESETS[kind];
  if(preset){
    applyPresetToUI({...preset.defaults,...(data.agent.config||{})});
  }
  if(data.rewardConfig) applyRewardConfigToUI(data.rewardConfig);
  }


async function loadTrainingFromFile(file){
  if(!agent||!file) return;
  if(watching){
    flash('Stop watching first',true);
    return;
  }
  const resume=training;
  if(resume) stopTraining();
  try{
    const text=await file.text();
    const data=JSON.parse(text);
    if(!data||!data.agent) throw new Error('Invalid save file');
    const algo=data.algo&&AGENT_PRESETS[data.algo]?data.algo:'dueling';
    applyPresetToUI({...AGENT_PRESETS[algo].defaults,...data.agent.config});
    if(data.rewardConfig) applyRewardConfigToUI(data.rewardConfig);
    const loadedEnvCount=data.envCount??data.meta?.envCount??envCount;
    const boardSize=typeof data.gridSize==='number'?data.gridSize:typeof data.meta?.boardSize==='number'?data.meta.boardSize:+ui.gridSize.value;
    ui.envCount.value=`${loadedEnvCount}`;
    ui.gridSize.value=`${boardSize}`;
    updateGridLabel();
    reconfigureEnvironment({count:loadedEnvCount,size:boardSize,force:true});
    instantiateAgent(algo,{useCurrentUI:true});
    await agent.importState(data.agent);
    applyConfigToAgent();
    if(data.playback) setPlaybackMode(data.playback);
    if(data.mode) setTrainingMode(data.mode);
    applyMeta(data.meta||{});
    flash('Loaded from file');
  }catch(err){
    console.error(err);
    flash('Failed to load',true);
  }finally{
    if(resume&&!watching) startTraining();
  }
}

/* ---------------- Init ---------------- */
const presetSelect=document.getElementById('presetSelect');
if(presetSelect){
  presetSelectEl=presetSelect;
  for(const [key,preset] of Object.entries(AGENT_PRESETS)){
    const option=document.createElement('option');
    option.value=key;
    option.textContent=preset.label;
    if(key===currentAlgoKey) option.selected=true;
    presetSelect.appendChild(option);
  }
  presetSelect.addEventListener('change',e=>{
    const key=e.target.value;
    if(AGENT_PRESETS[key]){
      instantiateAgent(key);
    }
  });
}

const stageSelect=document.getElementById('stagePresetSelect');
if(stageSelect){
  stageSelectEl=stageSelect;
  stagePresetField=stageSelect.closest('.field');
  refreshStagePresetOptions(currentAlgoKey);
  stageSelect.addEventListener('change',e=>{
    const presetKey=e.target.value;
    if(!presetKey){
      currentStagePresetKey='';
      if(AGENT_PRESETS[currentAlgoKey]){
        const basePreset=AGENT_PRESETS[currentAlgoKey];
        ui.algoBadge.textContent=basePreset.badge||basePreset.label;
      }
      refreshStagePresetOptions(currentAlgoKey);
      return;
    }
    const preset=STAGE_PRESETS[presetKey];
    if(preset){
      applyPreset(preset,{preserveProgress:true});
      currentStagePresetKey=presetKey;
      refreshStagePresetOptions(currentAlgoKey);
      stageSelect.value=presetKey;
    }
  });
}

aiTuner=createAITuner({
  getVecEnv:()=>vecEnv,
  fetchTelemetry:({interval})=>buildAITelemetrySnapshot(interval),
  applyRewardConfig:applyAITunerRewardConfig,
  applyHyperparameters:applyAITunerHyperparameters,
  log:logAITunerEvent,
  isCheckpointEnabled:()=>checkpointEnabled,
});
aiTuner.setInterval(aiAnalysisInterval);
aiTuner.setEnabled(aiAutoTuneEnabled);

window.addEventListener('load',()=>{
  bindUI();
  setPlaybackMode('cinematic');
  instantiateAgent('dueling');
  setImmediateState(env);
});
</script>
</body>
</html>
